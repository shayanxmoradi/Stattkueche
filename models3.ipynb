{
 "cells": [
  {
   "cell_type": "code",
   "id": "d183b14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:09:36.734936Z",
     "start_time": "2025-07-18T23:09:19.748453Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scipy.signal import savgol_filter\n",
    "\n",
    "df_weather2 = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather3.csv',parse_dates=['DateOfCancel','DateOfService'])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b9ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### vif after incorporating the weather varibales for df_encoded\n",
    "# #### do the vif check\n",
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# num_cols = df_weather2.select_dtypes(include=[np.number]).columns.to_list()\n",
    "#\n",
    "# drop_col = ['DateOfOrder', 'DateOfService','days_to_cancel','CanceledQty','OrderId', 'TransactionId','cancel_timing','BookingNr','DateOfCancel','net_qty']\n",
    "#\n",
    "# vif_col_1 = [c for c in num_cols if c not in drop_col]\n",
    "#\n",
    "# vif_dat_1 = df_encoded[vif_col_1].copy()\n",
    "# vif_dat_1.replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "# vif_dat_1 = vif_dat_1.dropna()\n",
    "# vif_scores_1 = [variance_inflation_factor(vif_dat_1,i)\n",
    "#               for i in range(vif_dat_1.shape[1])\n",
    "#               ]\n",
    "#\n",
    "# vif_table_1 = (pd.DataFrame({'feature':vif_col_1,'VIF':vif_scores_1}).sort_values('VIF',ascending=False).reset_index(drop=True))\n",
    "# print('VIF values')\n",
    "# display(vif_table_1)\n",
    "#\n",
    "# hg_vif_1 = vif_table_1.loc[vif_table_1['VIF']>10,'feature'].to_list()\n",
    "# print('the high vif columns dropped')\n",
    "# df_without_vif_1 = vif_dat_1.drop(columns=hg_vif_1)\n",
    "# print(df_without_vif_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "id": "34d54aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:09:59.130575Z",
     "start_time": "2025-07-18T23:09:56.235748Z"
    }
   },
   "source": [
    "### Stage A for random forest with df_weather2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# ── 1) Prepare X / y for Stage A ────────────────────────────────\n",
    "df_A    = df_weather2.copy()\n",
    "# binary target: cancelled if CanceledQty > 0\n",
    "y_A     = (df_A['CanceledQty'] > 0).astype(int)\n",
    "X_A     = df_A.drop(columns=[\n",
    "    'CanceledQty',      # leak\n",
    "    'cancel_timing',    # multi‐class leftover\n",
    "    # any other columns you know shouldn’t go in:\n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "\n",
    "# ── 2) Re‐use your transformers ───────────────────────────────────\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X      = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# ── 3) Drop any remaining non-numeric / ID columns ───────────────\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "drop_cols_A = [\n",
    "    # any string or ID cols still in X_A\n",
    "    'Site','MenuBase','MenuName','GroupName','MenuNorm','MenuCode','net_qty','days_to_cancel'\n",
    "]\n",
    "\n",
    "# ── 4) Assemble Stage A pipeline ─────────────────────────────────\n",
    "pipeline_A = ImbPipeline([\n",
    "    #('te',      TargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "\n",
    "\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                   n_clusters=5)),\n",
    "    ('drop',    ColumnDropper(drop_cols_A)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    #('smote',   SMOTE(random_state=24)),\n",
    "    ('clf',     RandomForestClassifier(\n",
    "                   n_estimators=500,\n",
    "                   criterion='entropy',     # your experiment\n",
    "                   max_depth=8,\n",
    "                   min_samples_split=5,\n",
    "                   min_samples_leaf=1,\n",
    "                   max_features='sqrt',\n",
    "                   class_weight='balanced_subsample',\n",
    "                   random_state=24,\n",
    "                   n_jobs=-1\n",
    "               )),\n",
    "])\n",
    "\n",
    "# ── 5) Evaluate with rolling AUC ──────────────────────────────────\n",
    "tscv    = TimeSeriesSplit(n_splits=5)\n",
    "scores  = cross_val_score(\n",
    "    pipeline_A, X_A, y_A,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Stage A ROC-AUC:\", np.round(scores.mean(),4))\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InCVTargetEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 103\u001B[39m\n\u001B[32m     95\u001B[39m drop_cols_A = [\n\u001B[32m     96\u001B[39m     \u001B[38;5;66;03m# any string or ID cols still in X_A\u001B[39;00m\n\u001B[32m     97\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mSite\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuBase\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuName\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mGroupName\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuNorm\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuCode\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mnet_qty\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mdays_to_cancel\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m     98\u001B[39m ]\n\u001B[32m    100\u001B[39m \u001B[38;5;66;03m# ── 4) Assemble Stage A pipeline ─────────────────────────────────\u001B[39;00m\n\u001B[32m    101\u001B[39m pipeline_A = ImbPipeline([\n\u001B[32m    102\u001B[39m     \u001B[38;5;66;03m#('te',      TargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mte\u001B[39m\u001B[33m'\u001B[39m,      \u001B[43mInCVTargetEncoder\u001B[49m(cols=[\u001B[33m'\u001B[39m\u001B[33mSite\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuBase\u001B[39m\u001B[33m'\u001B[39m], smoothing=\u001B[32m0.3\u001B[39m)),\n\u001B[32m    104\u001B[39m \n\u001B[32m    105\u001B[39m \n\u001B[32m    106\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mhist\u001B[39m\u001B[33m'\u001B[39m,    HistCancelRateTransformer()),\n\u001B[32m    107\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mcluster\u001B[39m\u001B[33m'\u001B[39m, ClusterTransformer(\n\u001B[32m    108\u001B[39m                    profile_feats=[\u001B[33m'\u001B[39m\u001B[33mhist_cancel_rate\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mrain_flag\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mtemp_dev\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33msin_doy\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mcos_doy\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mmonth\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    109\u001B[39m        \u001B[33m'\u001B[39m\u001B[33mday_of_month\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis_month_end\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis_month_start\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mtavg_C\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mprcp_mm\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    110\u001B[39m                    n_clusters=\u001B[32m5\u001B[39m)),\n\u001B[32m    111\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mdrop\u001B[39m\u001B[33m'\u001B[39m,    ColumnDropper(drop_cols_A)),\n\u001B[32m    112\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mimpute\u001B[39m\u001B[33m'\u001B[39m,  MissingFlagImputer()),\n\u001B[32m    113\u001B[39m     \u001B[38;5;66;03m#('smote',   SMOTE(random_state=24)),\u001B[39;00m\n\u001B[32m    114\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mclf\u001B[39m\u001B[33m'\u001B[39m,     RandomForestClassifier(\n\u001B[32m    115\u001B[39m                    n_estimators=\u001B[32m500\u001B[39m,\n\u001B[32m    116\u001B[39m                    criterion=\u001B[33m'\u001B[39m\u001B[33mentropy\u001B[39m\u001B[33m'\u001B[39m,     \u001B[38;5;66;03m# your experiment\u001B[39;00m\n\u001B[32m    117\u001B[39m                    max_depth=\u001B[32m8\u001B[39m,\n\u001B[32m    118\u001B[39m                    min_samples_split=\u001B[32m5\u001B[39m,\n\u001B[32m    119\u001B[39m                    min_samples_leaf=\u001B[32m1\u001B[39m,\n\u001B[32m    120\u001B[39m                    max_features=\u001B[33m'\u001B[39m\u001B[33msqrt\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    121\u001B[39m                    class_weight=\u001B[33m'\u001B[39m\u001B[33mbalanced_subsample\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    122\u001B[39m                    random_state=\u001B[32m24\u001B[39m,\n\u001B[32m    123\u001B[39m                    n_jobs=-\u001B[32m1\u001B[39m\n\u001B[32m    124\u001B[39m                )),\n\u001B[32m    125\u001B[39m ])\n\u001B[32m    127\u001B[39m \u001B[38;5;66;03m# ── 5) Evaluate with rolling AUC ──────────────────────────────────\u001B[39;00m\n\u001B[32m    128\u001B[39m tscv    = TimeSeriesSplit(n_splits=\u001B[32m5\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'InCVTargetEncoder' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "d62bdc45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:18:26.079834Z",
     "start_time": "2025-07-18T23:18:23.181829Z"
    }
   },
   "source": [
    "### stage A with df_weather2 and lgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "# ── 1) Prepare X / y for Stage A ────────────────────────────────\n",
    "df_A    = df_weather2.copy()\n",
    "# binary target: cancelled if CanceledQty > 0\n",
    "y_A     = (df_A['CanceledQty'] > 0).astype(int)\n",
    "X_A     = df_A.drop(columns=[\n",
    "    'CanceledQty',      # leak\n",
    "    'cancel_timing',    # multi‐class leftover\n",
    "    # any other columns you know shouldn’t go in:\n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "\n",
    "# ── 2) Re‐use your transformers ───────────────────────────────────\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X      = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# ── 3) Drop any remaining non-numeric / ID columns ───────────────\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "drop_cols_A = [\n",
    "    # any string or ID cols still in X_A\n",
    "    'Site','MenuBase','MenuName','GroupName','MenuNorm','MenuCode','net_qty','days_to_cancel'\n",
    "]\n",
    "\n",
    "# ── 4) Assemble Stage A pipeline ─────────────────────────────────\n",
    "pipeline_A_l = ImbPipeline([\n",
    "    #('te',      TargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "\n",
    "\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                   n_clusters=5)),\n",
    "    ('drop',    ColumnDropper(drop_cols_A)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    #('smote',   SMOTE(random_state=24)),\n",
    "    ('clf',       LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=len(np.unique(y_A)),\n",
    "                                 random_state=24,\n",
    "                                 metric    = \"multi_logloss\",\n",
    "                                 n_jobs=-1))\n",
    ",\n",
    "])\n",
    "param_dist_lgbm = {\n",
    "    'clf__n_estimators':      [200,500,800],\n",
    "    'clf__learning_rate':     [0.01,0.03,0.05],\n",
    "    'clf__num_leaves':        [31,15,10],\n",
    "    'clf__max_depth':         [10,6,20],\n",
    "    'clf__subsample':         [0.7,0.3,1],\n",
    "    'clf__colsample_bytree':  [1.0,2.0,0.7],\n",
    "    'clf__min_child_samples': [20,10,7]\n",
    "}\n",
    "\n",
    "# 6) Rolling CV search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_A_lgbm = RandomizedSearchCV(\n",
    "    pipeline_A_l,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_A_lgbm.fit(X_A, y_A)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_A_lgbm.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_A_lgbm.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'InCVTargetEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 106\u001B[39m\n\u001B[32m     98\u001B[39m drop_cols_A = [\n\u001B[32m     99\u001B[39m     \u001B[38;5;66;03m# any string or ID cols still in X_A\u001B[39;00m\n\u001B[32m    100\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mSite\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuBase\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuName\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mGroupName\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuNorm\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuCode\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mnet_qty\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mdays_to_cancel\u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    101\u001B[39m ]\n\u001B[32m    103\u001B[39m \u001B[38;5;66;03m# ── 4) Assemble Stage A pipeline ─────────────────────────────────\u001B[39;00m\n\u001B[32m    104\u001B[39m pipeline_A_l = ImbPipeline([\n\u001B[32m    105\u001B[39m     \u001B[38;5;66;03m#('te',      TargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mte\u001B[39m\u001B[33m'\u001B[39m,      \u001B[43mInCVTargetEncoder\u001B[49m(cols=[\u001B[33m'\u001B[39m\u001B[33mSite\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuBase\u001B[39m\u001B[33m'\u001B[39m], smoothing=\u001B[32m0.3\u001B[39m)),\n\u001B[32m    107\u001B[39m \n\u001B[32m    108\u001B[39m \n\u001B[32m    109\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mhist\u001B[39m\u001B[33m'\u001B[39m,    HistCancelRateTransformer()),\n\u001B[32m    110\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mcluster\u001B[39m\u001B[33m'\u001B[39m, ClusterTransformer(\n\u001B[32m    111\u001B[39m                    profile_feats=[\u001B[33m'\u001B[39m\u001B[33mhist_cancel_rate\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mrain_flag\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mtemp_dev\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33msin_doy\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mcos_doy\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mmonth\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    112\u001B[39m        \u001B[33m'\u001B[39m\u001B[33mday_of_month\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis_month_end\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mis_month_start\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mtavg_C\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mprcp_mm\u001B[39m\u001B[33m'\u001B[39m],\n\u001B[32m    113\u001B[39m                    n_clusters=\u001B[32m5\u001B[39m)),\n\u001B[32m    114\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mdrop\u001B[39m\u001B[33m'\u001B[39m,    ColumnDropper(drop_cols_A)),\n\u001B[32m    115\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mimpute\u001B[39m\u001B[33m'\u001B[39m,  MissingFlagImputer()),\n\u001B[32m    116\u001B[39m     \u001B[38;5;66;03m#('smote',   SMOTE(random_state=24)),\u001B[39;00m\n\u001B[32m    117\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mclf\u001B[39m\u001B[33m'\u001B[39m,       LGBMClassifier(objective=\u001B[33m'\u001B[39m\u001B[33mmulticlass\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m    118\u001B[39m                                  num_class=\u001B[38;5;28mlen\u001B[39m(np.unique(y_A)),\n\u001B[32m    119\u001B[39m                                  random_state=\u001B[32m24\u001B[39m,\n\u001B[32m    120\u001B[39m                                  metric    = \u001B[33m\"\u001B[39m\u001B[33mmulti_logloss\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    121\u001B[39m                                  n_jobs=-\u001B[32m1\u001B[39m))\n\u001B[32m    122\u001B[39m ,\n\u001B[32m    123\u001B[39m ])\n\u001B[32m    124\u001B[39m param_dist_lgbm = {\n\u001B[32m    125\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mclf__n_estimators\u001B[39m\u001B[33m'\u001B[39m:      [\u001B[32m200\u001B[39m,\u001B[32m500\u001B[39m,\u001B[32m800\u001B[39m],\n\u001B[32m    126\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mclf__learning_rate\u001B[39m\u001B[33m'\u001B[39m:     [\u001B[32m0.01\u001B[39m,\u001B[32m0.03\u001B[39m,\u001B[32m0.05\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    131\u001B[39m     \u001B[33m'\u001B[39m\u001B[33mclf__min_child_samples\u001B[39m\u001B[33m'\u001B[39m: [\u001B[32m20\u001B[39m,\u001B[32m10\u001B[39m,\u001B[32m7\u001B[39m]\n\u001B[32m    132\u001B[39m }\n\u001B[32m    134\u001B[39m \u001B[38;5;66;03m# 6) Rolling CV search\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'InCVTargetEncoder' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage B2 with hist_cancel_rate and cluster calculation inside the cv loop and target encoding\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "# drop any leak columns before X\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty',      \n",
    "    'cancel_timing','timing_code',     \n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "# 0) Build the Stage B2 subset once:\n",
    "#df_B2 = df_weather[\n",
    "#    (df_weather.CanceledQty > 0) &\n",
    "#    (df_weather.cancel_timing.isin([0,1,2]))\n",
    "#].copy()\n",
    "#X_full = df_B2.drop(columns='cancel_timing')\n",
    "#y_full = df_B2['cancel_timing']\n",
    "\n",
    "# Update to HistCancelRateTransformer and ClusterTransformer to avoid tuple KeyError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols      = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean','count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] + \n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_[c])\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        # Store parameters verbatim—do NOT convert to list here\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to list only when you need it\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        # build tuple keys and map\n",
    "        tuples = list(zip(*(X[k] for k in keys)))\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        profile_feats,                 # list of column names to form profiles\n",
    "        group_keys=('Site','MenuBase'),\n",
    "        n_clusters=5,\n",
    "        out_col='cluster_id'\n",
    "    ):\n",
    "        # Store parameters as‐is; don’t mutate them here\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 1) Build training set profiles by group\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (\n",
    "            X.groupby(keys)[self.profile_feats]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "        )\n",
    "        # 2) Median‐impute any missing values in the profiles\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(\n",
    "            prof[self.profile_feats].median()\n",
    "        )\n",
    "\n",
    "        # 3) Standardize and run K-Means on those profiles\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "\n",
    "        # 4) Build a lookup map from (Site,MenuBase) → cluster label\n",
    "        tuples = [tuple(row) for row in prof[keys].values]\n",
    "        labels = list(self.kmeans_.labels_)\n",
    "        self.cluster_map_ = dict(zip(tuples, labels))\n",
    "        # Use the median cluster as a fallback for unseen combos\n",
    "        self.default_ = int(np.median(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(row) for row in X[keys].values]\n",
    "        # Map each row’s key to its cluster (or default)\n",
    "        X[self.out_col] = [\n",
    "            self.cluster_map_.get(t, self.default_) for t in tuples\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify numeric columns\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        # Replace inf with NaN for fitting\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fit the imputer on cleaned data\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Replace inf with NaN before transforming\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Add missing flags\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        # Impute\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# Example integration into your pipeline:\n",
    "# pipeline_B2 = Pipeline([\n",
    "#     ('hist', HistCancelRateTransformer()),\n",
    "#     ('cluster', ClusterTransformer(...)),\n",
    "#     ('impute', MissingFlagImputer()),\n",
    "#     ('clf', RandomForestClassifier(...))\n",
    "# ])\n",
    "\n",
    "# 4) Assemble pipelin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop any columns we know are non-numeric, IDs, or leftovers\n",
    "        return X.drop(columns=self.cols_to_drop,errors='ignore')\n",
    "drop_cols_B2 = [\n",
    "    'MenuName','GroupName','MenuNorm','MenuCode'\n",
    "\n",
    "]\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 4) Assemble pipelin\n",
    "\n",
    "#pipeline_B2 = Pipeline([\n",
    "#    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "#    ('hist',    HistCancelRateTransformer()),         # train-only hist rates\n",
    "#    ('cluster', ClusterTransformer(\n",
    "#                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev'],\n",
    "#                   n_clusters=5)),\n",
    "#    ('impute',  MissingFlagImputer()),               # replace inf → NaN, flag + median\n",
    "#    ('clf',     RandomForestClassifier(\n",
    "#                   n_estimators=100,\n",
    "#                   class_weight='balanced',\n",
    "#                   random_state=24,\n",
    "#                   n_jobs=-1\n",
    "#               )),\n",
    "#])\n",
    "\n",
    "#print(\"Columns going into the pipeline:\", X_full.columns.tolist())\n",
    "pipeline_B2_smote = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),                # drop strings, IDs, leakage cols\n",
    "    ('hist',    HistCancelRateTransformer()),                # train‐only hist rates\n",
    "    ('cluster', ClusterTransformer(                          # train‐only clusters\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),               \n",
    "    ('impute',  MissingFlagImputer()),                       # inf→nan, flag & median‐impute\n",
    "    ('smote',   SMOTE(random_state=24)),                     # synthesize new minority samples\n",
    "    ('clf',     RandomForestClassifier(                      # your tuned RF head\n",
    "                   n_estimators=500,\n",
    "                   min_samples_split=5,\n",
    "                   min_samples_leaf=2,\n",
    "                   max_features='sqrt',\n",
    "                   max_depth=None,\n",
    "                   class_weight='balanced',\n",
    "                   random_state=24,\n",
    "                   n_jobs=-1\n",
    "               )),\n",
    "])\n",
    "# 5) Rolling CV evaluation\n",
    "tscv    = TimeSeriesSplit(n_splits=5)\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# 1) Define the time‐series split\n",
    "#tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 2) Specify the RF hyper‐parameter distributions\n",
    "param_dist = {\n",
    "    'clf__n_estimators':      [600,1200],\n",
    "    'clf__max_depth':         [10,6],\n",
    "    'clf__min_samples_split': [10,20],\n",
    "    'clf__min_samples_leaf':  [15],\n",
    "    'clf__bootstrap':         [True],\n",
    "    'clf__criterion':         ['entropy'],\n",
    "    'clf__max_features':      [0.8,1.2]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline_B2_smote,               # the full hist→cluster→impute→RF pipe\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                 # how many random combos to try\n",
    "    cv=tscv,                   # your time‐series folds\n",
    "    scoring='roc_auc_ovo_weighted',  # optimize roc_auc_ovo_weighted\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "# 4) Fit on your Stage B2 data\n",
    "search.fit(X_B2, y_B2)\n",
    "\n",
    "# 5) Check the results\n",
    "print(\"Best roc_auc_ovo_weighted:\", np.round(search.best_score_, 4))\n",
    "print(\"Best hyper-parameters:\")\n",
    "for k, v in search.best_params_.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "#scores  = cross_val_score(\n",
    "#    pipeline_B2,\n",
    "#    X_full,\n",
    "#    y_full,\n",
    "#    cv=tscv,\n",
    "#    scoring='average_precision',\n",
    "#    n_jobs=1\n",
    "#)\n",
    "#print(\"Stage B2 PR-AUC:\", np.round(scores.mean(),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e02002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage B2 with hist_cancel_rate and cluster calculation inside the cv loop and target encoding LGBM\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "# drop any leak columns before X\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty',      \n",
    "    'cancel_timing','timing_code',     \n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "# 0) Build the Stage B2 subset once:\n",
    "#df_B2 = df_weather[\n",
    "#    (df_weather.CanceledQty > 0) &\n",
    "#    (df_weather.cancel_timing.isin([0,1,2]))\n",
    "#].copy()\n",
    "#X_full = df_B2.drop(columns='cancel_timing')\n",
    "#y_full = df_B2['cancel_timing']\n",
    "\n",
    "# Update to HistCancelRateTransformer and ClusterTransformer to avoid tuple KeyError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols      = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean','count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] + \n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_[c])\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        # Store parameters verbatim—do NOT convert to list here\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to list only when you need it\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        # build tuple keys and map\n",
    "        tuples = list(zip(*(X[k] for k in keys)))\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        profile_feats,                 # list of column names to form profiles\n",
    "        group_keys=('Site','MenuBase'),\n",
    "        n_clusters=5,\n",
    "        out_col='cluster_id'\n",
    "    ):\n",
    "        # Store parameters as‐is; don’t mutate them here\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 1) Build training set profiles by group\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (\n",
    "            X.groupby(keys)[self.profile_feats]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "        )\n",
    "        # 2) Median‐impute any missing values in the profiles\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(\n",
    "            prof[self.profile_feats].median()\n",
    "        )\n",
    "\n",
    "        # 3) Standardize and run K-Means on those profiles\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "\n",
    "        # 4) Build a lookup map from (Site,MenuBase) → cluster label\n",
    "        tuples = [tuple(row) for row in prof[keys].values]\n",
    "        labels = list(self.kmeans_.labels_)\n",
    "        self.cluster_map_ = dict(zip(tuples, labels))\n",
    "        # Use the median cluster as a fallback for unseen combos\n",
    "        self.default_ = int(np.median(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(row) for row in X[keys].values]\n",
    "        # Map each row’s key to its cluster (or default)\n",
    "        X[self.out_col] = [\n",
    "            self.cluster_map_.get(t, self.default_) for t in tuples\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify numeric columns\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        # Replace inf with NaN for fitting\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fit the imputer on cleaned data\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Replace inf with NaN before transforming\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Add missing flags\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        # Impute\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# Example integration into your pipeline:\n",
    "# pipeline_B2 = Pipeline([\n",
    "#     ('hist', HistCancelRateTransformer()),\n",
    "#     ('cluster', ClusterTransformer(...)),\n",
    "#     ('impute', MissingFlagImputer()),\n",
    "#     ('clf', RandomForestClassifier(...))\n",
    "# ])\n",
    "\n",
    "# 4) Assemble pipelin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop any columns we know are non-numeric, IDs, or leftovers\n",
    "        return X.drop(columns=self.cols_to_drop,errors='ignore')\n",
    "drop_cols_B2 = [\n",
    "    'MenuName','GroupName','MenuNorm','MenuCode'\n",
    "\n",
    "]\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 4) Assemble pipelin\n",
    "\n",
    "#pipeline_B2 = Pipeline([\n",
    "#    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "#    ('hist',    HistCancelRateTransformer()),         # train-only hist rates\n",
    "#    ('cluster', ClusterTransformer(\n",
    "#                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev'],\n",
    "#                   n_clusters=5)),\n",
    "#    ('impute',  MissingFlagImputer()),               # replace inf → NaN, flag + median\n",
    "#    ('clf',     RandomForestClassifier(\n",
    "#                   n_estimators=100,\n",
    "#                   class_weight='balanced',\n",
    "#                   random_state=24,\n",
    "#                   n_jobs=-1\n",
    "#               )),\n",
    "#])\n",
    "\n",
    "#print(\"Columns going into the pipeline:\", X_full.columns.tolist())\n",
    "pipeline_B2_lgbm = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),                # drop strings, IDs, leakage cols\n",
    "    ('hist',    HistCancelRateTransformer()),                # train‐only hist rates\n",
    "    ('cluster', ClusterTransformer(                          # train‐only clusters\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),               \n",
    "    ('impute',  MissingFlagImputer()),                       # inf→nan, flag & median‐impute\n",
    "    #('smote',   SMOTE(random_state=24)),                     # synthesize new minority samples\n",
    "   ('clf',       LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=len(np.unique(y_B2)),\n",
    "                                 random_state=24,\n",
    "                                 metric    = \"multi_logloss\",\n",
    "                                 n_jobs=-1))\n",
    ",\n",
    "])\n",
    "param_dist_lgbm = {\n",
    "    'clf__n_estimators':      [200,600],\n",
    "    'clf__learning_rate':     [0.01,0.07],\n",
    "    'clf__num_leaves':        [31,40],\n",
    "    'clf__max_depth':         [10,20],\n",
    "    'clf__subsample':         [0.7,1],\n",
    "    'clf__colsample_bytree':  [1.0,2],\n",
    "    'clf__min_child_samples': [20,30]\n",
    "}\n",
    "\n",
    "# 6) Rolling CV search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_lgbm = RandomizedSearchCV(\n",
    "    pipeline_B2_lgbm,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_lgbm.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_lgbm.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_lgbm.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage B2 with hist_cancel_rate and cluster calculation inside the cv loop and target encoding catboost\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import make_scorer, f1_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "# drop any leak columns before X\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty',      \n",
    "    'cancel_timing','timing_code',     \n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "# 0) Build the Stage B2 subset once:\n",
    "#df_B2 = df_weather[\n",
    "#    (df_weather.CanceledQty > 0) &\n",
    "#    (df_weather.cancel_timing.isin([0,1,2]))\n",
    "#].copy()\n",
    "#X_full = df_B2.drop(columns='cancel_timing')\n",
    "#y_full = df_B2['cancel_timing']\n",
    "\n",
    "# Update to HistCancelRateTransformer and ClusterTransformer to avoid tuple KeyError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols      = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean','count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] + \n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_[c])\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        # Store parameters verbatim—do NOT convert to list here\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to list only when you need it\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        # build tuple keys and map\n",
    "        tuples = list(zip(*(X[k] for k in keys)))\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        profile_feats,                 # list of column names to form profiles\n",
    "        group_keys=('Site','MenuBase'),\n",
    "        n_clusters=5,\n",
    "        out_col='cluster_id'\n",
    "    ):\n",
    "        # Store parameters as‐is; don’t mutate them here\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 1) Build training set profiles by group\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (\n",
    "            X.groupby(keys)[self.profile_feats]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "        )\n",
    "        # 2) Median‐impute any missing values in the profiles\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(\n",
    "            prof[self.profile_feats].median()\n",
    "        )\n",
    "\n",
    "        # 3) Standardize and run K-Means on those profiles\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "\n",
    "        # 4) Build a lookup map from (Site,MenuBase) → cluster label\n",
    "        tuples = [tuple(row) for row in prof[keys].values]\n",
    "        labels = list(self.kmeans_.labels_)\n",
    "        self.cluster_map_ = dict(zip(tuples, labels))\n",
    "        # Use the median cluster as a fallback for unseen combos\n",
    "        self.default_ = int(np.median(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(row) for row in X[keys].values]\n",
    "        # Map each row’s key to its cluster (or default)\n",
    "        X[self.out_col] = [\n",
    "            self.cluster_map_.get(t, self.default_) for t in tuples\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify numeric columns\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        # Replace inf with NaN for fitting\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fit the imputer on cleaned data\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Replace inf with NaN before transforming\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Add missing flags\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        # Impute\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# Example integration into your pipeline:\n",
    "# pipeline_B2 = Pipeline([\n",
    "#     ('hist', HistCancelRateTransformer()),\n",
    "#     ('cluster', ClusterTransformer(...)),\n",
    "#     ('impute', MissingFlagImputer()),\n",
    "#     ('clf', RandomForestClassifier(...))\n",
    "# ])\n",
    "\n",
    "# 4) Assemble pipelin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop any columns we know are non-numeric, IDs, or leftovers\n",
    "        return X.drop(columns=self.cols_to_drop,errors='ignore')\n",
    "drop_cols_B2 = [\n",
    "    'MenuName','GroupName','MenuNorm','MenuCode'\n",
    "\n",
    "]\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 4) Assemble pipelin\n",
    "\n",
    "#pipeline_B2 = Pipeline([\n",
    "#    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "#    ('hist',    HistCancelRateTransformer()),         # train-only hist rates\n",
    "#    ('cluster', ClusterTransformer(\n",
    "#                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev'],\n",
    "#                   n_clusters=5)),\n",
    "#    ('impute',  MissingFlagImputer()),               # replace inf → NaN, flag + median\n",
    "#    ('clf',     RandomForestClassifier(\n",
    "#                   n_estimators=100,\n",
    "#                   class_weight='balanced',\n",
    "#                   random_state=24,\n",
    "#                   n_jobs=-1\n",
    "#               )),\n",
    "#])\n",
    "\n",
    "#print(\"Columns going into the pipeline:\", X_full.columns.tolist())\n",
    "pipeline_B2_cat = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),                # drop strings, IDs, leakage cols\n",
    "    ('hist',    HistCancelRateTransformer()),                # train‐only hist rates\n",
    "    ('cluster', ClusterTransformer(                          # train‐only clusters\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),               \n",
    "    ('impute',  MissingFlagImputer()),                       # inf→nan, flag & median‐impute\n",
    "    #('smote',   SMOTE(random_state=24)),                     # synthesize new minority samples\n",
    "    ('clf',     CatBoostClassifier(\n",
    "                    # safe defaults:\n",
    "                    iterations=500,\n",
    "                    #cat_features = cat_col,\n",
    "                    auto_class_weights='Balanced',\n",
    "                    loss_function='MultiClass',\n",
    "                    #use_focal_loss=True,\n",
    "                    #focal_loss_gamma=2,\n",
    "                    #loss_function='Focal',       # turn focal loss ON\n",
    "                    #focal_loss_gamma=2.0,\n",
    "                    learning_rate=0.05,\n",
    "                    depth=6,\n",
    "                    early_stopping_rounds=50,\n",
    "                    l2_leaf_reg=3,\n",
    "                    # avoid verbose logs\n",
    "                    verbose=False,\n",
    "                    random_seed=24,\n",
    "                    thread_count=-1\n",
    "                    \n",
    "               ))\n",
    "])\n",
    "\n",
    "# 3) CatBoost hyperparameter grid\n",
    "cat_param_dist = {\n",
    "    'clf__iterations':       [500,1000,800],\n",
    "    'clf__loss_function':      ['MultiClass'],\n",
    "    #'clf__use_focal_loss':     [True, False],       # try both\n",
    "    #'clf__focal_loss_gamma':   [1.0, 2.0, 5.0], \n",
    "    #'clf__focal_alpha':        [0.25, 1.0, 2.0], \n",
    "    'clf__learning_rate':    [0.01, 0.03, 0.1],\n",
    "    'clf__depth':            [6,8,10],\n",
    "    'clf__l2_leaf_reg':      [1,3,10],\n",
    "    'clf__bagging_temperature': [0,3,7],\n",
    "    'clf__rsm':              [0.5, 0.8, 1.0]\n",
    "}\n",
    "scoring_1 = {\n",
    "    'pr_auc':       'average_precision',       # your primary metric\n",
    "    'roc_auc_ovo':  'roc_auc_ovo',             # multiclass ROC‐AUC\n",
    "    'f1_macro':     'f1_macro',                # overall F1 across classes\n",
    "    'bal_acc':      make_scorer(balanced_accuracy_score),\n",
    "    'neg_log_loss': 'neg_log_loss'             # lower log‐loss = better\n",
    "}\n",
    "\n",
    "# 6) Rolling CV search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_cat = RandomizedSearchCV(\n",
    "    pipeline_B2_cat,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    refit='roc_auc_ovo',\n",
    "    scoring=scoring_1,\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_cat.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_cat.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_cat.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
