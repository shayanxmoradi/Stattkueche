{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T08:34:55.580965Z",
     "start_time": "2025-06-01T08:34:55.571359Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:46:15.600337Z",
     "start_time": "2025-06-01T07:46:15.574320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_dir = 'results'\n",
    "snapshot_dir = os.path.join(results_dir, 'snapshots')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(f\"Created directory: {results_dir}\")\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.makedirs(snapshot_dir)\n",
    "    print(f\"Created directory: {snapshot_dir}\")\n",
    "print(\"-\" * 50)\n"
   ],
   "id": "6fbf78a7de27498f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: results\n",
      "Created directory: results/snapshots\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:01:25.721038Z",
     "start_time": "2025-06-01T08:01:09.537371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = 'venvx/AnnonymData.csv'\n",
    "rows_for_snapshot = 1000\n",
    "try:\n",
    "    # For large files, be mindful of memory.\n",
    "    # We'll load it directly for now, but consider 'chunksize' or 'dtype' optimization if needed.\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded data from '{file_path}'.\")\n",
    "    print(f\"Dataset shape: {df.shape}\") # (rows, columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    snapshot1_path = os.path.join(snapshot_dir, 'snapshot_1_raw_loaded_dataset.html')\n",
    "    df.head(rows_for_snapshot).to_html(snapshot1_path, escape=False, index=False) # escape=False for cleaner HTML, index=False to not write pandas index\n",
    "    print(f\"\\nSnapshot 1: First {rows_for_snapshot} rows of the raw loaded dataset saved to '{snapshot1_path}'\")\n",
    "\n",
    "\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nDataset info:\")\n",
    "    df.info(verbose=True, show_counts=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at '{file_path}'. Please check the path and filename.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "    exit()\n",
    "print(\"-\" * 50)"
   ],
   "id": "9ebe4bee89b7866f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from 'venvx/AnnonymData.csv'.\n",
      "Dataset shape: (6538739, 14)\n",
      "\n",
      "Snapshot 1: First 1000 rows of the raw loaded dataset saved to 'results/snapshots/snapshot_1_raw_loaded_dataset.html'\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "    OrderId                     TransactionId DateOfService  \\\n",
      "0  11518978  4c5060636f584ef9a1effa77282755f5    2020-01-02   \n",
      "1  11285143  68472c70b9c84fb784834ecc257827d7    2020-01-02   \n",
      "2  11285146  7262eace0d104592b1269e38f5b45ec1    2020-01-02   \n",
      "3  11285152  8e451931e8fc4554869c3e4533b65e23    2020-01-02   \n",
      "4  11285155  bfa8fa0812ee40baa98e5aaf52d30e0b    2020-01-02   \n",
      "\n",
      "           DateOfOrder  OrderQty                     MenuName MenuPrice  \\\n",
      "0  2020-02-05 11:54:08         1             Mittagessen (Gs)      3,10   \n",
      "1  2019-12-16 10:30:51         1  Smart Eating Buffet (WGrus)      0,00   \n",
      "2  2019-12-16 10:31:33         1  Smart Eating Buffet (WGrus)      2,90   \n",
      "3  2019-12-16 10:32:05         1  Smart Eating Buffet (WGrus)      0,00   \n",
      "4  2019-12-16 10:32:31         1  Smart Eating Buffet (WGrus)      2,90   \n",
      "\n",
      "  MenuSubsidy      BookingNr                       GroupName  CanceledQty  \\\n",
      "0        0,00   349-88220481        xxx3,45€ normal 5T (68€)            0   \n",
      "1        3,50  248-141751492          Steinfurt Abo ermäßigt            0   \n",
      "2        0,60   248-77489928  Westerkappeln Grundschüler Abo            0   \n",
      "3        3,50   248-77558043          Steinfurt Abo ermäßigt            0   \n",
      "4        0,60   248-77420774  Westerkappeln Grundschüler Abo            0   \n",
      "\n",
      "  DateOfCancel Site SchoolID  \n",
      "0          NaN   LP   SCH001  \n",
      "1          NaN   BK   SCH002  \n",
      "2          NaN   BK   SCH002  \n",
      "3          NaN   BK   SCH002  \n",
      "4          NaN   BK   SCH002  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6538739 entries, 0 to 6538738\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   OrderId        6538739 non-null  int64 \n",
      " 1   TransactionId  6538739 non-null  object\n",
      " 2   DateOfService  6538739 non-null  object\n",
      " 3   DateOfOrder    6538739 non-null  object\n",
      " 4   OrderQty       6538739 non-null  int64 \n",
      " 5   MenuName       6538739 non-null  object\n",
      " 6   MenuPrice      6538739 non-null  object\n",
      " 7   MenuSubsidy    6538739 non-null  object\n",
      " 8   BookingNr      6538739 non-null  object\n",
      " 9   GroupName      6538739 non-null  object\n",
      " 10  CanceledQty    6538739 non-null  int64 \n",
      " 11  DateOfCancel   934479 non-null   object\n",
      " 12  Site           5692727 non-null  object\n",
      " 13  SchoolID       6538739 non-null  object\n",
      "dtypes: int64(3), object(11)\n",
      "memory usage: 698.4+ MB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:29:28.806266Z",
     "start_time": "2025-06-01T07:29:26.474506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"--- Step 2: Initial Data Cleaning & Preprocessing ---\")\n",
    "df_processed = df.copy()\n",
    "\n",
    "date_columns = ['DateOfService', 'DateOfOrder', 'DateOfCancel']\n",
    "for col in date_columns:\n",
    "    if col in df_processed.columns:\n",
    "        # Attempt conversion, coercing errors will turn unparseable dates into NaT (Not a Time)\n",
    "        df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
    "        print(f\"Column '{col}' converted. NaNs introduced by coercion: {df_processed[col].isnull().sum()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Date column '{col}' not found.\")"
   ],
   "id": "7db983729470c8ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Initial Data Cleaning & Preprocessing ---\n",
      "Column 'DateOfService' converted. NaNs introduced by coercion: 0\n",
      "Column 'DateOfOrder' converted. NaNs introduced by coercion: 0\n",
      "Column 'DateOfCancel' converted. NaNs introduced by coercion: 5604260\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:30:17.496144Z",
     "start_time": "2025-06-01T07:30:11.022911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nConverting financial columns ('MenuPrice', 'MenuSubsidy') to numeric...\")\n",
    "financial_columns = ['MenuPrice', 'MenuSubsidy']\n",
    "for col in financial_columns:\n",
    "    if col in df_processed.columns:\n",
    "        if df_processed[col].dtype == 'object': # Only process if it's an object type\n",
    "            # Remove currency symbols (e.g., €, $, £) and commas (as thousands separators)\n",
    "            # This regex is an example, adjust if your currency format is different\n",
    "            df_processed[col] = df_processed[col].astype(str).str.replace(r'[€\\$£,]', '', regex=True)\n",
    "            # Convert to numeric, coercing errors.\n",
    "            # If your numbers use ',' as decimal (e.g., German format), you'd first remove '.', then replace ',' with '.'\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "            print(f\"Column '{col}' converted to numeric. NaNs introduced: {df_processed[col].isnull().sum()}\")\n",
    "        elif pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "            print(f\"Column '{col}' is already numeric.\")\n",
    "        else:\n",
    "            print(f\"Column '{col}' is of type {df_processed[col].dtype} and was not processed as a typical currency string.\")\n",
    "    else:\n",
    "        print(f\"Warning: Financial column '{col}' not found.\")"
   ],
   "id": "ee332f452f40ca0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting financial columns ('MenuPrice', 'MenuSubsidy') to numeric...\n",
      "Column 'MenuPrice' converted to numeric. NaNs introduced: 0\n",
      "Column 'MenuSubsidy' converted to numeric. NaNs introduced: 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:30:44.682136Z",
     "start_time": "2025-06-01T07:30:44.665291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nEnsuring 'OrderQty' and 'CanceledQty' are numeric...\")\n",
    "for col in ['OrderQty', 'CanceledQty']:\n",
    "    if col in df_processed.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "            print(f\"Column '{col}' converted to numeric. NaNs introduced: {df_processed[col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"Column '{col}' is already numeric.\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found.\")"
   ],
   "id": "c9159035998f25b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensuring 'OrderQty' and 'CanceledQty' are numeric...\n",
      "Column 'OrderQty' is already numeric.\n",
      "Column 'CanceledQty' is already numeric.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:33:04.134680Z",
     "start_time": "2025-06-01T07:33:03.801834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'Site' in df_processed.columns:\n",
    "    missing_site_percentage = df_processed['Site'].isnull().mean() * 100\n",
    "    print(f\"\\nMissing values in 'Site': {missing_site_percentage:.2f}%\")\n",
    "    # Example: df_processed['Site'].fillna('Unknown', inplace=True)\n",
    "    # Or, if you know the main sites are MS, LP, BK, you might investigate if missing sites can be inferred.\n",
    "    # For now, let's fill with 'Unknown' as a placeholder strategy.\n",
    "    df_processed['Site'] = df_processed['Site'].fillna('UnknownSite')\n",
    "    print(\"Filled missing 'Site' values with 'UnknownSite'.\")"
   ],
   "id": "12679da622ba14ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in 'Site': 0.00%\n",
      "Filled missing 'Site' values with 'UnknownSite'.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:34:24.837009Z",
     "start_time": "2025-06-01T07:34:22.132844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nFiltering out irrelevant orders...\")\n",
    "initial_rows = len(df_processed)\n",
    "\n",
    "# Condition 1: Orders where DateOfCancel is after DateOfService\n",
    "if 'DateOfCancel' in df_processed.columns and 'DateOfService' in df_processed.columns:\n",
    "    condition1_filter = (df_processed['DateOfCancel'].notna()) & \\\n",
    "                        (df_processed['DateOfService'].notna()) & \\\n",
    "                        (df_processed['DateOfCancel'] > df_processed['DateOfService'])\n",
    "    rows_to_drop_cond1 = df_processed[condition1_filter]\n",
    "    if not rows_to_drop_cond1.empty:\n",
    "        print(f\"Found {len(rows_to_drop_cond1)} orders where DateOfCancel is after DateOfService. These will be dropped.\")\n",
    "        df_processed = df_processed[~condition1_filter]\n",
    "    else:\n",
    "        print(\"No orders found where DateOfCancel is after DateOfService.\")"
   ],
   "id": "8a18505926feec3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering out irrelevant orders...\n",
      "Found 70970 orders where DateOfCancel is after DateOfService. These will be dropped.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:35:19.317250Z",
     "start_time": "2025-06-01T07:35:18.170560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Condition 2: Orders where OrderQty < CanceledQty\n",
    "# Ensure both columns are numeric and handle potential NaNs before comparison\n",
    "if 'OrderQty' in df_processed.columns and 'CanceledQty' in df_processed.columns:\n",
    "    # Fill NaNs with 0 for comparison, assuming NaN in Qty means 0 for this specific filter logic\n",
    "    order_qty_filled = df_processed['OrderQty'].fillna(0)\n",
    "    canceled_qty_filled = df_processed['CanceledQty'].fillna(0)\n",
    "\n",
    "    condition2_filter = order_qty_filled < canceled_qty_filled\n",
    "    rows_to_drop_cond2 = df_processed[condition2_filter]\n",
    "    if not rows_to_drop_cond2.empty:\n",
    "        print(f\"Found {len(rows_to_drop_cond2)} orders where OrderQty < CanceledQty. These will be dropped.\")\n",
    "        df_processed = df_processed[~condition2_filter]\n",
    "    else:\n",
    "        print(\"No orders found where OrderQty < CanceledQty.\")\n",
    "\n",
    "rows_after_filtering = len(df_processed)\n",
    "print(f\"Rows dropped due to filtering: {initial_rows - rows_after_filtering}\")\n",
    "print(f\"Dataset shape after filtering: {df_processed.shape}\")\n"
   ],
   "id": "2206598b7b65abd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 793547 orders where OrderQty < CanceledQty. These will be dropped.\n",
      "Rows dropped due to filtering: 864517\n",
      "Dataset shape after filtering: (5674222, 14)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T07:36:18.023009Z",
     "start_time": "2025-06-01T07:36:17.035824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# f. Create the Target Variable: `NeededMeals`\n",
    "# NeededMeals = OrderQty - CanceledQty (after filtering and ensuring NaNs in Qty are handled)\n",
    "print(\"\\nCreating the target variable 'NeededMeals'...\")\n",
    "if 'OrderQty' in df_processed.columns and 'CanceledQty' in df_processed.columns:\n",
    "    # Assuming that if CanceledQty is NaN for an order that wasn't filtered out, it means 0 cancellations for that order.\n",
    "    # And if OrderQty is NaN (should be rare after initial checks), treat as 0 for this calculation.\n",
    "    df_processed['OrderQty_filled'] = df_processed['OrderQty'].fillna(0)\n",
    "    df_processed['CanceledQty_filled'] = df_processed['CanceledQty'].fillna(0)\n",
    "\n",
    "    df_processed['NeededMeals'] = df_processed['OrderQty_filled'] - df_processed['CanceledQty_filled']\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    df_processed.drop(columns=['OrderQty_filled', 'CanceledQty_filled'], inplace=True)\n",
    "\n",
    "    print(\"'NeededMeals' column created.\")\n",
    "    print(\"Summary of 'NeededMeals':\")\n",
    "    print(df_processed['NeededMeals'].describe())\n",
    "\n",
    "    # Sanity check: NeededMeals should not be negative if OrderQty < CanceledQty was filtered.\n",
    "    # However, if OrderQty was 0 and CanceledQty was 0, NeededMeals is 0.\n",
    "    # If OrderQty was 0 and CanceledQty was 1 (bank account not covered), this record should have been filtered.\n",
    "    if not df_processed[df_processed['NeededMeals'] < 0].empty:\n",
    "        print(f\"Warning: Found {len(df_processed[df_processed['NeededMeals'] < 0])} records with negative NeededMeals. Review filtering logic.\")\n",
    "        print(df_processed[df_processed['NeededMeals'] < 0][['OrderQty', 'CanceledQty', 'NeededMeals']].head())\n",
    "    else:\n",
    "        print(\"No negative 'NeededMeals' found after calculation, which is good.\")\n",
    "else:\n",
    "    print(\"Could not create 'NeededMeals' as 'OrderQty' or 'CanceledQty' is missing.\")"
   ],
   "id": "91788b25b730264b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the target variable 'NeededMeals'...\n",
      "'NeededMeals' column created.\n",
      "Summary of 'NeededMeals':\n",
      "count    5.674222e+06\n",
      "mean     9.985110e-01\n",
      "std      4.098289e-01\n",
      "min      0.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      1.000000e+00\n",
      "max      1.200000e+02\n",
      "Name: NeededMeals, dtype: float64\n",
      "No negative 'NeededMeals' found after calculation, which is good.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:14:47.441164Z",
     "start_time": "2025-06-01T08:14:44.522098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Addressing 'MenuName' Variations (Initial Pass) ---\")\n",
    "if 'MenuName' in df_processed.columns:\n",
    "    unique_menu_names_count_before = df_processed['MenuName'].nunique()\n",
    "    print(f\"Number of unique 'MenuName' entries before any cleaning: {unique_menu_names_count_before}\")\n",
    "\n",
    "    # Perform basic cleaning first\n",
    "    df_processed['MenuName_Cleaned'] = df_processed['MenuName'].astype(str).str.lower().str.strip()\n",
    "    unique_menu_names_count_after_basic = df_processed['MenuName_Cleaned'].nunique()\n",
    "    print(f\"Number of unique 'MenuName_Cleaned' entries after basic cleaning (lowercase, strip): {unique_menu_names_count_after_basic}\")\n",
    "\n",
    "    if unique_menu_names_count_after_basic > 50:\n",
    "        print(\"First 20 unique 'MenuName_Cleaned' (sample after basic cleaning):\")\n",
    "        print(df_processed['MenuName_Cleaned'].unique()[:20])\n",
    "    else:\n",
    "        print(\"Unique 'MenuName_Cleaned' (after basic cleaning):\")\n",
    "        print(df_processed['MenuName_Cleaned'].unique())\n",
    "\n",
    "    snapshot5_path = os.path.join(snapshot_dir, 'snapshot_5_after_basic_MenuName_cleaning.html')\n",
    "    # Displaying head() with the new MenuName_Cleaned column\n",
    "    df_processed.head(rows_for_snapshot).to_html(snapshot5_path, escape=False, index=False)\n",
    "    print(f\"\\nSnapshot 5: First {rows_for_snapshot} rows (showing 'MenuName_Cleaned') saved to '{snapshot5_path}'\")\n",
    "else:\n",
    "    print(\"Warning: 'MenuName' column not found.\")\n",
    "\n",
    "print(\"\\n--- Initial Data Cleaning & Preprocessing (Part 1 - Basic Steps) Complete ---\")\n",
    "print(\"Current DataFrame shape:\", df_processed.shape)\n",
    "print(\"-\" * 50)\n"
   ],
   "id": "e433e7aadf89cb5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Addressing 'MenuName' Variations (Initial Pass) ---\n",
      "Number of unique 'MenuName' entries before any cleaning: 322\n",
      "Number of unique 'MenuName_Cleaned' entries after basic cleaning (lowercase, strip): 322\n",
      "First 20 unique 'MenuName_Cleaned' (sample after basic cleaning):\n",
      "['mittagessen (gs)' 'smart eating buffet (wgrus)' 'mittagessen (bs)'\n",
      " 'smart eating buffet (primus)' 'menü a (hss)'\n",
      " 'smart eating buffet (gymbo)' 'smart eating buffet (brs)' 'menü b1'\n",
      " 'smart eating buffet (ema)' 'mittagessen (ml)'\n",
      " 'smart-eating buffet (egm)' 'dge-menü (mg)' 'menü a (bo-mitte)'\n",
      " 'menü a (marbi)' 'smart eating buffet (gma)' 'mittagessen (bk)' 'menü a'\n",
      " 'menü a (szg)' 'salatbar (mpg)' 'menü b2 (lui)']\n",
      "\n",
      "Snapshot 5: First 1000 rows (showing 'MenuName_Cleaned') saved to 'results/snapshots/snapshot_5_after_basic_MenuName_cleaning.html'\n",
      "\n",
      "--- Initial Data Cleaning & Preprocessing (Part 1 - Basic Steps) Complete ---\n",
      "Current DataFrame shape: (5674222, 16)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:18:13.506470Z",
     "start_time": "2025-06-01T08:18:13.490317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 2.h: Detailed `Site` Column Refinement ---\n",
    "print(\"\\n--- Step 2.h: Detailed `Site` Column Refinement ---\")\n",
    "if 'Site_Cleaned' in df_processed.columns:\n",
    "    print(\"Unique 'Site_Cleaned' values BEFORE detailed refinement:\")\n",
    "    print(df_processed['Site_Cleaned'].unique())\n",
    "\n",
    "    # Define expected sites and a mapping for common variations\n",
    "    # This mapping should be expanded based on your actual data exploration\n",
    "    site_mapping = {\n",
    "        'ms': 'MS', 'muenster': 'MS', 'münster': 'MS',\n",
    "        'lp': 'LP', 'lippstadt': 'LP',\n",
    "        'bk': 'BK', 'bergkamen': 'BK',\n",
    "        # Add other variations you find to this dictionary\n",
    "        'unknownsite': 'UnknownSite' # Ensure 'UnknownSite' itself is consistently cased if it came from NaNs\n",
    "    }\n",
    "\n",
    "    # Apply mapping: Convert to lowercase first for case-insensitive mapping\n",
    "    df_processed['Site_Standardized'] = df_processed['Site_Cleaned'].astype(str).str.lower().map(site_mapping).fillna(df_processed['Site_Cleaned'])\n",
    "    # For values not in mapping, keep original (or map to 'UnknownSite' if preferred)\n",
    "    # A more robust approach for unmapped values:\n",
    "    # df_processed['Site_Standardized'] = df_processed['Site_Cleaned'].astype(str).str.lower().map(site_mapping)\n",
    "    # df_processed['Site_Standardized'].fillna('UnknownSite_Unmapped', inplace=True) # Or some other category\n",
    "\n",
    "    expected_sites_final = ['MS', 'LP', 'BK', 'UnknownSite'] # Final expected categories\n",
    "\n",
    "    # Check for any sites not fitting the expected categories after mapping\n",
    "    current_standardized_sites = df_processed['Site_Standardized'].unique()\n",
    "    unexpected_standardized_sites = [site for site in current_standardized_sites if site not in expected_sites_final]\n",
    "\n",
    "    if unexpected_standardized_sites:\n",
    "        print(f\"Warning: Unexpected 'Site_Standardized' values found after mapping: {unexpected_standardized_sites}\")\n",
    "        print(\"These might need to be added to the site_mapping or handled as a separate category.\")\n",
    "        # For now, let's map remaining unexpected ones to 'UnknownSite' for simplicity in this example\n",
    "        df_processed.loc[df_processed['Site_Standardized'].isin(unexpected_standardized_sites), 'Site_Standardized'] = 'UnknownSite_Mapped'\n",
    "\n",
    "    print(\"\\nUnique 'Site_Standardized' values AFTER detailed refinement and mapping:\")\n",
    "    print(df_processed['Site_Standardized'].value_counts())\n",
    "\n",
    "    snapshot6_path = os.path.join(snapshot_dir, 'snapshot_6_after_Site_Standardization.html')\n",
    "    df_processed.head(rows_for_snapshot).to_html(snapshot6_path, escape=False, index=False)\n",
    "    print(f\"\\nSnapshot 6: First {rows_for_snapshot} rows (showing 'Site_Standardized') saved to '{snapshot6_path}'\")\n",
    "else:\n",
    "    print(\"Warning: 'Site_Cleaned' column not found. Skipping detailed Site refinement.\")\n",
    "print(\"-\" * 50)"
   ],
   "id": "33da3b2110e0e3a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.h: Detailed `Site` Column Refinement ---\n",
      "Warning: 'Site_Cleaned' column not found. Skipping detailed Site refinement.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:20:37.710357Z",
     "start_time": "2025-06-01T08:20:37.438672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_sites = df_processed['Site'].unique()\n",
    "print(unique_sites)\n"
   ],
   "id": "99636fe3b89e6ea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LP' 'BK' 'MS' 'UnknownSite']\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:32:14.666622Z",
     "start_time": "2025-06-01T08:32:13.599281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# showing datas wich Site in them is unknown\n",
    "unknown_count = (df_processed['Site'] == 'UnknownSite').sum()\n",
    "print(f\"Number of 'UnknownSite' entries: {unknown_count}\")\n",
    "\n",
    "# 2. Show table with rows that have 'UnknownSite'\n",
    "unknown_rows = df_processed[df_processed['Site'] == 'UnknownSite']\n",
    "print(\"Rows with 'UnknownSite':\")\n",
    "print(unknown_rows)\n",
    "# snapshot_unknown_path = os.path.join(snapshot_dir, 'snapshot_unknownsite.html')\n",
    "# df_unknown = df_processed[df_processed['Site'] == 'UnknownSite']\n",
    "# df_unknown = df[df['Site'] == 'UnknownSite'].head(rows_for_snapshot)\n",
    "\n",
    "\n",
    "# Save filtered table to HTML\n",
    "# df_unknown.to_html(snapshot_unknown_path, escape=False, index=False)\n",
    "\n",
    "\n",
    "snapshot7_path = os.path.join(snapshot_dir, 'snapshot_7_unknownsites.html')\n",
    "unknown_rows.head(rows_for_snapshot).to_html(snapshot7_path, escape=False, index=False)\n",
    "print(f\"\\nSnapshot 7: First {rows_for_snapshot} rows (showing 'Site_Standardized') saved to '{snapshot7_path}'\")"
   ],
   "id": "649f6cb168033381",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'UnknownSite' entries: 704888\n",
      "Rows with 'UnknownSite':\n",
      "          OrderId                           TransactionId DateOfService  \\\n",
      "1193     11212600        d53a378d399c4de8a3d63c82f6f6e9fe    2020-01-07   \n",
      "1200     11212719        c842569a7de84017ae3bc56a8e2a9c0f    2020-01-07   \n",
      "1202     11212733        f5486c5957b54098a70c7b75dcd292d2    2020-01-07   \n",
      "1204     11328252        0c65b7caba4042438cb047a0c37ab68d    2020-01-07   \n",
      "1217     11212904        79d8f18ab19b4954af0bebbb1f2e7b17    2020-01-07   \n",
      "...           ...                                     ...           ...   \n",
      "6536406  21292966  9b50982436e842959afaa7bd71465a6b_order    2023-12-21   \n",
      "6536412  21053392  f484fca48d884ebc87e3fbe1eda43a8a_order    2023-12-21   \n",
      "6536503  21558501        ddc7942370f345e0a13fd526bd22aadc    2023-12-21   \n",
      "6536652  21315597  c664aaff783d43feade8f9e275826c50_order    2023-12-21   \n",
      "6536827  21256212        e4f7a3719bf84c7da81c44685701457e    2023-12-21   \n",
      "\n",
      "                DateOfOrder  OrderQty           MenuName  MenuPrice  \\\n",
      "1193    2019-12-13 11:54:44         1  Menü A (BO-Mitte)        380   \n",
      "1200    2019-12-13 11:55:16         1  Menü A (BO-Mitte)        380   \n",
      "1202    2019-12-13 11:55:20         1   Mittagessen (BK)        335   \n",
      "1204    2020-01-03 15:01:52         1       Menü A (SZG)        389   \n",
      "1217    2019-12-13 11:56:14         1   Mittagessen (BK)        335   \n",
      "...                     ...       ...                ...        ...   \n",
      "6536406 2023-12-06 11:24:03         1  Mittagessen (LP1)        390   \n",
      "6536412 2023-11-20 11:35:44         1  Mittagessen (LP1)        390   \n",
      "6536503 2023-12-18 08:30:10         1  Mittagessen (LP1)        390   \n",
      "6536652 2023-12-11 09:38:01         1  Mittagessen (LP1)        390   \n",
      "6536827 2023-12-03 12:24:26         1  Mittagessen (LP1)        390   \n",
      "\n",
      "         MenuSubsidy      BookingNr              GroupName  CanceledQty  \\\n",
      "1193               0  597-142641298             Bochum Abo            0   \n",
      "1200               0  597-140412823             Bochum Abo            0   \n",
      "1202               0  714-139100216        Hamm Bonh 3,95€            0   \n",
      "1204               0  712-139876769  Gelsenkirchen Schüler            0   \n",
      "1217               0  714-139101218        Hamm Bonh 3,95€            0   \n",
      "...              ...            ...                    ...          ...   \n",
      "6536406            0  371-172027044                 3,90 €            0   \n",
      "6536412            0  371-172028046                 3,90 €            0   \n",
      "6536503            0  371-172029048                 3,90 €            0   \n",
      "6536652            0  371-172030050                 3,90 €            0   \n",
      "6536827            0  371-172031052                 3,90 €            0   \n",
      "\n",
      "        DateOfCancel         Site SchoolID  NeededMeals   MenuName_Cleaned  \n",
      "1193             NaT  UnknownSite   SCH015            1  menü a (bo-mitte)  \n",
      "1200             NaT  UnknownSite   SCH015            1  menü a (bo-mitte)  \n",
      "1202             NaT  UnknownSite   SCH018            1   mittagessen (bk)  \n",
      "1204             NaT  UnknownSite   SCH019            1       menü a (szg)  \n",
      "1217             NaT  UnknownSite   SCH018            1   mittagessen (bk)  \n",
      "...              ...          ...      ...          ...                ...  \n",
      "6536406          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536412          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536503          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536652          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536827          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "\n",
      "[704888 rows x 16 columns]\n",
      "\n",
      "Snapshot 7: First 1000 rows (showing 'Site_Standardized') saved to 'results/snapshots/snapshot_7_unknownsites.html'\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:01:25.784284Z",
     "start_time": "2025-06-01T08:01:25.736389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_processed.head())\n",
    "snapshot5_path = os.path.join(snapshot_dir, 'snapshot_5_after_basic_MenuName_cleaning.html')\n",
    "df_processed.head(rows_for_snapshot).to_html(snapshot5_path, escape=False, index=False)"
   ],
   "id": "f9c09d9b9850635a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OrderId                     TransactionId DateOfService  \\\n",
      "0  11518978  4c5060636f584ef9a1effa77282755f5    2020-01-02   \n",
      "1  11285143  68472c70b9c84fb784834ecc257827d7    2020-01-02   \n",
      "2  11285146  7262eace0d104592b1269e38f5b45ec1    2020-01-02   \n",
      "3  11285152  8e451931e8fc4554869c3e4533b65e23    2020-01-02   \n",
      "4  11285155  bfa8fa0812ee40baa98e5aaf52d30e0b    2020-01-02   \n",
      "\n",
      "          DateOfOrder  OrderQty                     MenuName  MenuPrice  \\\n",
      "0 2020-02-05 11:54:08         1             Mittagessen (Gs)        310   \n",
      "1 2019-12-16 10:30:51         1  Smart Eating Buffet (WGrus)          0   \n",
      "2 2019-12-16 10:31:33         1  Smart Eating Buffet (WGrus)        290   \n",
      "3 2019-12-16 10:32:05         1  Smart Eating Buffet (WGrus)          0   \n",
      "4 2019-12-16 10:32:31         1  Smart Eating Buffet (WGrus)        290   \n",
      "\n",
      "   MenuSubsidy      BookingNr                       GroupName  CanceledQty  \\\n",
      "0            0   349-88220481        xxx3,45€ normal 5T (68€)            0   \n",
      "1          350  248-141751492          Steinfurt Abo ermäßigt            0   \n",
      "2           60   248-77489928  Westerkappeln Grundschüler Abo            0   \n",
      "3          350   248-77558043          Steinfurt Abo ermäßigt            0   \n",
      "4           60   248-77420774  Westerkappeln Grundschüler Abo            0   \n",
      "\n",
      "  DateOfCancel Site SchoolID  NeededMeals  \n",
      "0          NaT   LP   SCH001            1  \n",
      "1          NaT   BK   SCH002            1  \n",
      "2          NaT   BK   SCH002            1  \n",
      "3          NaT   BK   SCH002            1  \n",
      "4          NaT   BK   SCH002            1  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T08:35:17.852282Z",
     "start_time": "2025-06-01T08:35:05.777225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 2.i: Advanced `MenuName` Standardization (Iterative Process) ---\n",
    "print(\"\\n--- Step 2.i: Advanced `MenuName` Standardization ---\")\n",
    "if 'MenuName_Cleaned' in df_processed.columns:\n",
    "    print(f\"Starting advanced MenuName standardization. Unique names so far: {df_processed['MenuName_Cleaned'].nunique()}\")\n",
    "\n",
    "    # Strategy 1: Further Text Normalization (example: remove special characters beyond basic strip/lower)\n",
    "    # This regex keeps alphanumeric chars, german umlauts, and spaces. Adjust as needed.\n",
    "    df_processed['MenuName_Normalized'] = df_processed['MenuName_Cleaned'].astype(str).apply(\n",
    "        lambda x: re.sub(r'[^a-zA-Z0-9äöüÄÖÜß\\s]', '', x) # Keep relevant chars\n",
    "    )\n",
    "    df_processed['MenuName_Normalized'] = df_processed['MenuName_Normalized'].str.replace(r'\\s+', ' ', regex=True).str.strip() # Replace multiple spaces with one\n",
    "\n",
    "    print(f\"Unique names after further normalization (alphanumeric, umlauts, spaces only): {df_processed['MenuName_Normalized'].nunique()}\")\n",
    "\n",
    "    # Strategy 2: Rule-based Mapping (Example - this needs to be built based on your data)\n",
    "    # This is highly iterative. You'd identify common patterns and map them.\n",
    "    # Example: (You will need to inspect your data to create meaningful rules)\n",
    "    menu_name_mapping = {\n",
    "        'spaghetti bolognese': 'spaghetti bolo',\n",
    "        'spagetti bolognese': 'spaghetti bolo',\n",
    "        'spag. bolo': 'spaghetti bolo',\n",
    "        'currywurst mit pommes': 'currywurst pommes',\n",
    "        'currywurst m. pommes': 'currywurst pommes',\n",
    "        'gemüsepfanne vegetarisch': 'gemüsepfanne veg',\n",
    "        'menü a': 'menü a standard', # Example if 'menü a' has variations\n",
    "        'menu a': 'menü a standard',\n",
    "        # ... add many more rules based on your data inspection ...\n",
    "    }\n",
    "    # Apply the mapping to the 'MenuName_Normalized' column\n",
    "    # Create a new column for the mapped names to preserve the previous step\n",
    "    df_processed['MenuName_Standardized'] = df_processed['MenuName_Normalized'].replace(menu_name_mapping)\n",
    "\n",
    "    print(f\"Unique names after example rule-based mapping: {df_processed['MenuName_Standardized'].nunique()}\")\n",
    "\n",
    "    if df_processed['MenuName_Standardized'].nunique() > 50:\n",
    "        print(\"First 20 unique 'MenuName_Standardized' (sample after mapping):\")\n",
    "        print(df_processed['MenuName_Standardized'].unique()[:20])\n",
    "    else:\n",
    "        print(\"Unique 'MenuName_Standardized':\")\n",
    "        print(df_processed['MenuName_Standardized'].unique())\n",
    "\n",
    "    print(\"\\nIMPORTANT: 'MenuName' standardization is an iterative process.\")\n",
    "    print(\"You'll likely need to: \")\n",
    "    print(\"  1. Analyze frequent variations of 'MenuName_Normalized' or 'MenuName_Standardized'.\")\n",
    "    print(\"  2. Expand your `menu_name_mapping` dictionary.\")\n",
    "    print(\"  3. Re-run and check unique counts until they are manageable and meaningful.\")\n",
    "    print(\"  Consider techniques like grouping by common substrings if appropriate.\")\n",
    "\n",
    "    snapshot7_path = os.path.join(snapshot_dir, 'snapshot_7_after_MenuName_Standardization.html')\n",
    "    df_processed.head(rows_for_snapshot).to_html(snapshot7_path, escape=False, index=False)\n",
    "    print(f\"\\nSnapshot 7: First {rows_for_snapshot} rows (showing 'MenuName_Standardized') saved to '{snapshot7_path}'\")\n",
    "else:\n",
    "    print(\"Warning: 'MenuName_Cleaned' column not found. Skipping advanced MenuName standardization.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"\\n--- Data Cleaning & Preprocessing (Core Steps Completed) ---\")\n",
    "print(\"Cleaned DataFrame shape (after site and menu name initial standardization):\", df_processed.shape)\n",
    "\n",
    "final_snapshot_path_cleaned = os.path.join(snapshot_dir, 'snapshot_final_cleaned_dataframe.html')\n",
    "df_processed.head(rows_for_snapshot).to_html(final_snapshot_path_cleaned, escape=False, index=False)\n",
    "print(f\"\\nFinal Cleaned Snapshot: First {rows_for_snapshot} rows of the processed DataFrame saved to '{final_snapshot_path_cleaned}'\")\n",
    "\n",
    "print(\"\\nProcessed DataFrame info (final cleaned):\")\n",
    "df_processed.info(verbose=True, show_counts=True)\n",
    "print(\"-\" * 50)"
   ],
   "id": "10c91ee1b6fe9ce8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.i: Advanced `MenuName` Standardization ---\n",
      "Starting advanced MenuName standardization. Unique names so far: 322\n",
      "Unique names after further normalization (alphanumeric, umlauts, spaces only): 319\n",
      "Unique names after example rule-based mapping: 319\n",
      "First 20 unique 'MenuName_Standardized' (sample after mapping):\n",
      "['mittagessen gs' 'smart eating buffet wgrus' 'mittagessen bs'\n",
      " 'smart eating buffet primus' 'menü a hss' 'smart eating buffet gymbo'\n",
      " 'smart eating buffet brs' 'menü b1' 'smart eating buffet ema'\n",
      " 'mittagessen ml' 'smarteating buffet egm' 'dgemenü mg' 'menü a bomitte'\n",
      " 'menü a marbi' 'smart eating buffet gma' 'mittagessen bk'\n",
      " 'menü a standard' 'menü a szg' 'salatbar mpg' 'menü b2 lui']\n",
      "\n",
      "IMPORTANT: 'MenuName' standardization is an iterative process.\n",
      "You'll likely need to: \n",
      "  1. Analyze frequent variations of 'MenuName_Normalized' or 'MenuName_Standardized'.\n",
      "  2. Expand your `menu_name_mapping` dictionary.\n",
      "  3. Re-run and check unique counts until they are manageable and meaningful.\n",
      "  Consider techniques like grouping by common substrings if appropriate.\n",
      "\n",
      "Snapshot 7: First 1000 rows (showing 'MenuName_Standardized') saved to 'results/snapshots/snapshot_7_after_MenuName_Standardization.html'\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Data Cleaning & Preprocessing (Core Steps Completed) ---\n",
      "Cleaned DataFrame shape (after site and menu name initial standardization): (5674222, 18)\n",
      "\n",
      "Final Cleaned Snapshot: First 1000 rows of the processed DataFrame saved to 'results/snapshots/snapshot_final_cleaned_dataframe.html'\n",
      "\n",
      "Processed DataFrame info (final cleaned):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5674222 entries, 0 to 6538738\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   OrderId                5674222 non-null  int64         \n",
      " 1   TransactionId          5674222 non-null  object        \n",
      " 2   DateOfService          5674222 non-null  datetime64[ns]\n",
      " 3   DateOfOrder            5674222 non-null  datetime64[ns]\n",
      " 4   OrderQty               5674222 non-null  int64         \n",
      " 5   MenuName               5674222 non-null  object        \n",
      " 6   MenuPrice              5674222 non-null  int64         \n",
      " 7   MenuSubsidy            5674222 non-null  int64         \n",
      " 8   BookingNr              5674222 non-null  object        \n",
      " 9   GroupName              5674222 non-null  object        \n",
      " 10  CanceledQty            5674222 non-null  int64         \n",
      " 11  DateOfCancel           69962 non-null    datetime64[ns]\n",
      " 12  Site                   5674222 non-null  object        \n",
      " 13  SchoolID               5674222 non-null  object        \n",
      " 14  NeededMeals            5674222 non-null  int64         \n",
      " 15  MenuName_Cleaned       5674222 non-null  object        \n",
      " 16  MenuName_Normalized    5674222 non-null  object        \n",
      " 17  MenuName_Standardized  5674222 non-null  object        \n",
      "dtypes: datetime64[ns](3), int64(6), object(9)\n",
      "memory usage: 822.5+ MB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
