{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T09:19:50.075874Z",
     "start_time": "2025-06-01T09:19:49.140546Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:19:50.085899Z",
     "start_time": "2025-06-01T09:19:50.083714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_dir = 'results'\n",
    "snapshot_dir = os.path.join(results_dir, 'snapshots')\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(f\"Created directory: {results_dir}\")\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.makedirs(snapshot_dir)\n",
    "    print(f\"Created directory: {snapshot_dir}\")\n",
    "print(\"-\" * 50)\n"
   ],
   "id": "6fbf78a7de27498f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:03.568196Z",
     "start_time": "2025-06-01T09:19:50.100560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = 'venvx/AnnonymData.csv'\n",
    "rows_for_snapshot = 1000\n",
    "try:\n",
    "    # For large files, be mindful of memory.\n",
    "    # We'll load it directly for now, but consider 'chunksize' or 'dtype' optimization if needed.\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded data from '{file_path}'.\")\n",
    "    print(f\"Dataset shape: {df.shape}\") # (rows, columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    snapshot1_path = os.path.join(snapshot_dir, 'snapshot_1_raw_loaded_dataset.html')\n",
    "    df.head(rows_for_snapshot).to_html(snapshot1_path, escape=False, index=False) # escape=False for cleaner HTML, index=False to not write pandas index\n",
    "    print(f\"\\nSnapshot 1: First {rows_for_snapshot} rows of the raw loaded dataset saved to '{snapshot1_path}'\")\n",
    "\n",
    "\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nDataset info:\")\n",
    "    df.info(verbose=True, show_counts=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at '{file_path}'. Please check the path and filename.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "    exit()\n",
    "print(\"-\" * 50)"
   ],
   "id": "9ebe4bee89b7866f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data from 'venvx/AnnonymData.csv'.\n",
      "Dataset shape: (6538739, 14)\n",
      "\n",
      "Snapshot 1: First 1000 rows of the raw loaded dataset saved to 'results/snapshots/snapshot_1_raw_loaded_dataset.html'\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "    OrderId                     TransactionId DateOfService  \\\n",
      "0  11518978  4c5060636f584ef9a1effa77282755f5    2020-01-02   \n",
      "1  11285143  68472c70b9c84fb784834ecc257827d7    2020-01-02   \n",
      "2  11285146  7262eace0d104592b1269e38f5b45ec1    2020-01-02   \n",
      "3  11285152  8e451931e8fc4554869c3e4533b65e23    2020-01-02   \n",
      "4  11285155  bfa8fa0812ee40baa98e5aaf52d30e0b    2020-01-02   \n",
      "\n",
      "           DateOfOrder  OrderQty                     MenuName MenuPrice  \\\n",
      "0  2020-02-05 11:54:08         1             Mittagessen (Gs)      3,10   \n",
      "1  2019-12-16 10:30:51         1  Smart Eating Buffet (WGrus)      0,00   \n",
      "2  2019-12-16 10:31:33         1  Smart Eating Buffet (WGrus)      2,90   \n",
      "3  2019-12-16 10:32:05         1  Smart Eating Buffet (WGrus)      0,00   \n",
      "4  2019-12-16 10:32:31         1  Smart Eating Buffet (WGrus)      2,90   \n",
      "\n",
      "  MenuSubsidy      BookingNr                       GroupName  CanceledQty  \\\n",
      "0        0,00   349-88220481        xxx3,45€ normal 5T (68€)            0   \n",
      "1        3,50  248-141751492          Steinfurt Abo ermäßigt            0   \n",
      "2        0,60   248-77489928  Westerkappeln Grundschüler Abo            0   \n",
      "3        3,50   248-77558043          Steinfurt Abo ermäßigt            0   \n",
      "4        0,60   248-77420774  Westerkappeln Grundschüler Abo            0   \n",
      "\n",
      "  DateOfCancel Site SchoolID  \n",
      "0          NaN   LP   SCH001  \n",
      "1          NaN   BK   SCH002  \n",
      "2          NaN   BK   SCH002  \n",
      "3          NaN   BK   SCH002  \n",
      "4          NaN   BK   SCH002  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6538739 entries, 0 to 6538738\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   OrderId        6538739 non-null  int64 \n",
      " 1   TransactionId  6538739 non-null  object\n",
      " 2   DateOfService  6538739 non-null  object\n",
      " 3   DateOfOrder    6538739 non-null  object\n",
      " 4   OrderQty       6538739 non-null  int64 \n",
      " 5   MenuName       6538739 non-null  object\n",
      " 6   MenuPrice      6538739 non-null  object\n",
      " 7   MenuSubsidy    6538739 non-null  object\n",
      " 8   BookingNr      6538739 non-null  object\n",
      " 9   GroupName      6538739 non-null  object\n",
      " 10  CanceledQty    6538739 non-null  int64 \n",
      " 11  DateOfCancel   934479 non-null   object\n",
      " 12  Site           5692727 non-null  object\n",
      " 13  SchoolID       6538739 non-null  object\n",
      "dtypes: int64(3), object(11)\n",
      "memory usage: 698.4+ MB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:05.248739Z",
     "start_time": "2025-06-01T09:20:03.585262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"--- Step 2: Initial Data Cleaning & Preprocessing ---\")\n",
    "df_processed = df.copy()\n",
    "\n",
    "date_columns = ['DateOfService', 'DateOfOrder', 'DateOfCancel']\n",
    "for col in date_columns:\n",
    "    if col in df_processed.columns:\n",
    "        # Attempt conversion, coercing errors will turn unparseable dates into NaT (Not a Time)\n",
    "        df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
    "        print(f\"Column '{col}' converted. NaNs introduced by coercion: {df_processed[col].isnull().sum()}\")\n",
    "    else:\n",
    "        print(f\"Warning: Date column '{col}' not found.\")"
   ],
   "id": "7db983729470c8ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Initial Data Cleaning & Preprocessing ---\n",
      "Column 'DateOfService' converted. NaNs introduced by coercion: 0\n",
      "Column 'DateOfOrder' converted. NaNs introduced by coercion: 0\n",
      "Column 'DateOfCancel' converted. NaNs introduced by coercion: 5604260\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:12.192929Z",
     "start_time": "2025-06-01T09:20:05.263472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nConverting financial columns ('MenuPrice', 'MenuSubsidy') to numeric...\")\n",
    "financial_columns = ['MenuPrice', 'MenuSubsidy']\n",
    "for col in financial_columns:\n",
    "    if col in df_processed.columns:\n",
    "        if df_processed[col].dtype == 'object': # Only process if it's an object type\n",
    "            # Remove currency symbols (e.g., €, $, £) and commas (as thousands separators)\n",
    "            # This regex is an example, adjust if your currency format is different\n",
    "            df_processed[col] = df_processed[col].astype(str).str.replace(r'[€\\$£,]', '', regex=True)\n",
    "            # Convert to numeric, coercing errors.\n",
    "            # If your numbers use ',' as decimal (e.g., German format), you'd first remove '.', then replace ',' with '.'\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "            print(f\"Column '{col}' converted to numeric. NaNs introduced: {df_processed[col].isnull().sum()}\")\n",
    "        elif pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "            print(f\"Column '{col}' is already numeric.\")\n",
    "        else:\n",
    "            print(f\"Column '{col}' is of type {df_processed[col].dtype} and was not processed as a typical currency string.\")\n",
    "    else:\n",
    "        print(f\"Warning: Financial column '{col}' not found.\")"
   ],
   "id": "ee332f452f40ca0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting financial columns ('MenuPrice', 'MenuSubsidy') to numeric...\n",
      "Column 'MenuPrice' converted to numeric. NaNs introduced: 0\n",
      "Column 'MenuSubsidy' converted to numeric. NaNs introduced: 0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:12.210253Z",
     "start_time": "2025-06-01T09:20:12.207478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nEnsuring 'OrderQty' and 'CanceledQty' are numeric...\")\n",
    "for col in ['OrderQty', 'CanceledQty']:\n",
    "    if col in df_processed.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
    "            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "            print(f\"Column '{col}' converted to numeric. NaNs introduced: {df_processed[col].isnull().sum()}\")\n",
    "        else:\n",
    "            print(f\"Column '{col}' is already numeric.\")\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found.\")"
   ],
   "id": "c9159035998f25b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensuring 'OrderQty' and 'CanceledQty' are numeric...\n",
      "Column 'OrderQty' is already numeric.\n",
      "Column 'CanceledQty' is already numeric.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:12.575523Z",
     "start_time": "2025-06-01T09:20:12.258341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if 'Site' in df_processed.columns:\n",
    "    missing_site_percentage = df_processed['Site'].isnull().mean() * 100\n",
    "    print(f\"\\nMissing values in 'Site': {missing_site_percentage:.2f}%\")\n",
    "    # Example: df_processed['Site'].fillna('Unknown', inplace=True)\n",
    "    # Or, if you know the main sites are MS, LP, BK, you might investigate if missing sites can be inferred.\n",
    "    # For now, let's fill with 'Unknown' as a placeholder strategy.\n",
    "    df_processed['Site'] = df_processed['Site'].fillna('UnknownSite')\n",
    "    print(\"Filled missing 'Site' values with 'UnknownSite'.\")"
   ],
   "id": "12679da622ba14ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in 'Site': 12.94%\n",
      "Filled missing 'Site' values with 'UnknownSite'.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:13.523505Z",
     "start_time": "2025-06-01T09:20:12.587610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nFiltering out irrelevant orders...\")\n",
    "initial_rows = len(df_processed)\n",
    "\n",
    "# Condition 1: Orders where DateOfCancel is after DateOfService\n",
    "if 'DateOfCancel' in df_processed.columns and 'DateOfService' in df_processed.columns:\n",
    "    condition1_filter = (df_processed['DateOfCancel'].notna()) & \\\n",
    "                        (df_processed['DateOfService'].notna()) & \\\n",
    "                        (df_processed['DateOfCancel'] > df_processed['DateOfService'])\n",
    "    rows_to_drop_cond1 = df_processed[condition1_filter]\n",
    "    if not rows_to_drop_cond1.empty:\n",
    "        print(f\"Found {len(rows_to_drop_cond1)} orders where DateOfCancel is after DateOfService. These will be dropped.\")\n",
    "        df_processed = df_processed[~condition1_filter]\n",
    "    else:\n",
    "        print(\"No orders found where DateOfCancel is after DateOfService.\")"
   ],
   "id": "8a18505926feec3b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering out irrelevant orders...\n",
      "Found 70970 orders where DateOfCancel is after DateOfService. These will be dropped.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:14.010056Z",
     "start_time": "2025-06-01T09:20:13.535602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Condition 2: Orders where OrderQty < CanceledQty\n",
    "# Ensure both columns are numeric and handle potential NaNs before comparison\n",
    "if 'OrderQty' in df_processed.columns and 'CanceledQty' in df_processed.columns:\n",
    "    # Fill NaNs with 0 for comparison, assuming NaN in Qty means 0 for this specific filter logic\n",
    "    order_qty_filled = df_processed['OrderQty'].fillna(0)\n",
    "    canceled_qty_filled = df_processed['CanceledQty'].fillna(0)\n",
    "\n",
    "    condition2_filter = order_qty_filled < canceled_qty_filled\n",
    "    rows_to_drop_cond2 = df_processed[condition2_filter]\n",
    "    if not rows_to_drop_cond2.empty:\n",
    "        print(f\"Found {len(rows_to_drop_cond2)} orders where OrderQty < CanceledQty. These will be dropped.\")\n",
    "        df_processed = df_processed[~condition2_filter]\n",
    "    else:\n",
    "        print(\"No orders found where OrderQty < CanceledQty.\")\n",
    "\n",
    "rows_after_filtering = len(df_processed)\n",
    "print(f\"Rows dropped due to filtering: {initial_rows - rows_after_filtering}\")\n",
    "print(f\"Dataset shape after filtering: {df_processed.shape}\")\n"
   ],
   "id": "2206598b7b65abd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 793547 orders where OrderQty < CanceledQty. These will be dropped.\n",
      "Rows dropped due to filtering: 864517\n",
      "Dataset shape after filtering: (5674222, 14)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:14.375945Z",
     "start_time": "2025-06-01T09:20:14.027270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# f. Create the Target Variable: `NeededMeals`\n",
    "# NeededMeals = OrderQty - CanceledQty (after filtering and ensuring NaNs in Qty are handled)\n",
    "print(\"\\nCreating the target variable 'NeededMeals'...\")\n",
    "if 'OrderQty' in df_processed.columns and 'CanceledQty' in df_processed.columns:\n",
    "    # Assuming that if CanceledQty is NaN for an order that wasn't filtered out, it means 0 cancellations for that order.\n",
    "    # And if OrderQty is NaN (should be rare after initial checks), treat as 0 for this calculation.\n",
    "    df_processed['OrderQty_filled'] = df_processed['OrderQty'].fillna(0)\n",
    "    df_processed['CanceledQty_filled'] = df_processed['CanceledQty'].fillna(0)\n",
    "\n",
    "    df_processed['NeededMeals'] = df_processed['OrderQty_filled'] - df_processed['CanceledQty_filled']\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    df_processed.drop(columns=['OrderQty_filled', 'CanceledQty_filled'], inplace=True)\n",
    "\n",
    "    print(\"'NeededMeals' column created.\")\n",
    "    print(\"Summary of 'NeededMeals':\")\n",
    "    print(df_processed['NeededMeals'].describe())\n",
    "\n",
    "    # Sanity check: NeededMeals should not be negative if OrderQty < CanceledQty was filtered.\n",
    "    # However, if OrderQty was 0 and CanceledQty was 0, NeededMeals is 0.\n",
    "    # If OrderQty was 0 and CanceledQty was 1 (bank account not covered), this record should have been filtered.\n",
    "    if not df_processed[df_processed['NeededMeals'] < 0].empty:\n",
    "        print(f\"Warning: Found {len(df_processed[df_processed['NeededMeals'] < 0])} records with negative NeededMeals. Review filtering logic.\")\n",
    "        print(df_processed[df_processed['NeededMeals'] < 0][['OrderQty', 'CanceledQty', 'NeededMeals']].head())\n",
    "    else:\n",
    "        print(\"No negative 'NeededMeals' found after calculation, which is good.\")\n",
    "else:\n",
    "    print(\"Could not create 'NeededMeals' as 'OrderQty' or 'CanceledQty' is missing.\")"
   ],
   "id": "91788b25b730264b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating the target variable 'NeededMeals'...\n",
      "'NeededMeals' column created.\n",
      "Summary of 'NeededMeals':\n",
      "count    5.674222e+06\n",
      "mean     9.985110e-01\n",
      "std      4.098289e-01\n",
      "min      0.000000e+00\n",
      "25%      1.000000e+00\n",
      "50%      1.000000e+00\n",
      "75%      1.000000e+00\n",
      "max      1.200000e+02\n",
      "Name: NeededMeals, dtype: float64\n",
      "No negative 'NeededMeals' found after calculation, which is good.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:15.949033Z",
     "start_time": "2025-06-01T09:20:14.391366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Addressing 'MenuName' Variations (Initial Pass) ---\")\n",
    "if 'MenuName' in df_processed.columns:\n",
    "    unique_menu_names_count_before = df_processed['MenuName'].nunique()\n",
    "    print(f\"Number of unique 'MenuName' entries before any cleaning: {unique_menu_names_count_before}\")\n",
    "\n",
    "    # Perform basic cleaning first\n",
    "    df_processed['MenuName_Cleaned'] = df_processed['MenuName'].astype(str).str.lower().str.strip()\n",
    "    unique_menu_names_count_after_basic = df_processed['MenuName_Cleaned'].nunique()\n",
    "    print(f\"Number of unique 'MenuName_Cleaned' entries after basic cleaning (lowercase, strip): {unique_menu_names_count_after_basic}\")\n",
    "\n",
    "    if unique_menu_names_count_after_basic > 50:\n",
    "        print(\"First 20 unique 'MenuName_Cleaned' (sample after basic cleaning):\")\n",
    "        print(df_processed['MenuName_Cleaned'].unique()[:20])\n",
    "    else:\n",
    "        print(\"Unique 'MenuName_Cleaned' (after basic cleaning):\")\n",
    "        print(df_processed['MenuName_Cleaned'].unique())\n",
    "\n",
    "    snapshot5_path = os.path.join(snapshot_dir, 'snapshot_5_after_basic_MenuName_cleaning.html')\n",
    "    # Displaying head() with the new MenuName_Cleaned column\n",
    "    df_processed.head(rows_for_snapshot).to_html(snapshot5_path, escape=False, index=False)\n",
    "    print(f\"\\nSnapshot 5: First {rows_for_snapshot} rows (showing 'MenuName_Cleaned') saved to '{snapshot5_path}'\")\n",
    "else:\n",
    "    print(\"Warning: 'MenuName' column not found.\")\n",
    "\n",
    "print(\"\\n--- Initial Data Cleaning & Preprocessing (Part 1 - Basic Steps) Complete ---\")\n",
    "print(\"Current DataFrame shape:\", df_processed.shape)\n",
    "print(\"-\" * 50)\n"
   ],
   "id": "e433e7aadf89cb5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Addressing 'MenuName' Variations (Initial Pass) ---\n",
      "Number of unique 'MenuName' entries before any cleaning: 322\n",
      "Number of unique 'MenuName_Cleaned' entries after basic cleaning (lowercase, strip): 322\n",
      "First 20 unique 'MenuName_Cleaned' (sample after basic cleaning):\n",
      "['mittagessen (gs)' 'smart eating buffet (wgrus)' 'mittagessen (bs)'\n",
      " 'smart eating buffet (primus)' 'menü a (hss)'\n",
      " 'smart eating buffet (gymbo)' 'smart eating buffet (brs)' 'menü b1'\n",
      " 'smart eating buffet (ema)' 'mittagessen (ml)'\n",
      " 'smart-eating buffet (egm)' 'dge-menü (mg)' 'menü a (bo-mitte)'\n",
      " 'menü a (marbi)' 'smart eating buffet (gma)' 'mittagessen (bk)' 'menü a'\n",
      " 'menü a (szg)' 'salatbar (mpg)' 'menü b2 (lui)']\n",
      "\n",
      "Snapshot 5: First 1000 rows (showing 'MenuName_Cleaned') saved to 'results/snapshots/snapshot_5_after_basic_MenuName_cleaning.html'\n",
      "\n",
      "--- Initial Data Cleaning & Preprocessing (Part 1 - Basic Steps) Complete ---\n",
      "Current DataFrame shape: (5674222, 16)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:15.974205Z",
     "start_time": "2025-06-01T09:20:15.965779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 2.h: Detailed `Site` Column Refinement ---\n",
    "print(\"\\n--- Step 2.h: Detailed `Site` Column Refinement ---\")\n",
    "if 'Site_Cleaned' in df_processed.columns:\n",
    "    print(\"Unique 'Site_Cleaned' values BEFORE detailed refinement:\")\n",
    "    print(df_processed['Site_Cleaned'].unique())\n",
    "\n",
    "    # Define expected sites and a mapping for common variations\n",
    "    # This mapping should be expanded based on your actual data exploration\n",
    "    site_mapping = {\n",
    "        'ms': 'MS', 'muenster': 'MS', 'münster': 'MS',\n",
    "        'lp': 'LP', 'lippstadt': 'LP',\n",
    "        'bk': 'BK', 'bergkamen': 'BK',\n",
    "        # Add other variations you find to this dictionary\n",
    "        'unknownsite': 'UnknownSite' # Ensure 'UnknownSite' itself is consistently cased if it came from NaNs\n",
    "    }\n",
    "\n",
    "    # Apply mapping: Convert to lowercase first for case-insensitive mapping\n",
    "    df_processed['Site_Standardized'] = df_processed['Site_Cleaned'].astype(str).str.lower().map(site_mapping).fillna(df_processed['Site_Cleaned'])\n",
    "    # For values not in mapping, keep original (or map to 'UnknownSite' if preferred)\n",
    "    # A more robust approach for unmapped values:\n",
    "    # df_processed['Site_Standardized'] = df_processed['Site_Cleaned'].astype(str).str.lower().map(site_mapping)\n",
    "    # df_processed['Site_Standardized'].fillna('UnknownSite_Unmapped', inplace=True) # Or some other category\n",
    "\n",
    "    expected_sites_final = ['MS', 'LP', 'BK', 'UnknownSite'] # Final expected categories\n",
    "\n",
    "    # Check for any sites not fitting the expected categories after mapping\n",
    "    current_standardized_sites = df_processed['Site_Standardized'].unique()\n",
    "    unexpected_standardized_sites = [site for site in current_standardized_sites if site not in expected_sites_final]\n",
    "\n",
    "    if unexpected_standardized_sites:\n",
    "        print(f\"Warning: Unexpected 'Site_Standardized' values found after mapping: {unexpected_standardized_sites}\")\n",
    "        print(\"These might need to be added to the site_mapping or handled as a separate category.\")\n",
    "        # For now, let's map remaining unexpected ones to 'UnknownSite' for simplicity in this example\n",
    "        df_processed.loc[df_processed['Site_Standardized'].isin(unexpected_standardized_sites), 'Site_Standardized'] = 'UnknownSite_Mapped'\n",
    "\n",
    "    print(\"\\nUnique 'Site_Standardized' values AFTER detailed refinement and mapping:\")\n",
    "    print(df_processed['Site_Standardized'].value_counts())\n",
    "\n",
    "    snapshot6_path = os.path.join(snapshot_dir, 'snapshot_6_after_Site_Standardization.html')\n",
    "    df_processed.head(rows_for_snapshot).to_html(snapshot6_path, escape=False, index=False)\n",
    "    print(f\"\\nSnapshot 6: First {rows_for_snapshot} rows (showing 'Site_Standardized') saved to '{snapshot6_path}'\")\n",
    "else:\n",
    "    print(\"Warning: 'Site_Cleaned' column not found. Skipping detailed Site refinement.\")\n",
    "print(\"-\" * 50)"
   ],
   "id": "33da3b2110e0e3a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.h: Detailed `Site` Column Refinement ---\n",
      "Warning: 'Site_Cleaned' column not found. Skipping detailed Site refinement.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:16.223884Z",
     "start_time": "2025-06-01T09:20:16.000943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_sites = df_processed['Site'].unique()\n",
    "print(unique_sites)\n"
   ],
   "id": "99636fe3b89e6ea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LP' 'BK' 'MS' 'UnknownSite']\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:17.566682Z",
     "start_time": "2025-06-01T09:20:16.298129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# showing datas wich Site in them is unknown\n",
    "unknown_count = (df_processed['Site'] == 'UnknownSite').sum()\n",
    "print(f\"Number of 'UnknownSite' entries: {unknown_count}\")\n",
    "\n",
    "# 2. Show table with rows that have 'UnknownSite'\n",
    "unknown_rows = df_processed[df_processed['Site'] == 'UnknownSite']\n",
    "print(\"Rows with 'UnknownSite':\")\n",
    "print(unknown_rows)\n",
    "# snapshot_unknown_path = os.path.join(snapshot_dir, 'snapshot_unknownsite.html')\n",
    "# df_unknown = df_processed[df_processed['Site'] == 'UnknownSite']\n",
    "# df_unknown = df[df['Site'] == 'UnknownSite'].head(rows_for_snapshot)\n",
    "\n",
    "\n",
    "# Save filtered table to HTML\n",
    "# df_unknown.to_html(snapshot_unknown_path, escape=False, index=False)\n",
    "\n",
    "\n",
    "snapshot7_path = os.path.join(snapshot_dir, 'snapshot_7_unknownsites.html')\n",
    "unknown_rows.head(rows_for_snapshot).to_html(snapshot7_path, escape=False, index=False)\n",
    "print(f\"\\nSnapshot 7: First {rows_for_snapshot} rows (showing 'Site_Standardized') saved to '{snapshot7_path}'\")"
   ],
   "id": "649f6cb168033381",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'UnknownSite' entries: 704888\n",
      "Rows with 'UnknownSite':\n",
      "          OrderId                           TransactionId DateOfService  \\\n",
      "1193     11212600        d53a378d399c4de8a3d63c82f6f6e9fe    2020-01-07   \n",
      "1200     11212719        c842569a7de84017ae3bc56a8e2a9c0f    2020-01-07   \n",
      "1202     11212733        f5486c5957b54098a70c7b75dcd292d2    2020-01-07   \n",
      "1204     11328252        0c65b7caba4042438cb047a0c37ab68d    2020-01-07   \n",
      "1217     11212904        79d8f18ab19b4954af0bebbb1f2e7b17    2020-01-07   \n",
      "...           ...                                     ...           ...   \n",
      "6536406  21292966  9b50982436e842959afaa7bd71465a6b_order    2023-12-21   \n",
      "6536412  21053392  f484fca48d884ebc87e3fbe1eda43a8a_order    2023-12-21   \n",
      "6536503  21558501        ddc7942370f345e0a13fd526bd22aadc    2023-12-21   \n",
      "6536652  21315597  c664aaff783d43feade8f9e275826c50_order    2023-12-21   \n",
      "6536827  21256212        e4f7a3719bf84c7da81c44685701457e    2023-12-21   \n",
      "\n",
      "                DateOfOrder  OrderQty           MenuName  MenuPrice  \\\n",
      "1193    2019-12-13 11:54:44         1  Menü A (BO-Mitte)        380   \n",
      "1200    2019-12-13 11:55:16         1  Menü A (BO-Mitte)        380   \n",
      "1202    2019-12-13 11:55:20         1   Mittagessen (BK)        335   \n",
      "1204    2020-01-03 15:01:52         1       Menü A (SZG)        389   \n",
      "1217    2019-12-13 11:56:14         1   Mittagessen (BK)        335   \n",
      "...                     ...       ...                ...        ...   \n",
      "6536406 2023-12-06 11:24:03         1  Mittagessen (LP1)        390   \n",
      "6536412 2023-11-20 11:35:44         1  Mittagessen (LP1)        390   \n",
      "6536503 2023-12-18 08:30:10         1  Mittagessen (LP1)        390   \n",
      "6536652 2023-12-11 09:38:01         1  Mittagessen (LP1)        390   \n",
      "6536827 2023-12-03 12:24:26         1  Mittagessen (LP1)        390   \n",
      "\n",
      "         MenuSubsidy      BookingNr              GroupName  CanceledQty  \\\n",
      "1193               0  597-142641298             Bochum Abo            0   \n",
      "1200               0  597-140412823             Bochum Abo            0   \n",
      "1202               0  714-139100216        Hamm Bonh 3,95€            0   \n",
      "1204               0  712-139876769  Gelsenkirchen Schüler            0   \n",
      "1217               0  714-139101218        Hamm Bonh 3,95€            0   \n",
      "...              ...            ...                    ...          ...   \n",
      "6536406            0  371-172027044                 3,90 €            0   \n",
      "6536412            0  371-172028046                 3,90 €            0   \n",
      "6536503            0  371-172029048                 3,90 €            0   \n",
      "6536652            0  371-172030050                 3,90 €            0   \n",
      "6536827            0  371-172031052                 3,90 €            0   \n",
      "\n",
      "        DateOfCancel         Site SchoolID  NeededMeals   MenuName_Cleaned  \n",
      "1193             NaT  UnknownSite   SCH015            1  menü a (bo-mitte)  \n",
      "1200             NaT  UnknownSite   SCH015            1  menü a (bo-mitte)  \n",
      "1202             NaT  UnknownSite   SCH018            1   mittagessen (bk)  \n",
      "1204             NaT  UnknownSite   SCH019            1       menü a (szg)  \n",
      "1217             NaT  UnknownSite   SCH018            1   mittagessen (bk)  \n",
      "...              ...          ...      ...          ...                ...  \n",
      "6536406          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536412          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536503          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536652          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "6536827          NaT  UnknownSite   SCH038            1  mittagessen (lp1)  \n",
      "\n",
      "[704888 rows x 16 columns]\n",
      "\n",
      "Snapshot 7: First 1000 rows (showing 'Site_Standardized') saved to 'results/snapshots/snapshot_7_unknownsites.html'\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:17.634825Z",
     "start_time": "2025-06-01T09:20:17.591413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(df_processed.head())\n",
    "snapshot5_path = os.path.join(snapshot_dir, 'snapshot_5_after_basic_MenuName_cleaning.html')\n",
    "df_processed.head(rows_for_snapshot).to_html(snapshot5_path, escape=False, index=False)"
   ],
   "id": "f9c09d9b9850635a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    OrderId                     TransactionId DateOfService  \\\n",
      "0  11518978  4c5060636f584ef9a1effa77282755f5    2020-01-02   \n",
      "1  11285143  68472c70b9c84fb784834ecc257827d7    2020-01-02   \n",
      "2  11285146  7262eace0d104592b1269e38f5b45ec1    2020-01-02   \n",
      "3  11285152  8e451931e8fc4554869c3e4533b65e23    2020-01-02   \n",
      "4  11285155  bfa8fa0812ee40baa98e5aaf52d30e0b    2020-01-02   \n",
      "\n",
      "          DateOfOrder  OrderQty                     MenuName  MenuPrice  \\\n",
      "0 2020-02-05 11:54:08         1             Mittagessen (Gs)        310   \n",
      "1 2019-12-16 10:30:51         1  Smart Eating Buffet (WGrus)          0   \n",
      "2 2019-12-16 10:31:33         1  Smart Eating Buffet (WGrus)        290   \n",
      "3 2019-12-16 10:32:05         1  Smart Eating Buffet (WGrus)          0   \n",
      "4 2019-12-16 10:32:31         1  Smart Eating Buffet (WGrus)        290   \n",
      "\n",
      "   MenuSubsidy      BookingNr                       GroupName  CanceledQty  \\\n",
      "0            0   349-88220481        xxx3,45€ normal 5T (68€)            0   \n",
      "1          350  248-141751492          Steinfurt Abo ermäßigt            0   \n",
      "2           60   248-77489928  Westerkappeln Grundschüler Abo            0   \n",
      "3          350   248-77558043          Steinfurt Abo ermäßigt            0   \n",
      "4           60   248-77420774  Westerkappeln Grundschüler Abo            0   \n",
      "\n",
      "  DateOfCancel Site SchoolID  NeededMeals             MenuName_Cleaned  \n",
      "0          NaT   LP   SCH001            1             mittagessen (gs)  \n",
      "1          NaT   BK   SCH002            1  smart eating buffet (wgrus)  \n",
      "2          NaT   BK   SCH002            1  smart eating buffet (wgrus)  \n",
      "3          NaT   BK   SCH002            1  smart eating buffet (wgrus)  \n",
      "4          NaT   BK   SCH002            1  smart eating buffet (wgrus)  \n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:29.022661Z",
     "start_time": "2025-06-01T09:20:17.646323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Step 2.i: Advanced `MenuName` Standardization (Iterative Process) ---\n",
    "print(\"\\n--- Step 2.i: Advanced `MenuName` Standardization ---\")\n",
    "if 'MenuName_Cleaned' in df_processed.columns:\n",
    "    print(f\"Starting advanced MenuName standardization. Unique names so far: {df_processed['MenuName_Cleaned'].nunique()}\")\n",
    "\n",
    "    # Strategy 1: Further Text Normalization (example: remove special characters beyond basic strip/lower)\n",
    "    # This regex keeps alphanumeric chars, german umlauts, and spaces. Adjust as needed.\n",
    "    df_processed['MenuName_Normalized'] = df_processed['MenuName_Cleaned'].astype(str).apply(\n",
    "        lambda x: re.sub(r'[^a-zA-Z0-9äöüÄÖÜß\\s]', '', x) # Keep relevant chars\n",
    "    )\n",
    "    df_processed['MenuName_Normalized'] = df_processed['MenuName_Normalized'].str.replace(r'\\s+', ' ', regex=True).str.strip() # Replace multiple spaces with one\n",
    "\n",
    "    print(f\"Unique names after further normalization (alphanumeric, umlauts, spaces only): {df_processed['MenuName_Normalized'].nunique()}\")\n",
    "\n",
    "    # Strategy 2: Rule-based Mapping (Example - this needs to be built based on your data)\n",
    "    # This is highly iterative. You'd identify common patterns and map them.\n",
    "    # Example: (You will need to inspect your data to create meaningful rules)\n",
    "    menu_name_mapping = {\n",
    "        'spaghetti bolognese': 'spaghetti bolo',\n",
    "        'spagetti bolognese': 'spaghetti bolo',\n",
    "        'spag. bolo': 'spaghetti bolo',\n",
    "        'currywurst mit pommes': 'currywurst pommes',\n",
    "        'currywurst m. pommes': 'currywurst pommes',\n",
    "        'gemüsepfanne vegetarisch': 'gemüsepfanne veg',\n",
    "        'menü a': 'menü a standard', # Example if 'menü a' has variations\n",
    "        'menu a': 'menü a standard',\n",
    "        # ... add many more rules based on your data inspection ...\n",
    "    }\n",
    "    # Apply the mapping to the 'MenuName_Normalized' column\n",
    "    # Create a new column for the mapped names to preserve the previous step\n",
    "    df_processed['MenuName_Standardized'] = df_processed['MenuName_Normalized'].replace(menu_name_mapping)\n",
    "\n",
    "    print(f\"Unique names after example rule-based mapping: {df_processed['MenuName_Standardized'].nunique()}\")\n",
    "\n",
    "    if df_processed['MenuName_Standardized'].nunique() > 50:\n",
    "        print(\"First 20 unique 'MenuName_Standardized' (sample after mapping):\")\n",
    "        print(df_processed['MenuName_Standardized'].unique()[:20])\n",
    "    else:\n",
    "        print(\"Unique 'MenuName_Standardized':\")\n",
    "        print(df_processed['MenuName_Standardized'].unique())\n",
    "\n",
    "    print(\"\\nIMPORTANT: 'MenuName' standardization is an iterative process.\")\n",
    "    print(\"You'll likely need to: \")\n",
    "    print(\"  1. Analyze frequent variations of 'MenuName_Normalized' or 'MenuName_Standardized'.\")\n",
    "    print(\"  2. Expand your `menu_name_mapping` dictionary.\")\n",
    "    print(\"  3. Re-run and check unique counts until they are manageable and meaningful.\")\n",
    "    print(\"  Consider techniques like grouping by common substrings if appropriate.\")\n",
    "\n",
    "    snapshot7_path = os.path.join(snapshot_dir, 'snapshot_7_after_MenuName_Standardization.html')\n",
    "    df_processed.head(rows_for_snapshot).to_html(snapshot7_path, escape=False, index=False)\n",
    "    print(f\"\\nSnapshot 7: First {rows_for_snapshot} rows (showing 'MenuName_Standardized') saved to '{snapshot7_path}'\")\n",
    "\n",
    "else:\n",
    "    print(\"Warning: 'MenuName_Cleaned' column not found. Skipping advanced MenuName standardization.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"\\n--- Data Cleaning & Preprocessing (Core Steps Completed) ---\")\n",
    "print(\"Cleaned DataFrame shape (after site and menu name initial standardization):\", df_processed.shape)\n",
    "\n",
    "final_snapshot_path_cleaned = os.path.join(snapshot_dir, 'snapshot_final_cleaned_dataframe.html')\n",
    "df_processed.head(rows_for_snapshot).to_html(final_snapshot_path_cleaned, escape=False, index=False)\n",
    "print(f\"\\nFinal Cleaned Snapshot: First {rows_for_snapshot} rows of the processed DataFrame saved to '{final_snapshot_path_cleaned}'\")\n",
    "\n",
    "print(\"\\nProcessed DataFrame info (final cleaned):\")\n",
    "df_processed.info(verbose=True, show_counts=True)\n",
    "print(\"-\" * 50)"
   ],
   "id": "10c91ee1b6fe9ce8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2.i: Advanced `MenuName` Standardization ---\n",
      "Starting advanced MenuName standardization. Unique names so far: 322\n",
      "Unique names after further normalization (alphanumeric, umlauts, spaces only): 319\n",
      "Unique names after example rule-based mapping: 319\n",
      "First 20 unique 'MenuName_Standardized' (sample after mapping):\n",
      "['mittagessen gs' 'smart eating buffet wgrus' 'mittagessen bs'\n",
      " 'smart eating buffet primus' 'menü a hss' 'smart eating buffet gymbo'\n",
      " 'smart eating buffet brs' 'menü b1' 'smart eating buffet ema'\n",
      " 'mittagessen ml' 'smarteating buffet egm' 'dgemenü mg' 'menü a bomitte'\n",
      " 'menü a marbi' 'smart eating buffet gma' 'mittagessen bk'\n",
      " 'menü a standard' 'menü a szg' 'salatbar mpg' 'menü b2 lui']\n",
      "\n",
      "IMPORTANT: 'MenuName' standardization is an iterative process.\n",
      "You'll likely need to: \n",
      "  1. Analyze frequent variations of 'MenuName_Normalized' or 'MenuName_Standardized'.\n",
      "  2. Expand your `menu_name_mapping` dictionary.\n",
      "  3. Re-run and check unique counts until they are manageable and meaningful.\n",
      "  Consider techniques like grouping by common substrings if appropriate.\n",
      "\n",
      "Snapshot 7: First 1000 rows (showing 'MenuName_Standardized') saved to 'results/snapshots/snapshot_7_after_MenuName_Standardization.html'\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Data Cleaning & Preprocessing (Core Steps Completed) ---\n",
      "Cleaned DataFrame shape (after site and menu name initial standardization): (5674222, 18)\n",
      "\n",
      "Final Cleaned Snapshot: First 1000 rows of the processed DataFrame saved to 'results/snapshots/snapshot_final_cleaned_dataframe.html'\n",
      "\n",
      "Processed DataFrame info (final cleaned):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5674222 entries, 0 to 6538738\n",
      "Data columns (total 18 columns):\n",
      " #   Column                 Non-Null Count    Dtype         \n",
      "---  ------                 --------------    -----         \n",
      " 0   OrderId                5674222 non-null  int64         \n",
      " 1   TransactionId          5674222 non-null  object        \n",
      " 2   DateOfService          5674222 non-null  datetime64[ns]\n",
      " 3   DateOfOrder            5674222 non-null  datetime64[ns]\n",
      " 4   OrderQty               5674222 non-null  int64         \n",
      " 5   MenuName               5674222 non-null  object        \n",
      " 6   MenuPrice              5674222 non-null  int64         \n",
      " 7   MenuSubsidy            5674222 non-null  int64         \n",
      " 8   BookingNr              5674222 non-null  object        \n",
      " 9   GroupName              5674222 non-null  object        \n",
      " 10  CanceledQty            5674222 non-null  int64         \n",
      " 11  DateOfCancel           69962 non-null    datetime64[ns]\n",
      " 12  Site                   5674222 non-null  object        \n",
      " 13  SchoolID               5674222 non-null  object        \n",
      " 14  NeededMeals            5674222 non-null  int64         \n",
      " 15  MenuName_Cleaned       5674222 non-null  object        \n",
      " 16  MenuName_Normalized    5674222 non-null  object        \n",
      " 17  MenuName_Standardized  5674222 non-null  object        \n",
      "dtypes: datetime64[ns](3), int64(6), object(9)\n",
      "memory usage: 822.5+ MB\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:57.482911Z",
     "start_time": "2025-06-01T09:20:56.634945Z"
    }
   },
   "cell_type": "code",
   "source": "df_processed",
   "id": "c1e95fbc4ad330dc",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df_processed\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/core/displayhook.py:279\u001B[39m, in \u001B[36mDisplayHook.__call__\u001B[39m\u001B[34m(self, result)\u001B[39m\n\u001B[32m    277\u001B[39m \u001B[38;5;28mself\u001B[39m.start_displayhook()\n\u001B[32m    278\u001B[39m \u001B[38;5;28mself\u001B[39m.write_output_prompt()\n\u001B[32m--> \u001B[39m\u001B[32m279\u001B[39m format_dict, md_dict = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_format_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    280\u001B[39m \u001B[38;5;28mself\u001B[39m.update_user_ns(result)\n\u001B[32m    281\u001B[39m \u001B[38;5;28mself\u001B[39m.fill_exec_result(result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/core/displayhook.py:164\u001B[39m, in \u001B[36mDisplayHook.compute_format_data\u001B[39m\u001B[34m(self, result)\u001B[39m\n\u001B[32m    134\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_format_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, result):\n\u001B[32m    135\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Compute format data of the object to be displayed.\u001B[39;00m\n\u001B[32m    136\u001B[39m \n\u001B[32m    137\u001B[39m \u001B[33;03m    The format data is a generalization of the :func:`repr` of an object.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    162\u001B[39m \n\u001B[32m    163\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m164\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mshell\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdisplay_formatter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/core/formatters.py:238\u001B[39m, in \u001B[36mDisplayFormatter.format\u001B[39m\u001B[34m(self, obj, include, exclude)\u001B[39m\n\u001B[32m    236\u001B[39m md = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    237\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m238\u001B[39m     data = \u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# FIXME: log the exception\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/decorator.py:235\u001B[39m, in \u001B[36mdecorate.<locals>.fun\u001B[39m\u001B[34m(*args, **kw)\u001B[39m\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwsyntax:\n\u001B[32m    234\u001B[39m     args, kw = fix(args, kw, sig)\n\u001B[32m--> \u001B[39m\u001B[32m235\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcaller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m(\u001B[49m\u001B[43mextras\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/core/formatters.py:282\u001B[39m, in \u001B[36mcatch_format_error\u001B[39m\u001B[34m(method, self, *args, **kwargs)\u001B[39m\n\u001B[32m    280\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"show traceback on failed format call\"\"\"\u001B[39;00m\n\u001B[32m    281\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m282\u001B[39m     r = \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    283\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m:\n\u001B[32m    284\u001B[39m     \u001B[38;5;66;03m# don't warn on NotImplementedErrors\u001B[39;00m\n\u001B[32m    285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._check_return(\u001B[38;5;28;01mNone\u001B[39;00m, args[\u001B[32m0\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/core/formatters.py:770\u001B[39m, in \u001B[36mPlainTextFormatter.__call__\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    763\u001B[39m stream = StringIO()\n\u001B[32m    764\u001B[39m printer = pretty.RepresentationPrinter(stream, \u001B[38;5;28mself\u001B[39m.verbose,\n\u001B[32m    765\u001B[39m     \u001B[38;5;28mself\u001B[39m.max_width, \u001B[38;5;28mself\u001B[39m.newline,\n\u001B[32m    766\u001B[39m     max_seq_length=\u001B[38;5;28mself\u001B[39m.max_seq_length,\n\u001B[32m    767\u001B[39m     singleton_pprinters=\u001B[38;5;28mself\u001B[39m.singleton_printers,\n\u001B[32m    768\u001B[39m     type_pprinters=\u001B[38;5;28mself\u001B[39m.type_printers,\n\u001B[32m    769\u001B[39m     deferred_pprinters=\u001B[38;5;28mself\u001B[39m.deferred_printers)\n\u001B[32m--> \u001B[39m\u001B[32m770\u001B[39m \u001B[43mprinter\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpretty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    771\u001B[39m printer.flush()\n\u001B[32m    772\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m stream.getvalue()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/lib/pretty.py:411\u001B[39m, in \u001B[36mRepresentationPrinter.pretty\u001B[39m\u001B[34m(self, obj)\u001B[39m\n\u001B[32m    400\u001B[39m                         \u001B[38;5;28;01mreturn\u001B[39;00m meth(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[32m    401\u001B[39m                 \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    402\u001B[39m                     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mobject\u001B[39m\n\u001B[32m    403\u001B[39m                     \u001B[38;5;66;03m# check if cls defines __repr__\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    409\u001B[39m                     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(_safe_getattr(\u001B[38;5;28mcls\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m__repr__\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    410\u001B[39m                 ):\n\u001B[32m--> \u001B[39m\u001B[32m411\u001B[39m                     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_repr_pprint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcycle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_pprint(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[32m    414\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/IPython/lib/pretty.py:786\u001B[39m, in \u001B[36m_repr_pprint\u001B[39m\u001B[34m(obj, p, cycle)\u001B[39m\n\u001B[32m    784\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001B[39;00m\n\u001B[32m    785\u001B[39m \u001B[38;5;66;03m# Find newlines and replace them with p.break_()\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m786\u001B[39m output = \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    787\u001B[39m lines = output.splitlines()\n\u001B[32m    788\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m p.group():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/frame.py:1214\u001B[39m, in \u001B[36mDataFrame.__repr__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1211\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m buf.getvalue()\n\u001B[32m   1213\u001B[39m repr_params = fmt.get_dataframe_repr_params()\n\u001B[32m-> \u001B[39m\u001B[32m1214\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mto_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mrepr_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001B[39m, in \u001B[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    327\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) > num_allow_args:\n\u001B[32m    328\u001B[39m     warnings.warn(\n\u001B[32m    329\u001B[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001B[32m    330\u001B[39m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[32m    331\u001B[39m         stacklevel=find_stack_level(),\n\u001B[32m    332\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/frame.py:1376\u001B[39m, in \u001B[36mDataFrame.to_string\u001B[39m\u001B[34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001B[39m\n\u001B[32m   1373\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m option_context\n\u001B[32m   1375\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[33m\"\u001B[39m\u001B[33mdisplay.max_colwidth\u001B[39m\u001B[33m\"\u001B[39m, max_colwidth):\n\u001B[32m-> \u001B[39m\u001B[32m1376\u001B[39m     formatter = \u001B[43mfmt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataFrameFormatter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1377\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1378\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1379\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcol_space\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcol_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1380\u001B[39m \u001B[43m        \u001B[49m\u001B[43mna_rep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mna_rep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1381\u001B[39m \u001B[43m        \u001B[49m\u001B[43mformatters\u001B[49m\u001B[43m=\u001B[49m\u001B[43mformatters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1382\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfloat_format\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfloat_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1383\u001B[39m \u001B[43m        \u001B[49m\u001B[43msparsify\u001B[49m\u001B[43m=\u001B[49m\u001B[43msparsify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1384\u001B[39m \u001B[43m        \u001B[49m\u001B[43mjustify\u001B[49m\u001B[43m=\u001B[49m\u001B[43mjustify\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1385\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex_names\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1386\u001B[39m \u001B[43m        \u001B[49m\u001B[43mheader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mheader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1387\u001B[39m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1388\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmin_rows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmin_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1389\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_rows\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1390\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_cols\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmax_cols\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1391\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_dimensions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_dimensions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1392\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecimal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecimal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1393\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1394\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m fmt.DataFrameRenderer(formatter).to_string(\n\u001B[32m   1395\u001B[39m         buf=buf,\n\u001B[32m   1396\u001B[39m         encoding=encoding,\n\u001B[32m   1397\u001B[39m         line_width=line_width,\n\u001B[32m   1398\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/formats/format.py:469\u001B[39m, in \u001B[36mDataFrameFormatter.__init__\u001B[39m\u001B[34m(self, frame, columns, col_space, header, index, na_rep, formatters, justify, float_format, sparsify, index_names, max_rows, min_rows, max_cols, show_dimensions, decimal, bold_rows, escape)\u001B[39m\n\u001B[32m    466\u001B[39m \u001B[38;5;28mself\u001B[39m.max_rows_fitted = \u001B[38;5;28mself\u001B[39m._calc_max_rows_fitted()\n\u001B[32m    468\u001B[39m \u001B[38;5;28mself\u001B[39m.tr_frame = \u001B[38;5;28mself\u001B[39m.frame\n\u001B[32m--> \u001B[39m\u001B[32m469\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    470\u001B[39m \u001B[38;5;28mself\u001B[39m.adj = printing.get_adjustment()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/formats/format.py:655\u001B[39m, in \u001B[36mDataFrameFormatter.truncate\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    651\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    652\u001B[39m \u001B[33;03mCheck whether the frame should be truncated. If so, slice the frame up.\u001B[39;00m\n\u001B[32m    653\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    654\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_truncated_horizontally:\n\u001B[32m--> \u001B[39m\u001B[32m655\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_truncate_horizontally\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    657\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_truncated_vertically:\n\u001B[32m    658\u001B[39m     \u001B[38;5;28mself\u001B[39m._truncate_vertically()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/formats/format.py:673\u001B[39m, in \u001B[36mDataFrameFormatter._truncate_horizontally\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    671\u001B[39m left = \u001B[38;5;28mself\u001B[39m.tr_frame.iloc[:, :col_num]\n\u001B[32m    672\u001B[39m right = \u001B[38;5;28mself\u001B[39m.tr_frame.iloc[:, -col_num:]\n\u001B[32m--> \u001B[39m\u001B[32m673\u001B[39m \u001B[38;5;28mself\u001B[39m.tr_frame = \u001B[43mconcat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mleft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mright\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    675\u001B[39m \u001B[38;5;66;03m# truncate formatter\u001B[39;00m\n\u001B[32m    676\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.formatters, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/reshape/concat.py:395\u001B[39m, in \u001B[36mconcat\u001B[39m\u001B[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001B[39m\n\u001B[32m    380\u001B[39m     copy = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    382\u001B[39m op = _Concatenator(\n\u001B[32m    383\u001B[39m     objs,\n\u001B[32m    384\u001B[39m     axis=axis,\n\u001B[32m   (...)\u001B[39m\u001B[32m    392\u001B[39m     sort=sort,\n\u001B[32m    393\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m395\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/reshape/concat.py:684\u001B[39m, in \u001B[36m_Concatenator.get_result\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    680\u001B[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001B[32m    682\u001B[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001B[32m--> \u001B[39m\u001B[32m684\u001B[39m new_data = \u001B[43mconcatenate_managers\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmgrs_indexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnew_axes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconcat_axis\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbm_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    687\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.copy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_copy_on_write():\n\u001B[32m    688\u001B[39m     new_data._consolidate_inplace()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/internals/concat.py:131\u001B[39m, in \u001B[36mconcatenate_managers\u001B[39m\u001B[34m(mgrs_indexers, axes, concat_axis, copy)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;66;03m# Assertions disabled for performance\u001B[39;00m\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# for tup in mgrs_indexers:\u001B[39;00m\n\u001B[32m    126\u001B[39m \u001B[38;5;66;03m#    # caller is responsible for ensuring this\u001B[39;00m\n\u001B[32m    127\u001B[39m \u001B[38;5;66;03m#    indexers = tup[1]\u001B[39;00m\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m#    assert concat_axis not in indexers\u001B[39;00m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m concat_axis == \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m131\u001B[39m     mgrs = \u001B[43m_maybe_reindex_columns_na_proxy\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmgrs_indexers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mneeds_copy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    132\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m mgrs[\u001B[32m0\u001B[39m].concat_horizontal(mgrs, axes)\n\u001B[32m    134\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mgrs_indexers) > \u001B[32m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m mgrs_indexers[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m].nblocks > \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/internals/concat.py:230\u001B[39m, in \u001B[36m_maybe_reindex_columns_na_proxy\u001B[39m\u001B[34m(axes, mgrs_indexers, needs_copy)\u001B[39m\n\u001B[32m    220\u001B[39m         mgr = mgr.reindex_indexer(\n\u001B[32m    221\u001B[39m             axes[i],\n\u001B[32m    222\u001B[39m             indexers[i],\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m             use_na_proxy=\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# only relevant for i==0\u001B[39;00m\n\u001B[32m    228\u001B[39m         )\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m needs_copy \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m indexers:\n\u001B[32m--> \u001B[39m\u001B[32m230\u001B[39m         mgr = \u001B[43mmgr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    232\u001B[39m     new_mgrs.append(mgr)\n\u001B[32m    233\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m new_mgrs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/internals/managers.py:593\u001B[39m, in \u001B[36mBaseBlockManager.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m    590\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    591\u001B[39m         new_axes = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m.axes)\n\u001B[32m--> \u001B[39m\u001B[32m593\u001B[39m res = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcopy\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdeep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    594\u001B[39m res.axes = new_axes\n\u001B[32m    596\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.ndim > \u001B[32m1\u001B[39m:\n\u001B[32m    597\u001B[39m     \u001B[38;5;66;03m# Avoid needing to re-compute these\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001B[39m, in \u001B[36mBaseBlockManager.apply\u001B[39m\u001B[34m(self, f, align_keys, **kwargs)\u001B[39m\n\u001B[32m    361\u001B[39m         applied = b.apply(f, **kwargs)\n\u001B[32m    362\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m363\u001B[39m         applied = \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    364\u001B[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001B[32m    366\u001B[39m out = \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).from_blocks(result_blocks, \u001B[38;5;28mself\u001B[39m.axes)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/core/internals/blocks.py:796\u001B[39m, in \u001B[36mBlock.copy\u001B[39m\u001B[34m(self, deep)\u001B[39m\n\u001B[32m    794\u001B[39m refs: BlockValuesRefs | \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    795\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m deep:\n\u001B[32m--> \u001B[39m\u001B[32m796\u001B[39m     values = \u001B[43mvalues\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    797\u001B[39m     refs = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    798\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:21:01.732621Z",
     "start_time": "2025-06-01T09:20:59.701561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n--- Step 3: Feature Engineering ---\")\n",
    "\n",
    "# Ensure DateOfService is datetime\n",
    "if 'DateOfService' in df_processed.columns and not pd.api.types.is_datetime64_any_dtype(df_processed['DateOfService']):\n",
    "    df_processed['DateOfService'] = pd.to_datetime(df_processed['DateOfService'], errors='coerce')\n",
    "    df_processed.dropna(subset=['DateOfService'], inplace=True) # Drop rows where DateOfService became NaT\n",
    "\n",
    "if 'DateOfService' not in df_processed.columns:\n",
    "    print(\"ERROR: 'DateOfService' column is missing. Cannot proceed with feature engineering.\")\n",
    "    exit()\n",
    "\n",
    "# a. Extract Date/Time Features from DateOfService\n",
    "print(\"\\nExtracting date/time features from 'DateOfService'...\")\n",
    "df_processed['Year'] = df_processed['DateOfService'].dt.year\n",
    "df_processed['Month'] = df_processed['DateOfService'].dt.month\n",
    "df_processed['DayOfMonth'] = df_processed['DateOfService'].dt.day\n",
    "df_processed['DayOfWeek'] = df_processed['DateOfService'].dt.dayofweek # Monday=0, Sunday=6\n",
    "df_processed['DayName'] = df_processed['DateOfService'].dt.day_name()\n",
    "df_processed['DayOfYear'] = df_processed['DateOfService'].dt.dayofyear\n",
    "df_processed['WeekOfYear'] = df_processed['DateOfService'].dt.isocalendar().week.astype(int)\n",
    "df_processed['Quarter'] = df_processed['DateOfService'].dt.quarter\n",
    "df_processed['IsWeekend'] = df_processed['DayOfWeek'].isin([5, 6]).astype(int) # Saturday or Sunday\n",
    "\n",
    "print(\"Date/time features created: Year, Month, DayOfMonth, DayOfWeek, DayName, DayOfYear, WeekOfYear, Quarter, IsWeekend\")\n",
    "\n",
    "# b. Order Lead Time Features (relative to DateOfService)\n",
    "print(\"\\nCreating order lead time features...\")\n",
    "if 'DateOfOrder' in df_processed.columns and pd.api.types.is_datetime64_any_dtype(df_processed['DateOfOrder']):\n",
    "    df_processed['OrderLeadTimeDays'] = (df_processed['DateOfService'] - df_processed['DateOfOrder']).dt.days\n",
    "    # Categorize lead time based on use case owner's benchmarks\n",
    "    bins = [-float('inf'), 1, 5, float('inf')] # Bins: <=1 day, 2-5 days, >5 days\n",
    "    labels = ['1_day_or_less', '2_to_5_days', 'gt_5_days']\n",
    "    df_processed['OrderLeadTimeCategory'] = pd.cut(df_processed['OrderLeadTimeDays'], bins=bins, labels=labels, right=True)\n",
    "    print(\"'OrderLeadTimeDays' and 'OrderLeadTimeCategory' created.\")\n",
    "    print(df_processed['OrderLeadTimeCategory'].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"Warning: 'DateOfOrder' not found or not datetime. Skipping OrderLeadTime features.\")\n",
    "\n",
    "# c. Cancellation Lead Time Features (relative to DateOfService)\n",
    "print(\"\\nCreating cancellation lead time features...\")\n",
    "if 'DateOfCancel' in df_processed.columns and pd.api.types.is_datetime64_any_dtype(df_processed['DateOfCancel']):\n",
    "    df_processed['CancelLeadTimeDays'] = (df_processed['DateOfService'] - df_processed['DateOfCancel']).dt.days\n",
    "    # Note: DateOfCancel can be NaT if not canceled. CancelLeadTimeDays will be NaN for these.\n",
    "    # We can categorize these NaNs as 'NotCanceled' if useful.\n",
    "    df_processed['CancelLeadTimeCategory'] = pd.cut(df_processed['CancelLeadTimeDays'], bins=bins, labels=labels, right=True)\n",
    "    # For orders that were not canceled, DateOfCancel is NaT, so CancelLeadTimeDays is NaT.\n",
    "    # Let's fill the category for these as 'NotCanceled'.\n",
    "    df_processed['CancelLeadTimeCategory'] = df_processed['CancelLeadTimeCategory'].cat.add_categories(['NotCanceled']).fillna('NotCanceled')\n",
    "\n",
    "    print(\"'CancelLeadTimeDays' and 'CancelLeadTimeCategory' created.\")\n",
    "    print(df_processed['CancelLeadTimeCategory'].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"Warning: 'DateOfCancel' not found or not datetime. Skipping CancelLeadTime features.\")\n",
    "\n",
    "\n",
    "# d. Holiday Features (Placeholder - Prophet handles this well during modeling)\n",
    "print(\"\\nHoliday Features Consideration:\")\n",
    "print(\"Prophet has built-in functionality for holidays (country-specific or custom).\")\n",
    "print(\"This will be addressed during the Prophet model setup for each time series.\")\n",
    "print(\"For Germany: Prophet can use add_country_holidays(country_name='DE').\")\n",
    "\n",
    "\n",
    "# e. Interaction Features or Other Derived Features (Examples)\n",
    "# print(\"\\nConsidering other derived features (examples):\")\n",
    "# Example: Is it a Monday after a holiday weekend? (More complex, requires holiday data)\n",
    "# Example: Price per meal (if OrderQty > 0)\n",
    "# if 'MenuPrice' in df_processed.columns and 'OrderQty' in df_processed.columns:\n",
    "#     df_processed['PricePerUnit'] = df_processed['MenuPrice'] / df_processed['OrderQty'].replace(0, np.nan) # Avoid division by zero\n",
    "\n",
    "print(\"\\nSnapshot after Feature Engineering:\")\n",
    "snapshot_fe_path = os.path.join(snapshot_dir, 'snapshot_FE_dataframe.html') # Assuming snapshot_dir is defined\n",
    "df_processed.head(rows_for_snapshot).to_html(snapshot_fe_path, escape=False, index=False)\n",
    "print(f\"First {rows_for_snapshot} rows of DataFrame with new features saved to '{snapshot_fe_path}'\")\n",
    "print(df_processed.info(verbose=False, show_counts=True))\n",
    "print(\"-\" * 50)\n"
   ],
   "id": "4333fe9f97048729",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Feature Engineering ---\n",
      "\n",
      "Extracting date/time features from 'DateOfService'...\n",
      "Date/time features created: Year, Month, DayOfMonth, DayOfWeek, DayName, DayOfYear, WeekOfYear, Quarter, IsWeekend\n",
      "\n",
      "Creating order lead time features...\n",
      "'OrderLeadTimeDays' and 'OrderLeadTimeCategory' created.\n",
      "OrderLeadTimeCategory\n",
      "gt_5_days        3961201\n",
      "1_day_or_less    1290896\n",
      "2_to_5_days       422125\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Creating cancellation lead time features...\n",
      "'CancelLeadTimeDays' and 'CancelLeadTimeCategory' created.\n",
      "CancelLeadTimeCategory\n",
      "NotCanceled      5604260\n",
      "gt_5_days          62519\n",
      "2_to_5_days         5054\n",
      "1_day_or_less       2389\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Holiday Features Consideration:\n",
      "Prophet has built-in functionality for holidays (country-specific or custom).\n",
      "This will be addressed during the Prophet model setup for each time series.\n",
      "For Germany: Prophet can use add_country_holidays(country_name='DE').\n",
      "\n",
      "Snapshot after Feature Engineering:\n",
      "First 1000 rows of DataFrame with new features saved to 'results/snapshots/snapshot_FE_dataframe.html'\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5674222 entries, 0 to 6538738\n",
      "Columns: 31 entries, OrderId to CancelLeadTimeCategory\n",
      "dtypes: category(2), datetime64[ns](3), float64(1), int32(6), int64(9), object(10)\n",
      "memory usage: 1.2+ GB\n",
      "None\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:21:08.614085Z",
     "start_time": "2025-06-01T09:21:05.308021Z"
    }
   },
   "cell_type": "code",
   "source": "df_processed",
   "id": "86c211e9bcc729d1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          OrderId                           TransactionId DateOfService  \\\n",
       "0        11518978        4c5060636f584ef9a1effa77282755f5    2020-01-02   \n",
       "1        11285143        68472c70b9c84fb784834ecc257827d7    2020-01-02   \n",
       "2        11285146        7262eace0d104592b1269e38f5b45ec1    2020-01-02   \n",
       "3        11285152        8e451931e8fc4554869c3e4533b65e23    2020-01-02   \n",
       "4        11285155        bfa8fa0812ee40baa98e5aaf52d30e0b    2020-01-02   \n",
       "...           ...                                     ...           ...   \n",
       "6538732  21565890  fd46957648df48d9bd95f785357cbe25_order    2023-12-22   \n",
       "6538734  20946470  c66797e99f7d4684a249d2fde8e03d4d_order    2023-12-22   \n",
       "6538735  21178498  cf8caf00f8fc4674ac8039562275c0fd_order    2023-12-22   \n",
       "6538736  21178551  ade8ad875b904ef89fb82aefb4d089ea_order    2023-12-22   \n",
       "6538738  20870956  93603cef11484663bc0ac8dfb08fe3c5_order    2023-12-22   \n",
       "\n",
       "                DateOfOrder  OrderQty                     MenuName  MenuPrice  \\\n",
       "0       2020-02-05 11:54:08         1             Mittagessen (Gs)        310   \n",
       "1       2019-12-16 10:30:51         1  Smart Eating Buffet (WGrus)          0   \n",
       "2       2019-12-16 10:31:33         1  Smart Eating Buffet (WGrus)        290   \n",
       "3       2019-12-16 10:32:05         1  Smart Eating Buffet (WGrus)          0   \n",
       "4       2019-12-16 10:32:31         1  Smart Eating Buffet (WGrus)        290   \n",
       "...                     ...       ...                          ...        ...   \n",
       "6538732 2023-12-18 08:50:21         1             Mittagessen (KL)        370   \n",
       "6538734 2023-11-14 15:11:29         1                  Mittagessen          0   \n",
       "6538735 2023-11-27 12:11:24         1             Mittagessen (KL)        370   \n",
       "6538736 2023-11-27 12:11:31         1             Mittagessen (KL)        370   \n",
       "6538738 2023-11-14 12:20:50         1             Mittagessen (KL)        370   \n",
       "\n",
       "         MenuSubsidy      BookingNr                       GroupName  ...  \\\n",
       "0                  0   349-88220481        xxx3,45€ normal 5T (68€)  ...   \n",
       "1                350  248-141751492          Steinfurt Abo ermäßigt  ...   \n",
       "2                 60   248-77489928  Westerkappeln Grundschüler Abo  ...   \n",
       "3                350   248-77558043          Steinfurt Abo ermäßigt  ...   \n",
       "4                 60   248-77420774  Westerkappeln Grundschüler Abo  ...   \n",
       "...              ...            ...                             ...  ...   \n",
       "6538732            0  349-167443917                          3,70 €  ...   \n",
       "6538734          375  721-161089197                       BuT (E,P)  ...   \n",
       "6538735            0  349-163628248                          3,70 €  ...   \n",
       "6538736            0  349-165661318                          3,70 €  ...   \n",
       "6538738            0  349-157380774                          3,70 €  ...   \n",
       "\n",
       "         DayOfWeek   DayName DayOfYear WeekOfYear  Quarter IsWeekend  \\\n",
       "0                3  Thursday         2          1        1         0   \n",
       "1                3  Thursday         2          1        1         0   \n",
       "2                3  Thursday         2          1        1         0   \n",
       "3                3  Thursday         2          1        1         0   \n",
       "4                3  Thursday         2          1        1         0   \n",
       "...            ...       ...       ...        ...      ...       ...   \n",
       "6538732          4    Friday       356         51        4         0   \n",
       "6538734          4    Friday       356         51        4         0   \n",
       "6538735          4    Friday       356         51        4         0   \n",
       "6538736          4    Friday       356         51        4         0   \n",
       "6538738          4    Friday       356         51        4         0   \n",
       "\n",
       "        OrderLeadTimeDays OrderLeadTimeCategory  CancelLeadTimeDays  \\\n",
       "0                     -35         1_day_or_less                 NaN   \n",
       "1                      16             gt_5_days                 NaN   \n",
       "2                      16             gt_5_days                 NaN   \n",
       "3                      16             gt_5_days                 NaN   \n",
       "4                      16             gt_5_days                 NaN   \n",
       "...                   ...                   ...                 ...   \n",
       "6538732                 3           2_to_5_days                 NaN   \n",
       "6538734                37             gt_5_days                 NaN   \n",
       "6538735                24             gt_5_days                 NaN   \n",
       "6538736                24             gt_5_days                 NaN   \n",
       "6538738                37             gt_5_days                 NaN   \n",
       "\n",
       "         CancelLeadTimeCategory  \n",
       "0                   NotCanceled  \n",
       "1                   NotCanceled  \n",
       "2                   NotCanceled  \n",
       "3                   NotCanceled  \n",
       "4                   NotCanceled  \n",
       "...                         ...  \n",
       "6538732             NotCanceled  \n",
       "6538734             NotCanceled  \n",
       "6538735             NotCanceled  \n",
       "6538736             NotCanceled  \n",
       "6538738             NotCanceled  \n",
       "\n",
       "[5674222 rows x 31 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderId</th>\n",
       "      <th>TransactionId</th>\n",
       "      <th>DateOfService</th>\n",
       "      <th>DateOfOrder</th>\n",
       "      <th>OrderQty</th>\n",
       "      <th>MenuName</th>\n",
       "      <th>MenuPrice</th>\n",
       "      <th>MenuSubsidy</th>\n",
       "      <th>BookingNr</th>\n",
       "      <th>GroupName</th>\n",
       "      <th>...</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DayName</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>WeekOfYear</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>OrderLeadTimeDays</th>\n",
       "      <th>OrderLeadTimeCategory</th>\n",
       "      <th>CancelLeadTimeDays</th>\n",
       "      <th>CancelLeadTimeCategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11518978</td>\n",
       "      <td>4c5060636f584ef9a1effa77282755f5</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2020-02-05 11:54:08</td>\n",
       "      <td>1</td>\n",
       "      <td>Mittagessen (Gs)</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>349-88220481</td>\n",
       "      <td>xxx3,45€ normal 5T (68€)</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-35</td>\n",
       "      <td>1_day_or_less</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11285143</td>\n",
       "      <td>68472c70b9c84fb784834ecc257827d7</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2019-12-16 10:30:51</td>\n",
       "      <td>1</td>\n",
       "      <td>Smart Eating Buffet (WGrus)</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>248-141751492</td>\n",
       "      <td>Steinfurt Abo ermäßigt</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11285146</td>\n",
       "      <td>7262eace0d104592b1269e38f5b45ec1</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2019-12-16 10:31:33</td>\n",
       "      <td>1</td>\n",
       "      <td>Smart Eating Buffet (WGrus)</td>\n",
       "      <td>290</td>\n",
       "      <td>60</td>\n",
       "      <td>248-77489928</td>\n",
       "      <td>Westerkappeln Grundschüler Abo</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11285152</td>\n",
       "      <td>8e451931e8fc4554869c3e4533b65e23</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2019-12-16 10:32:05</td>\n",
       "      <td>1</td>\n",
       "      <td>Smart Eating Buffet (WGrus)</td>\n",
       "      <td>0</td>\n",
       "      <td>350</td>\n",
       "      <td>248-77558043</td>\n",
       "      <td>Steinfurt Abo ermäßigt</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11285155</td>\n",
       "      <td>bfa8fa0812ee40baa98e5aaf52d30e0b</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2019-12-16 10:32:31</td>\n",
       "      <td>1</td>\n",
       "      <td>Smart Eating Buffet (WGrus)</td>\n",
       "      <td>290</td>\n",
       "      <td>60</td>\n",
       "      <td>248-77420774</td>\n",
       "      <td>Westerkappeln Grundschüler Abo</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538732</th>\n",
       "      <td>21565890</td>\n",
       "      <td>fd46957648df48d9bd95f785357cbe25_order</td>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>2023-12-18 08:50:21</td>\n",
       "      <td>1</td>\n",
       "      <td>Mittagessen (KL)</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>349-167443917</td>\n",
       "      <td>3,70 €</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>356</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2_to_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538734</th>\n",
       "      <td>20946470</td>\n",
       "      <td>c66797e99f7d4684a249d2fde8e03d4d_order</td>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>2023-11-14 15:11:29</td>\n",
       "      <td>1</td>\n",
       "      <td>Mittagessen</td>\n",
       "      <td>0</td>\n",
       "      <td>375</td>\n",
       "      <td>721-161089197</td>\n",
       "      <td>BuT (E,P)</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>356</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538735</th>\n",
       "      <td>21178498</td>\n",
       "      <td>cf8caf00f8fc4674ac8039562275c0fd_order</td>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>2023-11-27 12:11:24</td>\n",
       "      <td>1</td>\n",
       "      <td>Mittagessen (KL)</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>349-163628248</td>\n",
       "      <td>3,70 €</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>356</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538736</th>\n",
       "      <td>21178551</td>\n",
       "      <td>ade8ad875b904ef89fb82aefb4d089ea_order</td>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>2023-11-27 12:11:31</td>\n",
       "      <td>1</td>\n",
       "      <td>Mittagessen (KL)</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>349-165661318</td>\n",
       "      <td>3,70 €</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>356</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538738</th>\n",
       "      <td>20870956</td>\n",
       "      <td>93603cef11484663bc0ac8dfb08fe3c5_order</td>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>2023-11-14 12:20:50</td>\n",
       "      <td>1</td>\n",
       "      <td>Mittagessen (KL)</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>349-157380774</td>\n",
       "      <td>3,70 €</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>356</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>gt_5_days</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NotCanceled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5674222 rows × 31 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:34.702365Z",
     "start_time": "2025-06-01T09:20:34.701262Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f3dfd1b62ee2ad62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T09:20:34.739648Z",
     "start_time": "2025-06-01T09:20:34.738376Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "15478e9b72a5ab70",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
