{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-20T19:37:07.253970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# PART 1: SETUP, DATA LOADING & CUSTOM TRANSFORMERS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer # (MODIFIED) Only SimpleImputer is needed now\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "import time\n",
    "import shap\n",
    "import holidays\n",
    "\n",
    "# --- Setup ---\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 8)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Testing Flag ---\n",
    "TESTING_MODE = True\n",
    "\n",
    "# --- Data Loading ---\n",
    "print(\"PART 1: Loading and preparing data...\")\n",
    "try:\n",
    "    df = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather2.csv', parse_dates=['DateOfService', 'DateOfOrder', 'DateOfCancel'])\n",
    "    print(\"Successfully loaded df_weather2.csv.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: df_weather2.csv not found. Creating a dummy dataframe.\")\n",
    "    dates = pd.to_datetime(pd.date_range(start='2022-01-01', end='2023-12-31', freq='D'))\n",
    "    sites_map = {'MS': 150, 'LP': 1200, 'BK': 900}\n",
    "    dummy_data = []\n",
    "    for site, avg_orders in sites_map.items():\n",
    "        for date in dates:\n",
    "            order_qty = np.random.randint(int(avg_orders*0.8), int(avg_orders*1.2))\n",
    "            cancel_qty = int(order_qty * np.random.uniform(0.01, 0.15))\n",
    "            dummy_data.append({\n",
    "                'DateOfService': date, 'DateOfOrder': date - pd.Timedelta(days=1), 'DateOfCancel': date,\n",
    "                'Site': site, 'MenuBase': f'Menu_{np.random.randint(1,5)}', 'OrderQty': order_qty, 'CanceledQty': cancel_qty,\n",
    "                'MenuPrice': np.random.uniform(3.5, 5.5), 'MenuSubsidy': np.random.uniform(0.5, 1.5),\n",
    "                'tavg_C': np.random.uniform(5, 25), 'prcp_mm': np.random.uniform(0, 10),\n",
    "                'rain_flag': int(np.random.rand() > 0.7), 'temp_dev': np.random.uniform(-5, 5),\n",
    "                'is_holiday': int(np.random.rand() > 0.95), 'is_weekend': int(date.weekday() >= 5),\n",
    "                'SchoolID': f'School_{np.random.randint(1,20)}'\n",
    "            })\n",
    "    df = pd.DataFrame(dummy_data)\n",
    "\n",
    "\n",
    "if TESTING_MODE:\n",
    "    print(\"\\n>>> TESTING MODE ENABLED: Using a time slice (all of 2022) for a more realistic test.\")\n",
    "    df = df[df['DateOfService'].dt.year == 2022].copy()\n",
    "    print(f\"New DataFrame shape for testing: {df.shape}\")\n",
    "\n",
    "df['net_qty'] = df['OrderQty'] - df['CanceledQty']\n",
    "print(\"Initial DataFrame shape:\", df.shape)\n",
    "\n",
    "# --- Custom Transformer Definitions ---\n",
    "# (REMOVED) The custom MissingFlagImputer class is no longer needed.\n",
    "# We will use the built-in SimpleImputer(add_indicator=True) instead.\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"\\nPART 1 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2: DAILY AGGREGATION\n",
    "# =============================================================================\n",
    "print(\"\\nPART 2: Aggregating data to daily level...\")\n",
    "agg_dict = {\n",
    "    'net_qty': 'sum', 'OrderQty': 'sum', 'CanceledQty': 'sum', 'MenuPrice': 'mean',\n",
    "    'MenuSubsidy': 'mean', 'tavg_C': 'mean', 'prcp_mm': 'sum', 'rain_flag': 'max',\n",
    "    'temp_dev': 'mean', 'is_holiday': 'max', 'is_weekend': 'max', 'MenuBase': 'nunique',\n",
    "    'SchoolID': 'nunique'\n",
    "}\n",
    "df_daily = df.groupby(['DateOfService', 'Site']).agg(agg_dict).reset_index()\n",
    "df_daily['daily_cancel_pct'] = (df_daily['CanceledQty'] / df_daily['OrderQty']).fillna(0)\n",
    "df_daily = df_daily.sort_values('DateOfService').set_index('DateOfService')\n",
    "print(\"Daily aggregated DataFrame shape:\", df_daily.shape)\n",
    "print(\"\\nPART 2 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 3: FEATURE ENGINEERING PIPELINES\n",
    "# =============================================================================\n",
    "print(\"\\nPART 3: Defining feature engineering pipelines...\")\n",
    "class DailyFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, target_col='net_qty'):\n",
    "        self.target_col = target_col\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['day_of_year'] = X_copy.index.dayofyear\n",
    "        X_copy['weekday'] = X_copy.index.weekday\n",
    "        X_copy['month'] = X_copy.index.month\n",
    "        X_copy['year'] = X_copy.index.year\n",
    "        X_copy['week_of_year'] = X_copy.index.isocalendar().week.astype(int)\n",
    "        X_copy['quarter'] = X_copy.index.quarter\n",
    "        for lag in [1, 2, 7, 14, 28]:\n",
    "            X_copy[f'lag_{lag}_{self.target_col}'] = X_copy.groupby('Site')[self.target_col].shift(lag)\n",
    "        X_copy[f'rolling_mean_7_{self.target_col}'] = X_copy.groupby('Site')[self.target_col].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "        X_copy[f'rolling_std_7_{self.target_col}'] = X_copy.groupby('Site')[self.target_col].shift(1).rolling(window=7, min_periods=1).std()\n",
    "        return X_copy\n",
    "print(\"\\nPART 3 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4: CUSTOM METRIC AND HYPERPARAMETER DEFINITIONS\n",
    "# =============================================================================\n",
    "print(\"\\nPART 4: Defining custom metric and hyperparameter grids...\")\n",
    "\n",
    "def asymmetric_loss(y_true, y_pred, under_penalty=1.2):\n",
    "    error = y_true - y_pred\n",
    "    loss = np.mean(np.where(error < 0, -error * under_penalty, error))\n",
    "    return loss\n",
    "\n",
    "asymmetric_scorer = make_scorer(asymmetric_loss, greater_is_better=False)\n",
    "\n",
    "if TESTING_MODE:\n",
    "    print(\">>> TESTING MODE: Using smaller hyperparameter grids and fewer iterations.\")\n",
    "    param_dist_lgbm = {\n",
    "        'regressor__n_estimators': [50, 100], 'regressor__learning_rate': [0.05, 0.1],\n",
    "        'regressor__num_leaves': [20, 31], 'regressor__colsample_bytree': [0.8],\n",
    "    }\n",
    "    N_ITER_CQ, N_ITER_OQ, N_ITER_ERR = 4, 5, 4\n",
    "else:\n",
    "    param_dist_lgbm = {\n",
    "        'regressor__n_estimators': [200, 400, 600, 800], 'regressor__learning_rate': [0.02, 0.05, 0.1],\n",
    "        'regressor__num_leaves': [31, 40, 50, 60], 'regressor__max_depth': [-1, 10, 15],\n",
    "        'regressor__reg_alpha': [0, 0.1, 0.5, 1], 'regressor__reg_lambda': [0, 0.1, 0.5, 1],\n",
    "        'regressor__colsample_bytree': [0.7, 0.8, 0.9, 1.0], 'regressor__subsample': [0.7, 0.8, 0.9, 1.0]\n",
    "    }\n",
    "    N_ITER_CQ, N_ITER_OQ, N_ITER_ERR = 10, 15, 10\n",
    "print(\"\\nPART 4 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 5: ADVANCED CALENDAR & FINAL DATAFRAME PREP\n",
    "# =============================================================================\n",
    "print(\"\\nPART 5: Creating advanced calendar features...\")\n",
    "df_final = df_daily.copy()\n",
    "try:\n",
    "    years = df_final.index.year.unique().tolist()\n",
    "    df_final_sorted = df_final.reset_index().sort_values('DateOfService')\n",
    "\n",
    "    de_holidays_public = holidays.DE(subdiv='NW', years=years)\n",
    "    public_holidays_df = pd.DataFrame(list(de_holidays_public.items()), columns=['DateOfService', 'HolidayName'])\n",
    "    public_holidays_df['DateOfService'] = pd.to_datetime(public_holidays_df['DateOfService'])\n",
    "    merged_forward = pd.merge_asof(df_final_sorted, public_holidays_df[['DateOfService']].rename(columns={'DateOfService': 'NextHoliday'}), left_on='DateOfService', right_on='NextHoliday', direction='forward')\n",
    "    merged_backward = pd.merge_asof(df_final_sorted, public_holidays_df[['DateOfService']].rename(columns={'DateOfService': 'PreviousHoliday'}), left_on='DateOfService', right_on='PreviousHoliday', direction='backward')\n",
    "    df_final_sorted['days_until_holiday'] = (merged_forward['NextHoliday'] - merged_forward['DateOfService']).dt.days\n",
    "    df_final_sorted['days_since_holiday'] = (merged_backward['DateOfService'] - merged_backward['PreviousHoliday']).dt.days\n",
    "\n",
    "    de_holidays_school = holidays.DE(subdiv='NW', years=years, categories=\"SCHOOL\")\n",
    "    school_holidays_df = pd.DataFrame(list(de_holidays_school.items()), columns=['DateOfService', 'HolidayName'])\n",
    "    school_holidays_df['DateOfService'] = pd.to_datetime(school_holidays_df['DateOfService'])\n",
    "    df_final_sorted['is_school_vacation'] = df_final_sorted['DateOfService'].isin(school_holidays_df['DateOfService']).astype(int)\n",
    "    merged_forward_school = pd.merge_asof(df_final_sorted, school_holidays_df[['DateOfService']].rename(columns={'DateOfService': 'NextSchoolVacation'}), left_on='DateOfService', right_on='NextSchoolVacation', direction='forward')\n",
    "    merged_backward_school = pd.merge_asof(df_final_sorted, school_holidays_df[['DateOfService']].rename(columns={'DateOfService': 'PreviousSchoolVacation'}), left_on='DateOfService', right_on='PreviousSchoolVacation', direction='backward')\n",
    "    df_final_sorted['days_until_school_vacation'] = (merged_forward_school['NextSchoolVacation'] - merged_forward_school['DateOfService']).dt.days\n",
    "    df_final_sorted['days_since_school_vacation'] = (merged_backward_school['DateOfService'] - merged_backward_school['PreviousSchoolVacation']).dt.days\n",
    "\n",
    "    df_final = df_final_sorted.set_index('DateOfService')\n",
    "    fill_values = {'days_until_holiday': 0, 'days_since_holiday': 0, 'days_until_school_vacation': 0, 'days_since_school_vacation': 0}\n",
    "    df_final.fillna(fill_values, inplace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load holidays or process features, falling back. Error: {e}\")\n",
    "    df_final['days_until_holiday'], df_final['days_since_holiday'] = 0, 0\n",
    "    df_final['is_school_vacation'], df_final['days_until_school_vacation'], df_final['days_since_school_vacation'] = 0, 0, 0\n",
    "\n",
    "df_final['Site'] = df_final['Site'].astype('category')\n",
    "print(\"Advanced calendar features created.\")\n",
    "print(\"\\nPART 5 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 6: GLOBAL MODEL - STAGE 1 (Canceled Quantity Prediction)\n",
    "# =============================================================================\n",
    "print(\"\\nPART 6: Training GLOBAL Stage 1 model to predict CANCELED QUANTITY...\")\n",
    "start_time = time.time()\n",
    "cq_features_to_exclude = ['net_qty', 'OrderQty', 'CanceledQty', 'daily_cancel_pct']\n",
    "canceled_qty_pipeline = Pipeline([\n",
    "    ('daily_features', DailyFeatureEngineer(target_col='CanceledQty')),\n",
    "    # (MODIFIED) Replaced custom imputer with robust scikit-learn version\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('dropper', ColumnDropper(cols_to_drop=cq_features_to_exclude)),\n",
    "    ('regressor', LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "X_full, y_cq_full = df_final, df_final['CanceledQty']\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_cq = RandomizedSearchCV(estimator=canceled_qty_pipeline, param_distributions=param_dist_lgbm, n_iter=N_ITER_CQ, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
    "search_cq.fit(X_full, y_cq_full)\n",
    "global_canceled_qty_model = search_cq.best_estimator_\n",
    "end_time = time.time()\n",
    "print(f\"Global CanceledQty model tuned in {end_time - start_time:.2f}s.\")\n",
    "print(\"\\nPART 6 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 7: GLOBAL MODEL - STAGE 2 (Order Quantity Prediction)\n",
    "# =============================================================================\n",
    "print(\"\\nPART 7: Training GLOBAL Stage 2 model to predict ORDER QUANTITY...\")\n",
    "start_time = time.time()\n",
    "order_qty_features_to_exclude = ['net_qty', 'OrderQty', 'CanceledQty', 'daily_cancel_pct']\n",
    "order_qty_pipeline = Pipeline([\n",
    "    ('daily_features', DailyFeatureEngineer(target_col='OrderQty')),\n",
    "    # (MODIFIED) Replaced custom imputer with robust scikit-learn version\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('dropper', ColumnDropper(cols_to_drop=order_qty_features_to_exclude)),\n",
    "    ('regressor', LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "X_full, y_oq_full = df_final, df_final['OrderQty']\n",
    "search_oq = RandomizedSearchCV(estimator=order_qty_pipeline, param_distributions=param_dist_lgbm, n_iter=N_ITER_OQ, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
    "search_oq.fit(X_full, y_oq_full)\n",
    "global_order_quantity_model = search_oq.best_estimator_\n",
    "end_time = time.time()\n",
    "print(f\"Global OrderQty model tuned in {end_time - start_time:.2f}s.\")\n",
    "print(\"\\nPART 7 Complete.\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 7A: GENERATING BASE PREDICTIONS & ERRORS\n",
    "# =============================================================================\n",
    "print(\"\\nPART 7A: Generating out-of-sample errors for the error model...\")\n",
    "\n",
    "oos_cq_preds = pd.Series(index=X_full.index, dtype=float)\n",
    "oos_oq_preds = pd.Series(index=X_full.index, dtype=float)\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_full):\n",
    "    cq_model_fold = clone(global_canceled_qty_model)\n",
    "    oq_model_fold = clone(global_order_quantity_model)\n",
    "    X_train, y_cq_train, y_oq_train = X_full.iloc[train_idx], y_cq_full.iloc[train_idx], y_oq_full.iloc[train_idx]\n",
    "    X_val = X_full.iloc[val_idx]\n",
    "    cq_model_fold.fit(X_train, y_cq_train)\n",
    "    oq_model_fold.fit(X_train, y_oq_train)\n",
    "    oos_cq_preds.iloc[val_idx] = cq_model_fold.predict(X_val)\n",
    "    oos_oq_preds.iloc[val_idx] = oq_model_fold.predict(X_val)\n",
    "\n",
    "df_final['initial_net_pred'] = np.maximum(0, oos_oq_preds - oos_cq_preds)\n",
    "df_final['model_error'] = df_final['net_qty'] - df_final['initial_net_pred']\n",
    "print(\"Error column 'model_error' created for training the correction model.\")\n",
    "print(\"\\nPART 7A Complete.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 7B: TRAINING ERROR CORRECTION MODEL\n",
    "# =============================================================================\n",
    "print(\"\\nPART 7B: Training GLOBAL Stage 3 model to predict ERRORS...\")\n",
    "start_time = time.time()\n",
    "error_features_to_exclude = ['net_qty', 'OrderQty', 'CanceledQty', 'daily_cancel_pct', 'initial_net_pred', 'model_error']\n",
    "error_pipeline = Pipeline([\n",
    "    ('daily_features', DailyFeatureEngineer(target_col='net_qty')),\n",
    "    # (MODIFIED) Replaced custom imputer with robust scikit-learn version\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('dropper', ColumnDropper(cols_to_drop=error_features_to_exclude)),\n",
    "    ('regressor', LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "df_err_train = df_final.dropna(subset=['model_error'])\n",
    "X_err_full, y_err_full = df_err_train, df_err_train['model_error']\n",
    "\n",
    "search_err = RandomizedSearchCV(estimator=error_pipeline, param_distributions=param_dist_lgbm, n_iter=N_ITER_ERR, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
    "search_err.fit(X_err_full, y_err_full)\n",
    "global_error_model = search_err.best_estimator_\n",
    "end_time = time.time()\n",
    "print(f\"Global Error model tuned in {end_time - start_time:.2f}s.\")\n",
    "print(\"\\nPART 7B Complete.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 8: FINAL EVALUATION WITH ERROR CORRECTION\n",
    "# =============================================================================\n",
    "print(\"\\nPART 8: Evaluating 3-stage models with error correction...\")\n",
    "\n",
    "final_two_stage_scores = {}\n",
    "sites = df_daily['Site'].unique()\n",
    "\n",
    "for site in sites:\n",
    "    print(f\"\\n{'='*30} Final Evaluation for Site: {site} {'='*30}\")\n",
    "\n",
    "    df_site_final = df_final[df_final['Site'] == site].copy()\n",
    "    if len(df_site_final) < 100:\n",
    "        print(f\"    Skipping site {site} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    tscv_eval = TimeSeriesSplit(n_splits=5)\n",
    "    y_trues, y_preds = [], []\n",
    "\n",
    "    for train_idx, test_idx in tscv_eval.split(df_site_final):\n",
    "        test_data = df_site_final.iloc[test_idx]\n",
    "\n",
    "        pred_cq_test = global_canceled_qty_model.predict(test_data)\n",
    "        pred_oq_test = global_order_quantity_model.predict(test_data)\n",
    "        initial_net_pred_test = np.maximum(0, pred_oq_test - pred_cq_test)\n",
    "\n",
    "        error_correction_pred = global_error_model.predict(test_data)\n",
    "\n",
    "        final_predictions = initial_net_pred_test + error_correction_pred\n",
    "        final_predictions = np.maximum(0, final_predictions)\n",
    "\n",
    "        y_trues.append(test_data['net_qty'])\n",
    "        y_preds.append(pd.Series(final_predictions, index=test_data.index))\n",
    "\n",
    "    if not y_trues: continue\n",
    "\n",
    "    y_true_all, y_pred_all = pd.concat(y_trues), pd.concat(y_preds)\n",
    "    final_rmse = np.sqrt(mean_squared_error(y_true_all, y_pred_all))\n",
    "    final_asymmetric_loss = asymmetric_loss(y_true_all, y_pred_all)\n",
    "    final_two_stage_scores[site] = {'Asymmetric Loss': final_asymmetric_loss, 'RMSE': final_rmse}\n",
    "\n",
    "    print(f\"\\n--> Prediction Performance for Site: {site}\")\n",
    "    print(f\"    Final RMSE: {final_rmse:.4f}\")\n",
    "    print(f\"    Final Asymmetric Loss: {final_asymmetric_loss:.4f}\")\n",
    "\n",
    "    errors = y_true_all - y_pred_all\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    errors.plot(style='.', alpha=0.6, label='Error')\n",
    "    plt.axhline(0, color='red', linestyle='--', lw=2, label='Zero Error')\n",
    "    plt.title(f'Prediction Errors (Actual - Predicted) Over Time for Site: {site}', fontsize=16, weight='bold')\n",
    "    plt.ylabel('Error in Net Quantity')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n\\n{'='*30} OVERALL PERFORMANCE SUMMARY {'='*30}\")\n",
    "final_model_df = pd.DataFrame(final_two_stage_scores).T.rename(columns=lambda c: f\"Final 3-Stage Model {c}\")\n",
    "print(\"Final Model Performance Across All Sites:\")\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(final_model_df)\n",
    "except ImportError:\n",
    "    print(final_model_df)\n",
    "\n",
    "print(\"\\nPART 8 Complete.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 9: MODEL DIAGNOSTICS AND DEEPER ANALYSIS\n",
    "# =============================================================================\n",
    "print(\"\\n\\n\" + \"=\"*30 + \" PART 9: MODEL DIAGNOSTICS \" + \"=\"*30)\n",
    "\n",
    "def plot_feature_importance(model, feature_names, title):\n",
    "    df_importance = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
    "    df_importance = df_importance.sort_values('importance', ascending=False).head(20)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='importance', y='feature', data=df_importance, palette='viridis')\n",
    "    plt.title(title, fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# To get the correct feature names after the new imputer, we need to transform data first\n",
    "# CanceledQty Model\n",
    "trans_pipe_cq = Pipeline(global_canceled_qty_model.steps[:-1])\n",
    "feature_names_cq = trans_pipe_cq.fit_transform(X_full).columns\n",
    "plot_feature_importance(global_canceled_qty_model.named_steps['regressor'], feature_names_cq, 'Top 20 Features for CanceledQty Model')\n",
    "\n",
    "# OrderQty Model\n",
    "trans_pipe_oq = Pipeline(global_order_quantity_model.steps[:-1])\n",
    "feature_names_oq = trans_pipe_oq.fit_transform(X_full).columns\n",
    "plot_feature_importance(global_order_quantity_model.named_steps['regressor'], feature_names_oq, 'Top 20 Features for OrderQty Model')\n",
    "\n",
    "# Error Model\n",
    "trans_pipe_err = Pipeline(global_error_model.steps[:-1])\n",
    "feature_names_err = trans_pipe_err.fit_transform(X_err_full).columns\n",
    "plot_feature_importance(global_error_model.named_steps['regressor'], feature_names_err, 'Top 20 Features for Error Model')\n",
    "\n",
    "\n",
    "print(\"\\nSCRIPT COMPLETE.\")"
   ],
   "id": "b922ff7a760f85c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 1: Loading and preparing data...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "665a1960600b3580"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
