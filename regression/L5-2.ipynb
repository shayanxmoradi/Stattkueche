{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T18:40:24.209869Z",
     "start_time": "2025-07-18T18:40:18.229056Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# PART 1: SETUP, DATA LOADING & CUSTOM TRANSFORMERS (MODIFIED)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This section contains all necessary imports and loads the initial dataset.\n",
    "# Most importantly, it defines the powerful, custom scikit-learn transformers\n",
    "# from your original code. The MissingFlagImputer has been made more robust.\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from category_encoders import TargetEncoder\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "import shap\n",
    "\n",
    "# --- Setup ---\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set a consistent plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (18, 8)\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Data Loading ---\n",
    "print(\"PART 1: Loading and preparing data...\")\n",
    "try:\n",
    "    # This uses your real dataset. Ensure the path is correct.\n",
    "    df = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather2.csv', parse_dates=['DateOfService', 'DateOfOrder', 'DateOfCancel'])\n",
    "    print(\"Successfully loaded df_weather2.csv.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: df_weather2.csv not found. Please update the file path.\")\n",
    "    exit()\n",
    "\n",
    "# --- Initial Feature Creation ---\n",
    "# This is the target variable for the final regression model.\n",
    "df['net_qty'] = df['OrderQty'] - df['CanceledQty']\n",
    "print(\"Initial DataFrame shape:\", df.shape)\n",
    "\n",
    "\n",
    "# --- Custom Transformer Definitions (from your original code) ---\n",
    "# We define all your custom classes here to be used later in the pipeline.\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X_copy = X.copy()\n",
    "        X_copy[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X_copy\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=RANDOM_STATE).fit(scaled)\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X_copy[keys].values]\n",
    "        X_copy[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X_copy\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A target encoder that calculates encodings within cross-validation folds\n",
    "    to prevent data leakage.\n",
    "    \"\"\"\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols = cols\n",
    "        self.smoothing = smoothing\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df_temp = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df_temp.groupby(c)['target'].agg(['mean', 'count'])\n",
    "            agg['enc'] = ((agg['count'] * agg['mean'] + self.smoothing * self.global_mean_) /\n",
    "                          (agg['count'] + self.smoothing))\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        for c in self.cols:\n",
    "            X_copy[c] = X_copy[c].map(self.mapping_[c]).fillna(self.global_mean_)\n",
    "        return X_copy\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    (FIXED) Imputes missing values and adds flags, ensuring consistent columns\n",
    "    across different data splits to prevent errors.\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=np.number).columns\n",
    "        # Learn which columns have missing values in the training data\n",
    "        self.cols_to_flag_ = [c for c in self.num_cols if X[c].isnull().any()]\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(X[self.num_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        # Create flags for all columns that were identified during fit\n",
    "        for c in self.cols_to_flag_:\n",
    "            X_copy[c + '_missing'] = X_copy[c].isnull().astype(int)\n",
    "\n",
    "        # Impute the main numeric columns\n",
    "        X_copy[self.num_cols] = self.imputer_.transform(X_copy[self.num_cols])\n",
    "        return X_copy\n",
    "\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "print(\"\\nPART 1 Complete: All necessary libraries and custom classes are defined.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2: DAILY AGGREGATION, EDA, AND VIF ANALYSIS\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part creates the SIMPLE daily dataset for our baseline models.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 2: Aggregating data to daily level for baseline models...\")\n",
    "agg_dict = {\n",
    "    'net_qty': 'sum', 'OrderQty': 'sum', 'CanceledQty': 'sum', 'MenuPrice': 'mean',\n",
    "    'MenuSubsidy': 'mean', 'tavg_C': 'mean', 'prcp_mm': 'sum', 'rain_flag': 'max',\n",
    "    'temp_dev': 'mean', 'is_holiday': 'max', 'is_weekend': 'max', 'MenuBase': 'nunique',\n",
    "    'SchoolID': 'nunique', 'hist_cancel_rate': 'mean'\n",
    "}\n",
    "df_daily = df.groupby(['DateOfService', 'Site']).agg(agg_dict).reset_index()\n",
    "df_daily['daily_cancel_pct'] = (df_daily['CanceledQty'] / df_daily['OrderQty']).fillna(0)\n",
    "df_daily = df_daily.sort_values('DateOfService').set_index('DateOfService')\n",
    "print(\"\\nPART 2 Complete: Baseline daily data is ready.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 3: FEATURE ENGINEERING PIPELINE AND MODEL DEFINITION\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part defines the pipeline for the baseline models.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 3: Defining the baseline feature engineering and regression pipeline...\")\n",
    "\n",
    "class DailyFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = X.copy()\n",
    "        X_copy['day_of_year'] = X_copy.index.dayofyear\n",
    "        X_copy['weekday'] = X_copy.index.weekday\n",
    "        X_copy['month'] = X_copy.index.month\n",
    "        X_copy['year'] = X_copy.index.year\n",
    "        X_copy['week_of_year'] = X_copy.index.isocalendar().week.astype(int)\n",
    "        X_copy['quarter'] = X_copy.index.quarter\n",
    "        for lag in [1, 2, 7, 14, 28]:\n",
    "            X_copy[f'lag_{lag}'] = X_copy.groupby('Site')['net_qty'].shift(lag)\n",
    "        X_copy['rolling_mean_7'] = X_copy.groupby('Site')['net_qty'].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "        X_copy['rolling_std_7'] = X_copy.groupby('Site')['net_qty'].shift(1).rolling(window=7, min_periods=1).std()\n",
    "        return X_copy\n",
    "\n",
    "def asymmetric_loss(y_true, y_pred, under_penalty=1.2):\n",
    "    error = y_true - y_pred\n",
    "    loss = np.mean(np.where(error < 0, -error * under_penalty, error))\n",
    "    return loss\n",
    "asymmetric_scorer = make_scorer(asymmetric_loss, greater_is_better=False)\n",
    "\n",
    "features_to_exclude = ['net_qty', 'Site', 'OrderQty', 'CanceledQty', 'daily_cancel_pct']\n",
    "regression_pipeline = Pipeline([\n",
    "    ('daily_features', DailyFeatureEngineer()),\n",
    "    ('imputer', MissingFlagImputer(strategy='median')),\n",
    "    ('dropper', ColumnDropper(cols_to_drop=features_to_exclude)),\n",
    "    ('regressor', RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "print(\"\\nPART 3 Complete: Baseline pipeline is ready.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4: BASELINE MODEL TRAINING & EVALUATION\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part evaluates the simple baseline model.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 4: Starting BASELINE model training and evaluation...\")\n",
    "sites = df_daily['Site'].unique()\n",
    "baseline_scores = {}\n",
    "for site in sites:\n",
    "    df_site = df_daily[df_daily['Site'] == site].copy()\n",
    "    X, y = df_site, df_site['net_qty']\n",
    "    if len(X) < 100: continue\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    custom_scores = -cross_val_score(regression_pipeline, X, y, cv=tscv, scoring=asymmetric_scorer, n_jobs=-1)\n",
    "    rmse_scores = -cross_val_score(regression_pipeline, X, y, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    baseline_scores[site] = {'Asymmetric Loss': custom_scores.mean(), 'RMSE': rmse_scores.mean()}\n",
    "print(\"\\nPART 4 Complete: Baseline model performance established.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 5: HYPERPARAMETER TUNING FOR BASELINE MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part tunes the simple baseline model.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 5: Starting hyperparameter tuning for baseline model...\")\n",
    "param_dist_rf = {\n",
    "    'regressor__n_estimators': [100, 200, 300, 500], 'regressor__max_features': ['sqrt', 'log2', 0.7, 0.8],\n",
    "    'regressor__max_depth': [None, 10, 20, 30], 'regressor__min_samples_split': [2, 5, 10],\n",
    "    'regressor__min_samples_leaf': [1, 2, 4], 'regressor__bootstrap': [True, False]\n",
    "}\n",
    "tuned_rf_models = {}\n",
    "tuned_scores = {}\n",
    "for site in sites:\n",
    "    df_site = df_daily[df_daily['Site'] == site].copy()\n",
    "    X, y = df_site, df_site['net_qty']\n",
    "    if len(X) < 100: continue\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=regression_pipeline, param_distributions=param_dist_rf, n_iter=50,\n",
    "        cv=tscv, scoring=asymmetric_scorer, n_jobs=-1, random_state=RANDOM_STATE, verbose=0\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    tuned_rf_models[site] = search.best_estimator_\n",
    "    tuned_rmse = -cross_val_score(search.best_estimator_, X, y, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1).mean()\n",
    "    tuned_scores[site] = {'Asymmetric Loss': -search.best_score_, 'RMSE': tuned_rmse}\n",
    "print(\"\\nPART 5 Complete: Hyperparameter tuning for RandomForest finished.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 6: ADVANCED FEATURE ENGINEERING (MODIFIED)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This step creates the advanced feature set by using the custom transformers\n",
    "# on the original order-level data before aggregating.\n",
    "# (FIX) Added calculation for 'daily_cancel_pct' to this dataframe.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 6: Performing advanced feature engineering...\")\n",
    "# Step 1: Create order-level features\n",
    "hist_transformer = HistCancelRateTransformer(group_keys=('Site', 'MenuBase'), value_col='net_qty')\n",
    "df_adv = hist_transformer.fit_transform(df)\n",
    "cluster_transformer = ClusterTransformer(\n",
    "    profile_feats=['hist_cancel_rate', 'rain_flag', 'temp_dev', 'MenuPrice', 'MenuSubsidy'],\n",
    "    group_keys=('Site', 'MenuBase'), n_clusters=5\n",
    ")\n",
    "df_adv = cluster_transformer.fit_transform(df_adv)\n",
    "# Step 2: Aggregate the newly enriched data to the daily level\n",
    "adv_agg_dict = agg_dict.copy()\n",
    "adv_agg_dict['cluster_id'] = pd.Series.mode\n",
    "df_daily_adv = df_adv.groupby(['DateOfService', 'Site']).agg(adv_agg_dict).reset_index()\n",
    "# (FIX) Calculate daily_cancel_pct for the advanced dataframe\n",
    "df_daily_adv['daily_cancel_pct'] = (df_daily_adv['CanceledQty'] / df_daily_adv['OrderQty']).fillna(0)\n",
    "df_daily_adv['cluster_id'] = df_daily_adv['cluster_id'].apply(lambda x: x[0] if isinstance(x, np.ndarray) and len(x) > 0 else x).astype(int)\n",
    "df_daily_adv = df_daily_adv.sort_values('DateOfService').set_index('DateOfService')\n",
    "print(\"\\nPART 6 Complete: Advanced feature set created.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 7: ADVANCED MODEL TRAINING (UNTUNED)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part evaluates the untuned advanced model to see its baseline performance.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 7: Building and evaluating an untuned advanced pipeline with LightGBM...\")\n",
    "features_to_exclude_adv = features_to_exclude + ['MenuBase']\n",
    "advanced_pipeline_lgbm = Pipeline([\n",
    "    ('daily_features', DailyFeatureEngineer()),\n",
    "    ('imputer', MissingFlagImputer(strategy='median')),\n",
    "    ('dropper', ColumnDropper(cols_to_drop=features_to_exclude_adv)),\n",
    "    ('regressor', LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "advanced_lgbm_models = {}\n",
    "advanced_scores = {}\n",
    "for site in sites:\n",
    "    df_site_adv = df_daily_adv[df_daily_adv['Site'] == site].copy()\n",
    "    X, y = df_site_adv, df_site_adv['net_qty']\n",
    "    if len(X) < 100: continue\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    adv_custom_scores = -cross_val_score(advanced_pipeline_lgbm, X, y, cv=tscv, scoring=asymmetric_scorer, n_jobs=-1)\n",
    "    adv_rmse_scores = -cross_val_score(advanced_pipeline_lgbm, X, y, cv=tscv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    advanced_scores[site] = {'Asymmetric Loss': adv_custom_scores.mean(), 'RMSE': adv_rmse_scores.mean()}\n",
    "    advanced_lgbm_models[site] = advanced_pipeline_lgbm.fit(X, y)\n",
    "print(\"\\nPART 7 Complete: Untuned advanced model performance established.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 8: INTERIM ANALYSIS AND VISUALIZATION (MODIFIED)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part generates a comprehensive analysis of the models built so far.\n",
    "# The variable name has been fixed to prevent the NameError.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 8: Generating Interim Analysis and Visualizations...\")\n",
    "# --- Final Summary Table ---\n",
    "print(f\"\\n{'='*30} INTERIM PERFORMANCE SUMMARY {'='*30}\")\n",
    "baseline_df = pd.DataFrame(baseline_scores).T.rename(columns=lambda c: f\"Baseline RF {c}\")\n",
    "tuned_df = pd.DataFrame(tuned_scores).T.rename(columns=lambda c: f\"Tuned RF {c}\")\n",
    "advanced_df = pd.DataFrame(advanced_scores).T.rename(columns=lambda c: f\"Advanced LGBM {c}\")\n",
    "# (FIX) Renamed this variable for consistency\n",
    "final_summary = pd.concat([baseline_df, tuned_df, advanced_df], axis=1)\n",
    "print(\"Comparison of Model Performances:\")\n",
    "display(final_summary)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 9 (FIX): ADVANCED CALENDAR FEATURES & NEW TARGET DEFINITION\n",
    "# -----------------------------------------------------------------------------\n",
    "# This new part creates more sophisticated holiday features and defines our\n",
    "# new target variable: the daily cancellation rate, as suggested by the teacher.\n",
    "# This approach avoids the leakage issues from hist_cancel_rate and SchoolID.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 9: Creating advanced calendar features and new cancellation rate target...\")\n",
    "\n",
    "# --- Advanced Calendar Features ---\n",
    "# Use the advanced daily dataframe as our base\n",
    "df_final = df_daily_adv.copy()\n",
    "\n",
    "# Create a series of holiday dates for NRW, Germany\n",
    "# Note: In a real project, this would use a library like `holidays` for accuracy\n",
    "holiday_dates = pd.to_datetime(['2020-01-01', '2020-04-10', '2020-04-13', '2020-05-01', '2020-05-21', '2020-06-01', '2020-06-11', '2020-10-03', '2020-11-01', '2020-12-25', '2020-12-26',\n",
    "                                '2021-01-01', '2021-04-02', '2021-04-05', '2021-05-01', '2021-05-13', '2021-05-24', '2021-06-03', '2021-10-03', '2021-11-01', '2021-12-25', '2021-12-26',\n",
    "                                '2022-01-01', '2022-04-15', '2022-04-18', '2022-05-01', '2022-05-26', '2022-06-06', '2022-06-16', '2022-10-03', '2022-11-01', '2022-12-25', '2022-12-26',\n",
    "                                '2023-01-01', '2023-04-07', '2023-04-10', '2023-05-01', '2023-05-18', '2023-05-29', '2023-06-08', '2023-10-03', '2023-11-01', '2023-12-25', '2023-12-26'])\n",
    "\n",
    "# Calculate days to/from nearest holiday\n",
    "date_index = df_final.index.to_series()\n",
    "df_final['days_until_holiday'] = date_index.apply(lambda x: np.min(np.abs((holiday_dates - x).days)[holiday_dates > x]) if any(holiday_dates > x) else 0)\n",
    "df_final['days_since_holiday'] = date_index.apply(lambda x: np.min(np.abs((x - holiday_dates).days)[holiday_dates < x]) if any(holiday_dates < x) else 0)\n",
    "\n",
    "print(\"Advanced calendar features created:\")\n",
    "display(df_final[['is_holiday', 'days_until_holiday', 'days_since_holiday']].head())\n",
    "\n",
    "# --- New Target Variable ---\n",
    "# The target is now the daily cancellation percentage\n",
    "df_final['target_cancel_pct'] = df_final['daily_cancel_pct']\n",
    "\n",
    "print(\"\\nPART 9 Complete: Advanced features and new target are ready.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 10: TWO-STAGE FORECASTING MODEL\n",
    "# -----------------------------------------------------------------------------\n",
    "# This part implements the final, robust two-stage model.\n",
    "# Stage 1: Predict the cancellation rate.\n",
    "# Stage 2: Predict the total order quantity.\n",
    "# The two stages are combined for the final net_qty forecast.\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPART 10: Training and evaluating the final two-stage forecasting model...\")\n",
    "\n",
    "# --- Define the Final Pipeline for Predicting Cancellation Rate ---\n",
    "# We exclude features that could leak information about cancellations\n",
    "rate_features_to_exclude = features_to_exclude + ['MenuBase', 'SchoolID', 'hist_cancel_rate', 'target_cancel_pct', 'daily_cancel_pct']\n",
    "rate_pipeline = Pipeline([\n",
    "    ('daily_features', DailyFeatureEngineer()),\n",
    "    ('imputer', MissingFlagImputer(strategy='median')),\n",
    "    ('dropper', ColumnDropper(cols_to_drop=rate_features_to_exclude)),\n",
    "    ('regressor', LGBMRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "final_two_stage_scores = {}\n",
    "final_two_stage_models = {}\n",
    "\n",
    "for site in sites:\n",
    "    print(f\"\\n{'='*30} Evaluating Two-Stage Model for Site: {site} {'='*30}\")\n",
    "\n",
    "    df_site_final = df_final[df_final['Site'] == site].copy()\n",
    "\n",
    "    if len(df_site_final) < 100:\n",
    "        print(f\"Skipping site {site} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    # --- Custom Cross-Validation Loop for Two-Stage Model ---\n",
    "    y_trues, y_preds = [], []\n",
    "    for train_idx, test_idx in tscv.split(df_site_final):\n",
    "        train_data, test_data = df_site_final.iloc[train_idx], df_site_final.iloc[test_idx]\n",
    "\n",
    "        # --- Stage 1: Train and Predict Cancellation Rate ---\n",
    "        X_rate_train, y_rate_train = train_data, train_data['target_cancel_pct']\n",
    "        rate_pipeline.fit(X_rate_train, y_rate_train)\n",
    "        predicted_cancel_pct = rate_pipeline.predict(test_data)\n",
    "        # Ensure predictions are between 0 and 1\n",
    "        predicted_cancel_pct = np.clip(predicted_cancel_pct, 0, 1)\n",
    "\n",
    "        # --- Stage 2: Predict Order Quantity (using a simple baseline) ---\n",
    "        # Baseline: OrderQty will be the same as last week's OrderQty\n",
    "        train_data['lag_7_order_qty'] = train_data['OrderQty'].shift(7)\n",
    "        last_known_lag = train_data['lag_7_order_qty'].dropna().iloc[-1]\n",
    "        predicted_order_qty = pd.Series(last_known_lag, index=test_data.index) # Simple propagation\n",
    "\n",
    "        # --- Combine for Final Forecast ---\n",
    "        final_predictions = predicted_order_qty * (1 - predicted_cancel_pct)\n",
    "\n",
    "        y_trues.append(test_data['net_qty'])\n",
    "        y_preds.append(final_predictions)\n",
    "\n",
    "    # Concatenate results from all folds\n",
    "    y_true_all = pd.concat(y_trues)\n",
    "    y_pred_all = pd.concat(y_preds)\n",
    "\n",
    "    # Calculate final scores on the original scale\n",
    "    final_rmse = np.sqrt(np.mean((y_true_all - y_pred_all)**2))\n",
    "    final_asymmetric_loss = asymmetric_loss(y_true_all, y_pred_all)\n",
    "\n",
    "    final_two_stage_scores[site] = {'Asymmetric Loss': final_asymmetric_loss, 'RMSE': final_rmse}\n",
    "    print(f\"    - Final Two-Stage Model Asymmetric Loss: {final_asymmetric_loss:.2f}\")\n",
    "    print(f\"    - Final Two-Stage Model RMSE: {final_rmse:.2f} meals\")\n",
    "\n",
    "    # Fit the final rate model on all data for SHAP analysis\n",
    "    final_two_stage_models[site] = rate_pipeline.fit(df_site_final, df_site_final['target_cancel_pct'])\n",
    "\n",
    "\n",
    "# --- Final Summary Table with All Models ---\n",
    "print(f\"\\n\\n{'='*30} OVERALL PERFORMANCE SUMMARY {'='*30}\")\n",
    "final_model_df = pd.DataFrame(final_two_stage_scores).T.rename(columns=lambda c: f\"Final Two-Stage LGBM {c}\")\n",
    "overall_summary = pd.concat([final_summary, final_model_df], axis=1)\n",
    "print(\"Comparison of All Model Performances:\")\n",
    "display(overall_summary)\n",
    "\n",
    "# --- Detailed Visualization of the FINAL Two-Stage Model ---\n",
    "for site, best_pipeline in final_two_stage_models.items():\n",
    "    print(f\"\\n{'='*30} SHAP Analysis for FINAL Two-Stage Model at Site: {site} {'='*30}\")\n",
    "\n",
    "    df_site = df_final[df_final['Site'] == site].copy()\n",
    "    X, y = df_site, df_site['target_cancel_pct']\n",
    "\n",
    "    # Transform data for SHAP\n",
    "    X_featured = best_pipeline.named_steps['daily_features'].transform(X)\n",
    "    X_imputed = best_pipeline.named_steps['imputer'].transform(X_featured)\n",
    "    X_transformed = best_pipeline.named_steps['dropper'].transform(X_imputed)\n",
    "\n",
    "    regressor = best_pipeline.named_steps['regressor']\n",
    "    explainer = shap.TreeExplainer(regressor)\n",
    "    shap_values = explainer.shap_values(X_transformed)\n",
    "\n",
    "    plt.title(f'SHAP Feature Importance for FINAL Two-Stage Model at Site: {site}', fontsize=16, weight='bold')\n",
    "    shap.summary_plot(shap_values, X_transformed, plot_type=\"bar\", show=False)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSCRIPT COMPLETE: Final two-stage model has been evaluated.\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayan/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART 1: Loading and preparing data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 40\u001B[39m\n\u001B[32m     37\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPART 1: Loading and preparing data...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     38\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     39\u001B[39m     \u001B[38;5;66;03m# This uses your real dataset. Ensure the path is correct.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m     df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/Users/shayan/Desktop/IDS2/Stattkueche/df_weather2.csv\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mDateOfService\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mDateOfOrder\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mDateOfCancel\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSuccessfully loaded df_weather2.csv.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/parsers/readers.py:626\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[32m    625\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1923\u001B[39m, in \u001B[36mTextFileReader.read\u001B[39m\u001B[34m(self, nrows)\u001B[39m\n\u001B[32m   1916\u001B[39m nrows = validate_integer(\u001B[33m\"\u001B[39m\u001B[33mnrows\u001B[39m\u001B[33m\"\u001B[39m, nrows)\n\u001B[32m   1917\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1918\u001B[39m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[32m   1919\u001B[39m     (\n\u001B[32m   1920\u001B[39m         index,\n\u001B[32m   1921\u001B[39m         columns,\n\u001B[32m   1922\u001B[39m         col_dict,\n\u001B[32m-> \u001B[39m\u001B[32m1923\u001B[39m     ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[32m   1924\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[32m   1925\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1926\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[32m   1927\u001B[39m     \u001B[38;5;28mself\u001B[39m.close()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001B[39m, in \u001B[36mCParserWrapper.read\u001B[39m\u001B[34m(self, nrows)\u001B[39m\n\u001B[32m    232\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    233\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.low_memory:\n\u001B[32m--> \u001B[39m\u001B[32m234\u001B[39m         chunks = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_reader\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    235\u001B[39m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[32m    236\u001B[39m         data = _concatenate_chunks(chunks)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:838\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader.read_low_memory\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:905\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._read_rows\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:874\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:891\u001B[39m, in \u001B[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mparsers.pyx:2053\u001B[39m, in \u001B[36mpandas._libs.parsers.raise_parser_error\u001B[39m\u001B[34m()\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<frozen codecs>:334\u001B[39m, in \u001B[36mgetstate\u001B[39m\u001B[34m(self)\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T18:37:08.936711Z",
     "start_time": "2025-07-18T18:36:46.645016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# PART 11: IN-DEPTH FINAL MODEL ANALYSIS (NEW)\n",
    "# -----------------------------------------------------------------------------\n",
    "# This new final part provides a deep dive into the performance and logic\n",
    "# of the best model (the Two-Stage Model from Part 10), with detailed\n",
    "# visualizations and metrics as requested.\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\n\\n{'='*30} PART 11: IN-DEPTH ANALYSIS OF THE FINAL TWO-STAGE MODEL {'='*30}\")\n",
    "\n",
    "# --- Final Performance Table ---\n",
    "final_model_df = pd.DataFrame(final_two_stage_scores).T.rename(columns=lambda c: f\"Final Two-Stage LGBM {c}\")\n",
    "overall_summary = pd.concat([final_summary, final_model_df], axis=1)\n",
    "print(\"--- Overall Performance Summary ---\")\n",
    "display(overall_summary)\n",
    "\n",
    "# --- Detailed Visualization Loop ---\n",
    "for site, final_model_pipeline in final_two_stage_models.items():\n",
    "    print(f\"\\n{'='*30} Detailed Analysis for Site: {site} {'='*30}\")\n",
    "\n",
    "    df_site = df_final[df_final['Site'] == site].copy()\n",
    "    X, y = df_site, df_site['target_cancel_pct']\n",
    "\n",
    "    # --- Prepare Data for SHAP and Other Plots ---\n",
    "    X_featured = final_model_pipeline.named_steps['daily_features'].transform(X)\n",
    "    X_imputed = final_model_pipeline.named_steps['imputer'].transform(X_featured)\n",
    "    X_transformed = final_model_pipeline.named_steps['dropper'].transform(X_imputed)\n",
    "    regressor = final_model_pipeline.named_steps['regressor']\n",
    "\n",
    "    # --- 1. SHAP Feature Importance ---\n",
    "    print(\"\\n--> 1. SHAP Feature Importance (What the model thinks is important)\")\n",
    "    explainer = shap.TreeExplainer(regressor)\n",
    "    shap_values = explainer.shap_values(X_transformed)\n",
    "    plt.title(f'SHAP Feature Importance for Final Two-Stage Model at Site: {site}', fontsize=16, weight='bold')\n",
    "    shap.summary_plot(shap_values, X_transformed, plot_type=\"bar\", show=False)\n",
    "    plt.show()\n",
    "\n",
    "    # --- 2. SHAP Dependence Plots for Top Features ---\n",
    "    print(\"\\n--> 2. SHAP Dependence Plots (How top features affect predictions)\")\n",
    "    top_features = X_transformed.columns[np.argsort(np.abs(shap_values).mean(0))][::-1]\n",
    "    for feature in top_features[:3]: # Plot top 3 features\n",
    "        shap.dependence_plot(feature, shap_values, X_transformed, interaction_index=None, show=False)\n",
    "        plt.title(f'SHAP Dependence Plot for \"{feature}\" at Site: {site}', fontsize=14)\n",
    "        plt.show()\n",
    "\n",
    "    # --- 3. Error Analysis ---\n",
    "    # Re-run the CV loop to get the final predictions and errors for plotting\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    y_trues, y_preds = [], []\n",
    "    for train_idx, test_idx in tscv.split(df_site):\n",
    "        train_data, test_data = df_site.iloc[train_idx], df_site.iloc[test_idx]\n",
    "        X_rate_train, y_rate_train = train_data, train_data['target_cancel_pct']\n",
    "        final_model_pipeline.fit(X_rate_train, y_rate_train)\n",
    "        predicted_cancel_pct = np.clip(final_model_pipeline.predict(test_data), 0, 1)\n",
    "        train_data['lag_7_order_qty'] = train_data['OrderQty'].shift(7)\n",
    "        last_known_lag = train_data['lag_7_order_qty'].dropna().iloc[-1]\n",
    "        predicted_order_qty = pd.Series(last_known_lag, index=test_data.index)\n",
    "        final_predictions = predicted_order_qty * (1 - predicted_cancel_pct)\n",
    "        y_trues.append(test_data['net_qty'])\n",
    "        y_preds.append(final_predictions)\n",
    "\n",
    "    y_true_all = pd.concat(y_trues)\n",
    "    y_pred_all = pd.concat(y_preds)\n",
    "    errors = y_true_all - y_pred_all\n",
    "\n",
    "    print(\"\\n--> 3. Prediction Error Analysis\")\n",
    "    print(\"Descriptive Statistics of Prediction Errors:\")\n",
    "    display(errors.describe())\n",
    "\n",
    "    # Plot: Error Time Series\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    errors.plot(style='.', alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.title(f'Prediction Errors Over Time for Site: {site}', fontsize=16, weight='bold')\n",
    "    plt.ylabel('Error (Actual - Predicted)')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot: Actual vs. Predicted Scatter Plot\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(y_true_all, y_pred_all, alpha=0.5)\n",
    "    plt.plot([y_true_all.min(), y_true_all.max()], [y_true_all.min(), y_true_all.max()], '--', color='red', lw=2)\n",
    "    plt.title(f'Actual vs. Predicted Net Quantity for Site: {site}', fontsize=16, weight='bold')\n",
    "    plt.xlabel('Actual Net Quantity')\n",
    "    plt.ylabel('Predicted Net Quantity')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nSCRIPT COMPLETE: Final in-depth analysis has been generated.\")\n"
   ],
   "id": "f0f71d8a8630e222",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================== PART 11: IN-DEPTH ANALYSIS OF THE FINAL TWO-STAGE MODEL ==============================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 12\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m30\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m PART 11: IN-DEPTH ANALYSIS OF THE FINAL TWO-STAGE MODEL \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m=\u001B[39m\u001B[33m'\u001B[39m*\u001B[32m30\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# --- Final Performance Table ---\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m final_model_df = \u001B[43mpd\u001B[49m.DataFrame(final_two_stage_scores).T.rename(columns=\u001B[38;5;28;01mlambda\u001B[39;00m c: \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mFinal Two-Stage LGBM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     13\u001B[39m overall_summary = pd.concat([final_summary, final_model_df], axis=\u001B[32m1\u001B[39m)\n\u001B[32m     14\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m--- Overall Performance Summary ---\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'pd' is not defined"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
