{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "\n",
    "# Sklearn & related imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "\n",
    "# Load data\n",
    "# ‚ö†Ô∏è Ensure the file path is correct for your system.\n",
    "df = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather3.csv', parse_dates=['DateOfService'])\n",
    "\n"
   ],
   "id": "edc7f849d9f5402b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 2. REUSABLE CUSTOM TRANSFORMERS\n",
    "# (Copied from the original script)\n",
    "# =============================================================================\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        df['target'] = y # Use the regression target 'CanceledQty'\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = df.groupby(keys)['target'].mean()\n",
    "        self.default_ = y.mean()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X      = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        # Ensure profile feats exist before grouping\n",
    "        existing_profile_feats = [f for f in self.profile_feats if f in X.columns]\n",
    "        prof = (X.groupby(keys)[existing_profile_feats].mean().reset_index())\n",
    "        prof[existing_profile_feats] = prof[existing_profile_feats].fillna(prof[existing_profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[existing_profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[existing_profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42, n_init='auto').fit(scaled)\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        # Make the cluster_id categorical for the model\n",
    "        X[self.out_col] = X[self.out_col].astype('category')\n",
    "        return X\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            if X[c].isna().any():\n",
    "                X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "class FeatureNameSanitizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', str(col)) for col in df.columns]\n",
    "        return df\n",
    "\n"
   ],
   "id": "a43cfc127418fc9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 3. BASELINE MODEL: DATA PREPARATION\n",
    "# =============================================================================\n",
    "# The target variable is the actual quantity canceled.\n",
    "TARGET = 'CanceledQty'\n",
    "\n",
    "# Sort data by service date for time-series splitting\n",
    "df = df.sort_values('DateOfService').reset_index(drop=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "y = df[TARGET]\n",
    "X = df.drop(columns=[TARGET])\n",
    "\n",
    "# Define feature types for the pipeline\n",
    "# These columns will be dropped as they are identifiers, high cardinality,\n",
    "# redundant, or cause target leakage.\n",
    "COLS_TO_DROP = [\n",
    "    'OrderId', 'TransactionId', 'BookingNr', 'SchoolID', 'GroupName',\n",
    "    'DateOfOrder', 'DateOfCancel', 'DateOfService',\n",
    "    'MenuName', 'MenuNorm', 'MenuCode',\n",
    "    'cancel_timing' # This is post-event info (leakage)\n",
    "]\n",
    "\n",
    "# Features to be used for creating customer profile clusters\n",
    "CLUSTER_PROFILE_FEATS = [\n",
    "    'hist_cancel_rate', 'rain_flag', 'temp_dev', 'sin_doy', 'cos_doy',\n",
    "    'month', 'day_of_month', 'tavg_C', 'prcp_mm'\n",
    "]\n",
    "\n",
    "# LightGBM can handle categorical features directly, which is very efficient.\n",
    "CATEGORICAL_FEATURES = ['Site', 'MenuBase']\n",
    "\n"
   ],
   "id": "fc81f8f778e337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 4. BASELINE MODEL: PIPELINE DEFINITION & TRAINING\n",
    "# =============================================================================\n",
    "# We build a single, streamlined pipeline.\n",
    "# Note: For this baseline, we use default LGBM parameters.\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Step 1: Drop irrelevant or leaky columns first.\n",
    "    ('initial_drop', ColumnDropper(cols_to_drop=COLS_TO_DROP)),\n",
    "\n",
    "    # Step 2: Create the historical cancellation rate feature.\n",
    "    # We've adapted this to use the regression target 'CanceledQty'.\n",
    "    ('hist_rate', HistCancelRateTransformer(value_col=TARGET)),\n",
    "\n",
    "    # Step 3: Create customer profile clusters.\n",
    "    ('cluster', ClusterTransformer(profile_feats=CLUSTER_PROFILE_FEATS)),\n",
    "\n",
    "    # Step 4: Impute missing numerical values and add flags.\n",
    "    ('impute', MissingFlagImputer(strategy='median')),\n",
    "\n",
    "    # Step 5: Sanitize feature names to remove special characters.\n",
    "    ('sanitize', FeatureNameSanitizer()),\n",
    "\n",
    "    # Step 6: The regression model.\n",
    "    ('regressor', lgb.LGBMRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation to respect the data's temporal order.\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# --- Cross-Validation ---\n",
    "# We calculate negative RMSE because scikit-learn convention maximizes scores.\n",
    "print(\"üöÄ Starting 5-fold time-series cross-validation...\")\n",
    "scores = cross_val_score(\n",
    "    pipeline, X, y,\n",
    "    cv=tscv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "avg_rmse = -scores.mean()\n",
    "print(f\"‚úÖ Cross-validation complete.\\nAverage RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "\n",
    "# --- Final Model Training ---\n",
    "# Train the pipeline on the full dataset to create the final model.\n",
    "print(\"\\n‚öôÔ∏è Training final model on the entire dataset...\")\n",
    "pipeline.fit(X, y)\n",
    "print(\"‚úÖ Final model trained.\")\n",
    "\n"
   ],
   "id": "6cf612742542ee5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 5. BASELINE MODEL: EVALUATION & VISUALIZATION\n",
    "# =============================================================================\n",
    "# To evaluate, we need predictions. We'll get them from the last fold of TSCV.\n",
    "train_indices, val_indices = list(tscv.split(X))[-1]\n",
    "X_train, X_val = X.iloc[train_indices], X.iloc[val_indices]\n",
    "y_train, y_val = y.iloc[train_indices], y.iloc[val_indices]\n",
    "\n",
    "# Train a temporary pipeline on the training part of the split to get predictions\n",
    "# for the validation set. This avoids predicting on data the model has seen.\n",
    "print(\"\\nüìä Generating predictions on a hold-out validation set for evaluation...\")\n",
    "eval_pipeline = pipeline.fit(X_train, y_train)\n",
    "y_pred = eval_pipeline.predict(X_val)\n",
    "\n",
    "# Ensure predictions are non-negative integers (as CanceledQty can't be fractional or negative)\n",
    "y_pred = np.maximum(0, y_pred).round()\n",
    "\n",
    "# --- Performance Metrics ---\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Performance on Validation Set ---\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE):     {mae:.4f}\")\n",
    "print(f\"R-squared (R¬≤):                {r2:.4f}\")\n",
    "print(\"-------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Visualizations ---\n",
    "\n",
    "# 1. Predicted vs. Actual Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.regplot(x=y_val, y=y_pred,\n",
    "            scatter_kws={'alpha': 0.2, 's': 10, 'color': 'skyblue'},\n",
    "            line_kws={'color': 'red', 'linewidth': 2, 'linestyle': '--'})\n",
    "plt.plot([min(y_val), max(y_val)], [min(y_val), max(y_val)], color='black', linestyle='-', linewidth=1)\n",
    "plt.title('Predicted vs. Actual Canceled Quantities', fontsize=16, pad=20)\n",
    "plt.xlabel('Actual Values', fontsize=12)\n",
    "plt.ylabel('Predicted Values', fontsize=12)\n",
    "plt.legend(['Regression Line', 'Perfect Fit (y=x)'])\n",
    "plt.show()\n",
    "\n",
    "# 2. Residuals Plot\n",
    "residuals = y_val - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_pred, y=residuals, alpha=0.3, s=15, color='green')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs. Predicted Values', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted Values', fontsize=12)\n",
    "plt.ylabel('Residuals (Actual - Predicted)', fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# 3. Feature Importance Plot\n",
    "# Extract the trained model and feature names from the final pipeline\n",
    "lgbm_model = pipeline.named_steps['regressor']\n",
    "sanitized_cols = pipeline.named_steps['sanitize'].transform(\n",
    "    pipeline.named_steps['impute'].transform(\n",
    "        pipeline.named_steps['cluster'].transform(\n",
    "            pipeline.named_steps['hist_rate'].transform(\n",
    "                pipeline.named_steps['initial_drop'].transform(X)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ").columns\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': sanitized_cols,\n",
    "    'importance': lgbm_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(20)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n",
    "plt.title('Top 20 Most Important Features', fontsize=16, pad=20)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "44fb9ba05aa36f23"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
