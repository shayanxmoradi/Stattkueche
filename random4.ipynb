{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-08T10:52:16.971127Z",
     "start_time": "2025-06-08T10:52:16.968954Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T10:52:42.784773Z",
     "start_time": "2025-06-08T10:52:17.091988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sns\n",
    "import train\n",
    "import holidays\n",
    "\n",
    "df = pd.read_csv('cleaneddataxxx.csv')\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "de_holidays = holidays.Germany(prov='NW')\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n"
   ],
   "id": "b63e09b2971c0cf7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/yzd4h3k95051tj7kpg425rk80000gn/T/ipykernel_2412/2549309988.py:6: DtypeWarning: Columns (11,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('cleaneddataxxx.csv')\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T10:52:45.771390Z",
     "start_time": "2025-06-08T10:52:42.834850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 1: Cancellation Prediction Analysis ---\")\n",
    "df['is_canceled'] = (df['CanceledQty'] > 0).astype(int)\n",
    "cancel_features = [\n",
    "    'MenuPrice', 'MenuSubsidy', 'subsidy_pct', 'day_of_year', 'weekday',\n",
    "    'is_weekend', 'sin_doy', 'cos_doy', 'month', 'quarter', 'is_month_end',\n",
    "    'is_month_start', 'order_hour', 'morning', 'afternoon', 'evening',\n",
    "    'hist_cancel_rate', 'is_holiday'\n",
    "]\n",
    "X_cancel = df[cancel_features].copy().fillna(0)\n",
    "y_cancel = df['is_canceled']\n",
    "\n",
    "split_index = int(len(df) * 0.8)\n",
    "X_train_c, X_test_c = X_cancel[:split_index], X_cancel[split_index:]\n",
    "y_train_c, y_test_c = y_cancel[:split_index], y_cancel[split_index:]\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "rf_classifier.fit(X_train_c, y_train_c)\n",
    "y_pred_c = rf_classifier.predict(X_test_c)\n",
    "\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': cancel_features,\n",
    "    'importance': rf_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importances.head(10), palette='viridis')\n",
    "plt.title('Top 10 Features Driving Cancellations', fontsize=16)\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test_c, y_pred_c)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Canceled', 'Canceled'], yticklabels=['Not Canceled', 'Canceled'])\n",
    "plt.title('Cancellation Model Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_c, y_pred_c))\n",
    "print(\"\\n--- End of Step 1 ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 2: Data Preparation & Exploration ---\")\n",
    "# Aggregate data to Site-Day level and pivot\n",
    "daily_demand = df.groupby(['DateOfService', 'Site'], observed=False)['net_qty'].sum().reset_index()\n",
    "pivot_demand = daily_demand.pivot(index='DateOfService', columns='Site', values='net_qty').fillna(0)\n",
    "full_date_range = pd.date_range(start=pivot_demand.index.min(), end=pivot_demand.index.max(), freq='D')\n",
    "pivot_demand = pivot_demand.reindex(full_date_range, fill_value=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=pivot_demand, palette='Set2')\n",
    "plt.title('Distribution of Daily Meal Demand per Kitchen Location', fontsize=16)\n",
    "plt.ylabel('Net Meal Quantity')\n",
    "plt.xlabel('Kitchen Site')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "holiday_impact_data = pivot_demand.copy()\n",
    "holiday_impact_data['is_holiday'] = holiday_impact_data.index.to_series().apply(lambda x: x in de_holidays)\n",
    "melted_data = holiday_impact_data.melt(id_vars=['is_holiday'], var_name='Site', value_name='Demand')\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Site', y='Demand', hue='is_holiday', data=melted_data, palette={True: 'salmon', False: 'skyblue'})\n",
    "plt.title('Average Demand: Holiday vs. Non-Holiday', fontsize=16)\n",
    "plt.ylabel('Average Net Meal Quantity')\n",
    "plt.xlabel('Kitchen Site')\n",
    "plt.legend(title='Is Holiday?')\n",
    "plt.show()\n",
    "\n",
    "# Feature Engineering\n",
    "time_features = pd.DataFrame(index=pivot_demand.index)\n",
    "time_features['day_of_year'] = time_features.index.dayofyear\n",
    "time_features['weekday'] = time_features.index.weekday\n",
    "time_features['is_weekend'] = (time_features.index.weekday >= 5).astype(int)\n",
    "time_features['month'] = time_features.index.month\n",
    "time_features['quarter'] = time_features.index.quarter\n",
    "time_features['sin_doy'] = np.sin(2 * np.pi * time_features['day_of_year'] / 365.25)\n",
    "time_features['cos_doy'] = np.cos(2 * np.pi * time_features['day_of_year'] / 365.25)\n",
    "time_features['is_holiday'] = time_features.index.to_series().apply(lambda x: int(x in de_holidays))\n",
    "\n",
    "# Create lag and rolling features\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    time_features[f'lag_{lag}_all_sites'] = pivot_demand.shift(lag).mean(axis=1)\n",
    "time_features['rolling_mean_7_all_sites'] = time_features['lag_1_all_sites'].rolling(window=7).mean()\n",
    "time_features = time_features.fillna(0)\n",
    "\n",
    "# Create Target Variables for each horizon\n",
    "targets_1_day = pivot_demand.shift(-1)\n",
    "targets_2_5_days = pivot_demand.rolling(window=4, min_periods=1).mean().shift(-5)\n",
    "targets_6_10_days = pivot_demand.rolling(window=5, min_periods=1).mean().shift(-10)\n",
    "target_dfs = {\n",
    "    '1_Day_Ahead': targets_1_day,\n",
    "    '2-5_Days_Ahead': targets_2_5_days,\n",
    "    '6-10_Days_Ahead': targets_6_10_days,\n",
    "}\n",
    "print(\"\\n--- End of Step 2 ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"\\n--- Steps 3 & 4: Per-Kitchen Modeling & Evaluation ---\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "sites = pivot_demand.columns\n",
    "models = {}\n",
    "evaluation_results = []\n",
    "all_test_sets = {}\n",
    "\n",
    "\n",
    "def asymmetric_loss(y_true, y_pred):\n",
    "    errors = y_true - y_pred\n",
    "    # Penalize under-predictions (errors > 0) more\n",
    "    loss = np.where(errors > 0, errors**2 * 1.2, errors**2)\n",
    "    return np.sqrt(np.mean(loss))\n",
    "\n",
    "for site in sites:\n",
    "    print(f\"\\n--- Processing Kitchen: {site} ---\")\n",
    "    models[site] = {}\n",
    "    all_test_sets[site] = {}\n",
    "\n",
    "    site_features = time_features.copy()\n",
    "    for lag in [1, 2, 3, 7, 14, 21]:\n",
    "        site_features[f'site_lag_{lag}'] = pivot_demand[site].shift(lag)\n",
    "    site_features['site_rolling_mean_7'] = pivot_demand[site].shift(1).rolling(window=7).mean()\n",
    "    site_features = site_features.fillna(0)\n",
    "\n",
    "    for horizon_name, target_df in target_dfs.items():\n",
    "        print(f\"  Training for horizon: {horizon_name}\")\n",
    "        y_series = target_df[site]\n",
    "        temp_df = pd.concat([site_features, y_series.rename('target')], axis=1).dropna()\n",
    "        X = temp_df.drop('target', axis=1)\n",
    "        y = temp_df['target']\n",
    "\n",
    "        if len(X) < 10:\n",
    "            print(f\"    - Skipping {horizon_name} due to insufficient data.\")\n",
    "            continue\n",
    "\n",
    "        final_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, min_samples_leaf=3)\n",
    "        final_model.fit(X, y)\n",
    "        models[site][horizon_name] = {\n",
    "            'model': final_model,\n",
    "            'features': X.columns.tolist()\n",
    "        }\n",
    "\n",
    "        fold_scores = []\n",
    "        for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            y_pred = final_model.predict(X_test)\n",
    "            fold_scores.append(asymmetric_loss(y_test, y_pred))\n",
    "            if i == tscv.n_splits - 1:\n",
    "                all_test_sets[site][horizon_name] = {'y_test': y_test, 'y_pred': y_pred}\n",
    "\n",
    "        avg_loss = np.mean(fold_scores)\n",
    "        evaluation_results.append({'Site': site, 'Horizon': horizon_name, 'Avg_Asymmetric_Loss': avg_loss})\n",
    "        print(f\"    - Avg. Asymmetric Loss: {avg_loss:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Visualizing Feature Importances for Demand Forecast Models ---\")\n",
    "for site in models:\n",
    "    horizon_to_plot = '1_Day_Ahead'\n",
    "    if horizon_to_plot in models[site]:\n",
    "        model_info = models[site][horizon_to_plot]\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': model_info['features'],\n",
    "            'importance': model_info['model'].feature_importances_\n",
    "        }).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=importances, palette='cividis')\n",
    "        plt.title(f'Top 10 Features for Demand Forecast at {site} ({horizon_to_plot})', fontsize=16)\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n--- End of Steps 3 & 4 ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 5: Detailed Model Evaluation ---\")\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "\n",
    "if not results_df.empty:\n",
    "    performance_pivot = results_df.pivot(index='Site', columns='Horizon', values='Avg_Asymmetric_Loss')\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(performance_pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu_r\", linewidths=.5)\n",
    "    plt.title('Forecasting Performance (Asymmetric Loss) by Kitchen and Horizon', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "safety_buffer_factor = 1.05 # Add 5% to predictions as a safety net\n",
    "print(f\"\\nApplying a {int((safety_buffer_factor-1)*100)}% safety buffer to all predictions.\")\n",
    "\n",
    "for site in all_test_sets:\n",
    "    for horizon, data in all_test_sets[site].items():\n",
    "        y_test = data['y_test']\n",
    "        y_pred_raw = data['y_pred']\n",
    "        y_pred_buffered = y_pred_raw * safety_buffer_factor\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.plot(y_test.index, y_test, label='Actual Meals', color='blue', alpha=0.8)\n",
    "        plt.plot(y_test.index, y_pred_buffered, label=f'Forecast with Safety Buffer', color='red', linestyle='--')\n",
    "        plt.fill_between(y_test.index, y_pred_raw, y_pred_buffered, color='red', alpha=0.2, label='Safety Buffer Zone')\n",
    "        plt.title(f'Evaluation: Forecast vs. Actuals for {site} ({horizon})', fontsize=16)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Net Meal Quantity')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "for site in all_test_sets:\n",
    "    if '1_Day_Ahead' in all_test_sets[site]:\n",
    "        data = all_test_sets[site]['1_Day_Ahead']\n",
    "        y_test = data['y_test']\n",
    "        y_pred_buffered = data['y_pred'] * safety_buffer_factor\n",
    "        errors = y_test - y_pred_buffered\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.histplot(errors, kde=True, bins=30)\n",
    "        plt.title(f'Distribution of Prediction Errors for {site} (After Buffer)', fontsize=16)\n",
    "        plt.xlabel(f'Error (Actual - Forecast). Mean: {errors.mean():.2f}')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.axvline(0, color='red', linestyle='--')\n",
    "        plt.grid(True, linestyle='--', alpha=0.5)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n--- End of Step 5 ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"\\n--- Step 6: Generating 14-Day Future Forecasts ---\")\n",
    "last_date = pivot_demand.index.max()\n",
    "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=14, freq='D')\n",
    "future_forecasts_df = pd.DataFrame(index=future_dates)\n",
    "\n",
    "for site in models:\n",
    "    print(f\"  Generating forecast for {site}...\")\n",
    "    horizon = '1_Day_Ahead'\n",
    "    if horizon in models[site]:\n",
    "        model_info = models[site][horizon]\n",
    "        model = model_info['model']\n",
    "        features_list = model_info['features']\n",
    "\n",
    "        full_history_extended = pd.concat([pivot_demand, pd.DataFrame(index=future_dates)])\n",
    "        future_features = pd.DataFrame(index=future_dates)\n",
    "        future_features['day_of_year'] = future_features.index.dayofyear\n",
    "        future_features['weekday'] = future_features.index.weekday\n",
    "        future_features['is_weekend'] = (future_features.index.weekday >= 5).astype(int)\n",
    "        future_features['month'] = future_features.index.month\n",
    "        future_features['quarter'] = future_features.index.quarter\n",
    "        future_features['sin_doy'] = np.sin(2 * np.pi * future_features['day_of_year'] / 365.25)\n",
    "        future_features['cos_doy'] = np.cos(2 * np.pi * future_features['day_of_year'] / 365.25)\n",
    "        future_features['is_holiday'] = future_features.index.to_series().apply(lambda x: int(x in de_holidays))\n",
    "\n",
    "\n",
    "        for lag in [1, 7, 14, 21, 28]:\n",
    "            future_features[f'lag_{lag}_all_sites'] = full_history_extended.shift(lag).mean(axis=1).loc[future_features.index]\n",
    "        future_features['rolling_mean_7_all_sites'] = future_features['lag_1_all_sites'].rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "        site_history_extended = pd.concat([pivot_demand[site], pd.Series(index=future_dates, dtype='float64')])\n",
    "        for lag in [1, 2, 3, 7, 14, 21]:\n",
    "            future_features[f'site_lag_{lag}'] = site_history_extended.shift(lag).loc[future_features.index]\n",
    "        future_features['site_rolling_mean_7'] = site_history_extended.shift(1).rolling(window=7, min_periods=1).mean().loc[future_features.index]\n",
    "\n",
    "        future_features = future_features.ffill().bfill().fillna(0)[features_list]\n",
    "\n",
    "        raw_predictions = model.predict(future_features)\n",
    "        buffered_predictions = raw_predictions * safety_buffer_factor\n",
    "        final_predictions = np.ceil(buffered_predictions).astype(int)\n",
    "        future_forecasts_df[site] = final_predictions\n",
    "\n",
    "\n",
    "print(\"\\n--- Final 14-Day Meal Preparation Forecast (Meals to Prepare) ---\")\n",
    "future_forecasts_df.index.name = 'Date'\n",
    "print(future_forecasts_df.to_string())\n",
    "\n",
    "for site in future_forecasts_df.columns:\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    pivot_demand[site].tail(30).plot(label='Recent Historical Demand', color='black', alpha=0.7)\n",
    "\n",
    "    future_forecasts_df[site].plot(label=f'14-Day Forecast (with Buffer)', linestyle='--', marker='o')\n",
    "    plt.title(f'Demand Forecast for {site}', fontsize=16)\n",
    "    plt.ylabel('Predicted Meals')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- End of Process ---\")"
   ],
   "id": "ef0fa89a82c6663b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Cancellation Prediction Analysis ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 19\u001B[39m\n\u001B[32m     16\u001B[39m y_train_c, y_test_c = y_cancel[:split_index], y_cancel[split_index:]\n\u001B[32m     18\u001B[39m rf_classifier = RandomForestClassifier(n_estimators=\u001B[32m100\u001B[39m, random_state=\u001B[32m42\u001B[39m, class_weight=\u001B[33m'\u001B[39m\u001B[33mbalanced\u001B[39m\u001B[33m'\u001B[39m, n_jobs=-\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m \u001B[43mrf_classifier\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_c\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train_c\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m y_pred_c = rf_classifier.predict(X_test_c)\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# Visualization 1.1: Feature Importance for Cancellations\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/ensemble/_forest.py:375\u001B[39m, in \u001B[36mBaseForest.fit\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m    369\u001B[39m \u001B[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001B[39;00m\n\u001B[32m    370\u001B[39m \u001B[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001B[39;00m\n\u001B[32m    371\u001B[39m \u001B[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001B[39;00m\n\u001B[32m    372\u001B[39m \u001B[38;5;66;03m# missing values.\u001B[39;00m\n\u001B[32m    373\u001B[39m estimator = \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m.estimator)(criterion=\u001B[38;5;28mself\u001B[39m.criterion)\n\u001B[32m    374\u001B[39m missing_values_in_feature_mask = (\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m     \u001B[43mestimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_compute_missing_values_in_feature_mask\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    376\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__class__\u001B[39;49m\u001B[43m.\u001B[49m\u001B[34;43m__name__\u001B[39;49m\n\u001B[32m    377\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    378\u001B[39m )\n\u001B[32m    380\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    381\u001B[39m     sample_weight = _check_sample_weight(sample_weight, X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/tree/_classes.py:222\u001B[39m, in \u001B[36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001B[39m\u001B[34m(self, X, estimator_name)\u001B[39m\n\u001B[32m    218\u001B[39m     overall_sum = np.sum(X)\n\u001B[32m    220\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np.isfinite(overall_sum):\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# Raise a ValueError in case of the presence of an infinite element.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m     \u001B[43m_assert_all_finite_element_wise\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcommon_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    224\u001B[39m \u001B[38;5;66;03m# If the sum is not nan, then there are no missing values\u001B[39;00m\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np.isnan(overall_sum):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001B[39m, in \u001B[36m_assert_all_finite_element_wise\u001B[39m\u001B[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name == \u001B[33m\"\u001B[39m\u001B[33mX\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[32m    153\u001B[39m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[32m    154\u001B[39m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[32m    155\u001B[39m     msg_err += (\n\u001B[32m    156\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not accept missing values\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m#estimators-that-handle-nan-values\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    168\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m169\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[31mValueError\u001B[39m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
