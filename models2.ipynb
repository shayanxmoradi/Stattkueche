{
 "cells": [
  {
   "cell_type": "code",
   "id": "d183b14c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:02:54.600906Z",
     "start_time": "2025-07-18T23:02:00.486135Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scipy.signal import savgol_filter\n",
    "\n",
    "df_weather2 = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather3.csv',parse_dates=['DateOfCancel','DateOfService'])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "64b9ba66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:02:55.107537Z",
     "start_time": "2025-07-18T23:02:54.621202Z"
    }
   },
   "source": [
    "###### vif after incorporating the weather varibales for df_encoded\n",
    "#### do the vif check\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "num_cols = df_weather2.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "drop_col = ['DateOfOrder', 'DateOfService','days_to_cancel','CanceledQty','OrderId', 'TransactionId','cancel_timing','BookingNr','DateOfCancel','net_qty']\n",
    "\n",
    "vif_col_1 = [c for c in num_cols if c not in drop_col]\n",
    "\n",
    "vif_dat_1 = df_encoded[vif_col_1].copy()\n",
    "vif_dat_1.replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "vif_dat_1 = vif_dat_1.dropna()\n",
    "vif_scores_1 = [variance_inflation_factor(vif_dat_1,i)\n",
    "              for i in range(vif_dat_1.shape[1])\n",
    "              ]\n",
    "\n",
    "vif_table_1 = (pd.DataFrame({'feature':vif_col_1,'VIF':vif_scores_1}).sort_values('VIF',ascending=False).reset_index(drop=True))\n",
    "print('VIF values')\n",
    "display(vif_table_1)\n",
    "\n",
    "hg_vif_1 = vif_table_1.loc[vif_table_1['VIF']>10,'feature'].to_list()\n",
    "print('the high vif columns dropped')\n",
    "df_without_vif_1 = vif_dat_1.drop(columns=hg_vif_1)\n",
    "print(df_without_vif_1.columns)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      6\u001B[39m drop_col = [\u001B[33m'\u001B[39m\u001B[33mDateOfOrder\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mDateOfService\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mdays_to_cancel\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mCanceledQty\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mOrderId\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mTransactionId\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mcancel_timing\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mBookingNr\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mDateOfCancel\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mnet_qty\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      8\u001B[39m vif_col_1 = [c \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m num_cols \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m drop_col]\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m vif_dat_1 = \u001B[43mdf_encoded\u001B[49m[vif_col_1].copy()\n\u001B[32m     11\u001B[39m vif_dat_1.replace([np.inf,-np.inf],np.nan,inplace=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m     12\u001B[39m vif_dat_1 = vif_dat_1.dropna()\n",
      "\u001B[31mNameError\u001B[39m: name 'df_encoded' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d54aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stage A for random forest with df_weather2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ── 1) Prepare X / y for Stage A ────────────────────────────────\n",
    "df_A    = df_weather2.copy()\n",
    "# binary target: cancelled if CanceledQty > 0\n",
    "y_A     = (df_A['CanceledQty'] > 0).astype(int)\n",
    "X_A     = df_A.drop(columns=[\n",
    "    'CanceledQty',      # leak\n",
    "    'cancel_timing',    # multi‐class leftover\n",
    "    # any other columns you know shouldn’t go in:\n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "\n",
    "# ── 2) Re‐use your transformers ───────────────────────────────────\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X      = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# ── 3) Drop any remaining non-numeric / ID columns ───────────────\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "drop_cols_A = [\n",
    "    # any string or ID cols still in X_A\n",
    "    'Site','MenuBase','MenuName','GroupName','MenuNorm','MenuCode','net_qty','days_to_cancel'\n",
    "]\n",
    "\n",
    "# ── 4) Assemble Stage A pipeline ─────────────────────────────────\n",
    "pipeline_A = ImbPipeline([\n",
    "    #('te',      TargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "\n",
    "\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                   n_clusters=5)),\n",
    "    ('drop',    ColumnDropper(drop_cols_A)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    #('smote',   SMOTE(random_state=24)),\n",
    "    ('clf',     RandomForestClassifier(\n",
    "                   n_estimators=500,\n",
    "                   criterion='entropy',     # your experiment\n",
    "                   max_depth=8,\n",
    "                   min_samples_split=5,\n",
    "                   min_samples_leaf=1,\n",
    "                   max_features='sqrt',\n",
    "                   class_weight='balanced_subsample',\n",
    "                   random_state=24,\n",
    "                   n_jobs=-1\n",
    "               )),\n",
    "])\n",
    "\n",
    "# ── 5) Evaluate with rolling AUC ──────────────────────────────────\n",
    "tscv    = TimeSeriesSplit(n_splits=5)\n",
    "scores  = cross_val_score(\n",
    "    pipeline_A, X_A, y_A,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Stage A ROC-AUC:\", np.round(scores.mean(),4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bdc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Karriert ,Preis_EUR/ltr ==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -79.9494      1.326    -60.274      0.000     -82.558     -77.340\n",
      "x1             0.0001   1.82e-06     60.904      0.000       0.000       0.000\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "### stage A with df_weather2 and lgbm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from imblearn.pipeline      import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# ── 1) Prepare X / y for Stage A ────────────────────────────────\n",
    "df_A    = df_weather2.copy()\n",
    "# binary target: cancelled if CanceledQty > 0\n",
    "y_A     = (df_A['CanceledQty'] > 0).astype(int)\n",
    "X_A     = df_A.drop(columns=[\n",
    "    'CanceledQty',      # leak\n",
    "    'cancel_timing',    # multi‐class leftover\n",
    "    # any other columns you know shouldn’t go in:\n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "\n",
    "# ── 2) Re‐use your transformers ───────────────────────────────────\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X      = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# ── 3) Drop any remaining non-numeric / ID columns ───────────────\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "drop_cols_A = [\n",
    "    # any string or ID cols still in X_A\n",
    "    'Site','MenuBase','MenuName','GroupName','MenuNorm','MenuCode','net_qty','days_to_cancel'\n",
    "]\n",
    "\n",
    "# ── 4) Assemble Stage A pipeline ─────────────────────────────────\n",
    "pipeline_A_l = ImbPipeline([\n",
    "    #('te',      TargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "\n",
    "\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                   n_clusters=5)),\n",
    "    ('drop',    ColumnDropper(drop_cols_A)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    #('smote',   SMOTE(random_state=24)),\n",
    "    ('clf',       LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=len(np.unique(y_A)),\n",
    "                                 random_state=24,\n",
    "                                 metric    = \"multi_logloss\",\n",
    "                                 n_jobs=-1))\n",
    ",\n",
    "])\n",
    "param_dist_lgbm = {\n",
    "    'clf__n_estimators':      [200,500,800],\n",
    "    'clf__learning_rate':     [0.01,0.03,0.05],\n",
    "    'clf__num_leaves':        [31,15,10],\n",
    "    'clf__max_depth':         [10,6,20],\n",
    "    'clf__subsample':         [0.7,0.3,1],\n",
    "    'clf__colsample_bytree':  [1.0,2.0,0.7],\n",
    "    'clf__min_child_samples': [20,10,7]\n",
    "}\n",
    "\n",
    "# 6) Rolling CV search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_A_lgbm = RandomizedSearchCV(\n",
    "    pipeline_A_l,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_A_lgbm.fit(X_A, y_A)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_A_lgbm.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_A_lgbm.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage B2 with hist_cancel_rate and cluster calculation inside the cv loop and target encoding\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "\n",
    "\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "# drop any leak columns before X\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty',      \n",
    "    'cancel_timing','timing_code',     \n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "# 0) Build the Stage B2 subset once:\n",
    "#df_B2 = df_weather[\n",
    "#    (df_weather.CanceledQty > 0) &\n",
    "#    (df_weather.cancel_timing.isin([0,1,2]))\n",
    "#].copy()\n",
    "#X_full = df_B2.drop(columns='cancel_timing')\n",
    "#y_full = df_B2['cancel_timing']\n",
    "\n",
    "# Update to HistCancelRateTransformer and ClusterTransformer to avoid tuple KeyError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols      = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean','count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] + \n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_[c])\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        # Store parameters verbatim—do NOT convert to list here\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to list only when you need it\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        # build tuple keys and map\n",
    "        tuples = list(zip(*(X[k] for k in keys)))\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        profile_feats,                 # list of column names to form profiles\n",
    "        group_keys=('Site','MenuBase'),\n",
    "        n_clusters=5,\n",
    "        out_col='cluster_id'\n",
    "    ):\n",
    "        # Store parameters as‐is; don’t mutate them here\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 1) Build training set profiles by group\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (\n",
    "            X.groupby(keys)[self.profile_feats]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "        )\n",
    "        # 2) Median‐impute any missing values in the profiles\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(\n",
    "            prof[self.profile_feats].median()\n",
    "        )\n",
    "\n",
    "        # 3) Standardize and run K-Means on those profiles\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "\n",
    "        # 4) Build a lookup map from (Site,MenuBase) → cluster label\n",
    "        tuples = [tuple(row) for row in prof[keys].values]\n",
    "        labels = list(self.kmeans_.labels_)\n",
    "        self.cluster_map_ = dict(zip(tuples, labels))\n",
    "        # Use the median cluster as a fallback for unseen combos\n",
    "        self.default_ = int(np.median(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(row) for row in X[keys].values]\n",
    "        # Map each row’s key to its cluster (or default)\n",
    "        X[self.out_col] = [\n",
    "            self.cluster_map_.get(t, self.default_) for t in tuples\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify numeric columns\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        # Replace inf with NaN for fitting\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fit the imputer on cleaned data\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Replace inf with NaN before transforming\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Add missing flags\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        # Impute\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# Example integration into your pipeline:\n",
    "# pipeline_B2 = Pipeline([\n",
    "#     ('hist', HistCancelRateTransformer()),\n",
    "#     ('cluster', ClusterTransformer(...)),\n",
    "#     ('impute', MissingFlagImputer()),\n",
    "#     ('clf', RandomForestClassifier(...))\n",
    "# ])\n",
    "\n",
    "# 4) Assemble pipelin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop any columns we know are non-numeric, IDs, or leftovers\n",
    "        return X.drop(columns=self.cols_to_drop,errors='ignore')\n",
    "drop_cols_B2 = [\n",
    "    'MenuName','GroupName','MenuNorm','MenuCode'\n",
    "\n",
    "]\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 4) Assemble pipelin\n",
    "\n",
    "#pipeline_B2 = Pipeline([\n",
    "#    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "#    ('hist',    HistCancelRateTransformer()),         # train-only hist rates\n",
    "#    ('cluster', ClusterTransformer(\n",
    "#                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev'],\n",
    "#                   n_clusters=5)),\n",
    "#    ('impute',  MissingFlagImputer()),               # replace inf → NaN, flag + median\n",
    "#    ('clf',     RandomForestClassifier(\n",
    "#                   n_estimators=100,\n",
    "#                   class_weight='balanced',\n",
    "#                   random_state=24,\n",
    "#                   n_jobs=-1\n",
    "#               )),\n",
    "#])\n",
    "\n",
    "#print(\"Columns going into the pipeline:\", X_full.columns.tolist())\n",
    "pipeline_B2_smote = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),                # drop strings, IDs, leakage cols\n",
    "    ('hist',    HistCancelRateTransformer()),                # train‐only hist rates\n",
    "    ('cluster', ClusterTransformer(                          # train‐only clusters\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),               \n",
    "    ('impute',  MissingFlagImputer()),                       # inf→nan, flag & median‐impute\n",
    "    ('smote',   SMOTE(random_state=24)),                     # synthesize new minority samples\n",
    "    ('clf',     RandomForestClassifier(                      # your tuned RF head\n",
    "                   n_estimators=500,\n",
    "                   min_samples_split=5,\n",
    "                   min_samples_leaf=2,\n",
    "                   max_features='sqrt',\n",
    "                   max_depth=None,\n",
    "                   class_weight='balanced',\n",
    "                   random_state=24,\n",
    "                   n_jobs=-1\n",
    "               )),\n",
    "])\n",
    "# 5) Rolling CV evaluation\n",
    "tscv    = TimeSeriesSplit(n_splits=5)\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# 1) Define the time‐series split\n",
    "#tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# 2) Specify the RF hyper‐parameter distributions\n",
    "param_dist = {\n",
    "    'clf__n_estimators':      [600,1200],\n",
    "    'clf__max_depth':         [10,6],\n",
    "    'clf__min_samples_split': [10,20],\n",
    "    'clf__min_samples_leaf':  [15],\n",
    "    'clf__bootstrap':         [True],\n",
    "    'clf__criterion':         ['entropy'],\n",
    "    'clf__max_features':      [0.8,1.2]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline_B2_smote,               # the full hist→cluster→impute→RF pipe\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                 # how many random combos to try\n",
    "    cv=tscv,                   # your time‐series folds\n",
    "    scoring='roc_auc_ovo_weighted',  # optimize roc_auc_ovo_weighted\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "# 4) Fit on your Stage B2 data\n",
    "search.fit(X_B2, y_B2)\n",
    "\n",
    "# 5) Check the results\n",
    "print(\"Best roc_auc_ovo_weighted:\", np.round(search.best_score_, 4))\n",
    "print(\"Best hyper-parameters:\")\n",
    "for k, v in search.best_params_.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "#scores  = cross_val_score(\n",
    "#    pipeline_B2,\n",
    "#    X_full,\n",
    "#    y_full,\n",
    "#    cv=tscv,\n",
    "#    scoring='average_precision',\n",
    "#    n_jobs=1\n",
    "#)\n",
    "#print(\"Stage B2 PR-AUC:\", np.round(scores.mean(),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e02002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage B2 with hist_cancel_rate and cluster calculation inside the cv loop and target encoding LGBM\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "# drop any leak columns before X\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty',      \n",
    "    'cancel_timing','timing_code',     \n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "# 0) Build the Stage B2 subset once:\n",
    "#df_B2 = df_weather[\n",
    "#    (df_weather.CanceledQty > 0) &\n",
    "#    (df_weather.cancel_timing.isin([0,1,2]))\n",
    "#].copy()\n",
    "#X_full = df_B2.drop(columns='cancel_timing')\n",
    "#y_full = df_B2['cancel_timing']\n",
    "\n",
    "# Update to HistCancelRateTransformer and ClusterTransformer to avoid tuple KeyError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols      = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean','count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] + \n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_[c])\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        # Store parameters verbatim—do NOT convert to list here\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to list only when you need it\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        # build tuple keys and map\n",
    "        tuples = list(zip(*(X[k] for k in keys)))\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        profile_feats,                 # list of column names to form profiles\n",
    "        group_keys=('Site','MenuBase'),\n",
    "        n_clusters=5,\n",
    "        out_col='cluster_id'\n",
    "    ):\n",
    "        # Store parameters as‐is; don’t mutate them here\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 1) Build training set profiles by group\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (\n",
    "            X.groupby(keys)[self.profile_feats]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "        )\n",
    "        # 2) Median‐impute any missing values in the profiles\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(\n",
    "            prof[self.profile_feats].median()\n",
    "        )\n",
    "\n",
    "        # 3) Standardize and run K-Means on those profiles\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "\n",
    "        # 4) Build a lookup map from (Site,MenuBase) → cluster label\n",
    "        tuples = [tuple(row) for row in prof[keys].values]\n",
    "        labels = list(self.kmeans_.labels_)\n",
    "        self.cluster_map_ = dict(zip(tuples, labels))\n",
    "        # Use the median cluster as a fallback for unseen combos\n",
    "        self.default_ = int(np.median(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(row) for row in X[keys].values]\n",
    "        # Map each row’s key to its cluster (or default)\n",
    "        X[self.out_col] = [\n",
    "            self.cluster_map_.get(t, self.default_) for t in tuples\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify numeric columns\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        # Replace inf with NaN for fitting\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fit the imputer on cleaned data\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Replace inf with NaN before transforming\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Add missing flags\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        # Impute\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# Example integration into your pipeline:\n",
    "# pipeline_B2 = Pipeline([\n",
    "#     ('hist', HistCancelRateTransformer()),\n",
    "#     ('cluster', ClusterTransformer(...)),\n",
    "#     ('impute', MissingFlagImputer()),\n",
    "#     ('clf', RandomForestClassifier(...))\n",
    "# ])\n",
    "\n",
    "# 4) Assemble pipelin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop any columns we know are non-numeric, IDs, or leftovers\n",
    "        return X.drop(columns=self.cols_to_drop,errors='ignore')\n",
    "drop_cols_B2 = [\n",
    "    'MenuName','GroupName','MenuNorm','MenuCode'\n",
    "\n",
    "]\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 4) Assemble pipelin\n",
    "\n",
    "#pipeline_B2 = Pipeline([\n",
    "#    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "#    ('hist',    HistCancelRateTransformer()),         # train-only hist rates\n",
    "#    ('cluster', ClusterTransformer(\n",
    "#                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev'],\n",
    "#                   n_clusters=5)),\n",
    "#    ('impute',  MissingFlagImputer()),               # replace inf → NaN, flag + median\n",
    "#    ('clf',     RandomForestClassifier(\n",
    "#                   n_estimators=100,\n",
    "#                   class_weight='balanced',\n",
    "#                   random_state=24,\n",
    "#                   n_jobs=-1\n",
    "#               )),\n",
    "#])\n",
    "\n",
    "#print(\"Columns going into the pipeline:\", X_full.columns.tolist())\n",
    "pipeline_B2_lgbm = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),                # drop strings, IDs, leakage cols\n",
    "    ('hist',    HistCancelRateTransformer()),                # train‐only hist rates\n",
    "    ('cluster', ClusterTransformer(                          # train‐only clusters\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),               \n",
    "    ('impute',  MissingFlagImputer()),                       # inf→nan, flag & median‐impute\n",
    "    #('smote',   SMOTE(random_state=24)),                     # synthesize new minority samples\n",
    "   ('clf',       LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=len(np.unique(y_B2)),\n",
    "                                 random_state=24,\n",
    "                                 metric    = \"multi_logloss\",\n",
    "                                 n_jobs=-1))\n",
    ",\n",
    "])\n",
    "param_dist_lgbm = {\n",
    "    'clf__n_estimators':      [200,600],\n",
    "    'clf__learning_rate':     [0.01,0.07],\n",
    "    'clf__num_leaves':        [31,40],\n",
    "    'clf__max_depth':         [10,20],\n",
    "    'clf__subsample':         [0.7,1],\n",
    "    'clf__colsample_bytree':  [1.0,2],\n",
    "    'clf__min_child_samples': [20,30]\n",
    "}\n",
    "\n",
    "# 6) Rolling CV search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_lgbm = RandomizedSearchCV(\n",
    "    pipeline_B2_lgbm,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_lgbm.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_lgbm.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_lgbm.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3b82aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### stage B2 with hist_cancel_rate and cluster calculation inside the cv loop and target encoding catboost\n",
    "from sklearn.base           import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline       import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster       import KMeans\n",
    "from sklearn.impute        import SimpleImputer\n",
    "from sklearn.ensemble      import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import make_scorer, f1_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "# drop any leak columns before X\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty',      \n",
    "    'cancel_timing','timing_code',     \n",
    "    'DateOfOrder','DateOfService','DateOfCancel',\n",
    "    'OrderId','TransactionId','BookingNr','hist_cancel_rate','GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "# 0) Build the Stage B2 subset once:\n",
    "#df_B2 = df_weather[\n",
    "#    (df_weather.CanceledQty > 0) &\n",
    "#    (df_weather.cancel_timing.isin([0,1,2]))\n",
    "#].copy()\n",
    "#X_full = df_B2.drop(columns='cancel_timing')\n",
    "#y_full = df_B2['cancel_timing']\n",
    "\n",
    "# Update to HistCancelRateTransformer and ClusterTransformer to avoid tuple KeyError\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols      = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean','count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] + \n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_[c])\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        # Store parameters verbatim—do NOT convert to list here\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Convert to list only when you need it\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        # build tuple keys and map\n",
    "        tuples = list(zip(*(X[k] for k in keys)))\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        profile_feats,                 # list of column names to form profiles\n",
    "        group_keys=('Site','MenuBase'),\n",
    "        n_clusters=5,\n",
    "        out_col='cluster_id'\n",
    "    ):\n",
    "        # Store parameters as‐is; don’t mutate them here\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # 1) Build training set profiles by group\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (\n",
    "            X.groupby(keys)[self.profile_feats]\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "        )\n",
    "        # 2) Median‐impute any missing values in the profiles\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(\n",
    "            prof[self.profile_feats].median()\n",
    "        )\n",
    "\n",
    "        # 3) Standardize and run K-Means on those profiles\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42).fit(scaled)\n",
    "\n",
    "        # 4) Build a lookup map from (Site,MenuBase) → cluster label\n",
    "        tuples = [tuple(row) for row in prof[keys].values]\n",
    "        labels = list(self.kmeans_.labels_)\n",
    "        self.cluster_map_ = dict(zip(tuples, labels))\n",
    "        # Use the median cluster as a fallback for unseen combos\n",
    "        self.default_ = int(np.median(labels))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(row) for row in X[keys].values]\n",
    "        # Map each row’s key to its cluster (or default)\n",
    "        X[self.out_col] = [\n",
    "            self.cluster_map_.get(t, self.default_) for t in tuples\n",
    "        ]\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Identify numeric columns\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        # Replace inf with NaN for fitting\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Fit the imputer on cleaned data\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy)\n",
    "        self.imputer_.fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        # Replace inf with NaN before transforming\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        # Add missing flags\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        # Impute\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "# Example integration into your pipeline:\n",
    "# pipeline_B2 = Pipeline([\n",
    "#     ('hist', HistCancelRateTransformer()),\n",
    "#     ('cluster', ClusterTransformer(...)),\n",
    "#     ('impute', MissingFlagImputer()),\n",
    "#     ('clf', RandomForestClassifier(...))\n",
    "# ])\n",
    "\n",
    "# 4) Assemble pipelin\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Drop any columns we know are non-numeric, IDs, or leftovers\n",
    "        return X.drop(columns=self.cols_to_drop,errors='ignore')\n",
    "drop_cols_B2 = [\n",
    "    'MenuName','GroupName','MenuNorm','MenuCode'\n",
    "\n",
    "]\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 4) Assemble pipelin\n",
    "\n",
    "#pipeline_B2 = Pipeline([\n",
    "#    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "#    ('hist',    HistCancelRateTransformer()),         # train-only hist rates\n",
    "#    ('cluster', ClusterTransformer(\n",
    "#                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev'],\n",
    "#                   n_clusters=5)),\n",
    "#    ('impute',  MissingFlagImputer()),               # replace inf → NaN, flag + median\n",
    "#    ('clf',     RandomForestClassifier(\n",
    "#                   n_estimators=100,\n",
    "#                   class_weight='balanced',\n",
    "#                   random_state=24,\n",
    "#                   n_jobs=-1\n",
    "#               )),\n",
    "#])\n",
    "\n",
    "#print(\"Columns going into the pipeline:\", X_full.columns.tolist())\n",
    "pipeline_B2_cat = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),                # drop strings, IDs, leakage cols\n",
    "    ('hist',    HistCancelRateTransformer()),                # train‐only hist rates\n",
    "    ('cluster', ClusterTransformer(                          # train‐only clusters\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),               \n",
    "    ('impute',  MissingFlagImputer()),                       # inf→nan, flag & median‐impute\n",
    "    #('smote',   SMOTE(random_state=24)),                     # synthesize new minority samples\n",
    "    ('clf',     CatBoostClassifier(\n",
    "                    # safe defaults:\n",
    "                    iterations=500,\n",
    "                    #cat_features = cat_col,\n",
    "                    auto_class_weights='Balanced',\n",
    "                    loss_function='MultiClass',\n",
    "                    #use_focal_loss=True,\n",
    "                    #focal_loss_gamma=2,\n",
    "                    #loss_function='Focal',       # turn focal loss ON\n",
    "                    #focal_loss_gamma=2.0,\n",
    "                    learning_rate=0.05,\n",
    "                    depth=6,\n",
    "                    early_stopping_rounds=50,\n",
    "                    l2_leaf_reg=3,\n",
    "                    # avoid verbose logs\n",
    "                    verbose=False,\n",
    "                    random_seed=24,\n",
    "                    thread_count=-1\n",
    "                    \n",
    "               ))\n",
    "])\n",
    "\n",
    "# 3) CatBoost hyperparameter grid\n",
    "cat_param_dist = {\n",
    "    'clf__iterations':       [500,1000,800],\n",
    "    'clf__loss_function':      ['MultiClass'],\n",
    "    #'clf__use_focal_loss':     [True, False],       # try both\n",
    "    #'clf__focal_loss_gamma':   [1.0, 2.0, 5.0], \n",
    "    #'clf__focal_alpha':        [0.25, 1.0, 2.0], \n",
    "    'clf__learning_rate':    [0.01, 0.03, 0.1],\n",
    "    'clf__depth':            [6,8,10],\n",
    "    'clf__l2_leaf_reg':      [1,3,10],\n",
    "    'clf__bagging_temperature': [0,3,7],\n",
    "    'clf__rsm':              [0.5, 0.8, 1.0]\n",
    "}\n",
    "scoring_1 = {\n",
    "    'pr_auc':       'average_precision',       # your primary metric\n",
    "    'roc_auc_ovo':  'roc_auc_ovo',             # multiclass ROC‐AUC\n",
    "    'f1_macro':     'f1_macro',                # overall F1 across classes\n",
    "    'bal_acc':      make_scorer(balanced_accuracy_score),\n",
    "    'neg_log_loss': 'neg_log_loss'             # lower log‐loss = better\n",
    "}\n",
    "\n",
    "# 6) Rolling CV search\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_cat = RandomizedSearchCV(\n",
    "    pipeline_B2_cat,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    refit='roc_auc_ovo',\n",
    "    scoring=scoring_1,\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_cat.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_cat.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_cat.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
