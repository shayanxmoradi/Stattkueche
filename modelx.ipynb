{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-18T23:26:06.264252Z",
     "start_time": "2025-07-18T23:26:06.259268Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:26:23.686448Z",
     "start_time": "2025-07-18T23:26:06.286644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display_functions import display\n",
    "\n",
    "# Sklearn & related imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Other libraries\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "\n",
    "# Load data\n",
    "df_weather2 = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather3.csv', parse_dates=['DateOfCancel', 'DateOfService'])\n",
    "\n"
   ],
   "id": "224d152a936ccb5a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:26:23.714020Z",
     "start_time": "2025-07-18T23:26:23.702780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 2. CUSTOM TRANSFORMER DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean', 'count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                (agg['count'] * agg['mean'] +\n",
    "                 self.smoothing * self.global_mean_)\n",
    "                / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c]\\\n",
    "                .map(self.mapping_.get(c))\\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site','MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col  = value_col\n",
    "        self.out_col    = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_    = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X      = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site','MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys    = group_keys\n",
    "        self.n_clusters    = n_clusters\n",
    "        self.out_col       = out_col\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled      = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10).fit(scaled) # Set n_init explicitly\n",
    "        tuples      = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_     = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys   = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf,-np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n"
   ],
   "id": "6e7968e4b4bab42",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:28:52.685541Z",
     "start_time": "2025-07-18T23:26:23.725998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 3. VIF CHECK\n",
    "# =============================================================================\n",
    "num_cols = df_weather2.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "drop_col = ['DateOfOrder', 'DateOfService','days_to_cancel','CanceledQty','OrderId', 'TransactionId','cancel_timing','BookingNr','DateOfCancel','net_qty']\n",
    "vif_col_1 = [c for c in num_cols if c not in drop_col]\n",
    "\n",
    "# FIX 1: Changed df_encoded to df_weather2\n",
    "vif_dat_1 = df_weather2[vif_col_1].copy()\n",
    "vif_dat_1.replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "vif_dat_1 = vif_dat_1.dropna()\n",
    "\n",
    "vif_scores_1 = [variance_inflation_factor(vif_dat_1.values, i) for i in range(vif_dat_1.shape[1])]\n",
    "\n",
    "vif_table_1 = (pd.DataFrame({'feature':vif_col_1,'VIF':vif_scores_1}).sort_values('VIF',ascending=False).reset_index(drop=True))\n",
    "print('VIF values')\n",
    "display(vif_table_1)\n",
    "\n",
    "hg_vif_1 = vif_table_1.loc[vif_table_1['VIF']>10,'feature'].to_list()\n",
    "print('the high vif columns dropped')\n",
    "df_without_vif_1 = vif_dat_1.drop(columns=hg_vif_1)\n",
    "print(df_without_vif_1.columns)\n"
   ],
   "id": "6a71fac386b6cd07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shayan/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             feature           VIF\n",
       "0          afternoon           inf\n",
       "1            morning           inf\n",
       "2            evening           inf\n",
       "3        day_of_year  3.102898e+04\n",
       "4              month  3.086469e+04\n",
       "5       day_of_month  1.300759e+02\n",
       "6            quarter  2.209046e+01\n",
       "7        MenuSubsidy  6.593575e+00\n",
       "8          MenuPrice  6.495096e+00\n",
       "9         order_hour  5.359547e+00\n",
       "10         rain_flag  1.776355e+00\n",
       "11           prcp_mm  1.762272e+00\n",
       "12            tavg_C  1.680347e+00\n",
       "13        is_weekend  1.621648e+00\n",
       "14           weekday  1.556778e+00\n",
       "15          temp_dev  1.465128e+00\n",
       "16  hist_cancel_rate  1.328794e+00\n",
       "17        Unnamed: 0  1.260494e+00\n",
       "18      is_month_end  1.188034e+00\n",
       "19    is_month_start  1.166747e+00\n",
       "20           cos_doy  1.104725e+00\n",
       "21        is_holiday  1.065092e+00\n",
       "22          OrderQty  1.003279e+00\n",
       "23           sin_doy  9.746198e-02"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afternoon</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>morning</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>evening</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day_of_year</td>\n",
       "      <td>3.102898e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>month</td>\n",
       "      <td>3.086469e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_month</td>\n",
       "      <td>1.300759e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quarter</td>\n",
       "      <td>2.209046e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MenuSubsidy</td>\n",
       "      <td>6.593575e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MenuPrice</td>\n",
       "      <td>6.495096e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>order_hour</td>\n",
       "      <td>5.359547e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rain_flag</td>\n",
       "      <td>1.776355e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prcp_mm</td>\n",
       "      <td>1.762272e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tavg_C</td>\n",
       "      <td>1.680347e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>is_weekend</td>\n",
       "      <td>1.621648e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>weekday</td>\n",
       "      <td>1.556778e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>temp_dev</td>\n",
       "      <td>1.465128e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>hist_cancel_rate</td>\n",
       "      <td>1.328794e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>1.260494e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is_month_end</td>\n",
       "      <td>1.188034e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>is_month_start</td>\n",
       "      <td>1.166747e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cos_doy</td>\n",
       "      <td>1.104725e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>is_holiday</td>\n",
       "      <td>1.065092e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>OrderQty</td>\n",
       "      <td>1.003279e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sin_doy</td>\n",
       "      <td>9.746198e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the high vif columns dropped\n",
      "Index(['Unnamed: 0', 'OrderQty', 'MenuPrice', 'MenuSubsidy', 'weekday',\n",
      "       'is_weekend', 'sin_doy', 'cos_doy', 'is_month_end', 'is_month_start',\n",
      "       'order_hour', 'hist_cancel_rate', 'is_holiday', 'tavg_C', 'prcp_mm',\n",
      "       'rain_flag', 'temp_dev'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-18T23:34:59.115857Z",
     "start_time": "2025-07-18T23:28:52.716055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 4. STAGE A: RANDOM FOREST\n",
    "# =============================================================================\n",
    "df_A    = df_weather2.copy()\n",
    "y_A     = (df_A['CanceledQty'] > 0).astype(int)\n",
    "X_A     = df_A.drop(columns=[\n",
    "    'CanceledQty', 'cancel_timing', 'DateOfOrder', 'DateOfService', 'DateOfCancel',\n",
    "    'OrderId', 'TransactionId', 'BookingNr', 'hist_cancel_rate', 'GroupName', 'SchoolID'\n",
    "])\n",
    "\n",
    "drop_cols_A = [\n",
    "    'Site','MenuBase','MenuName','GroupName','MenuNorm','MenuCode','net_qty','days_to_cancel'\n",
    "]\n",
    "\n",
    "pipeline_A = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)), # FIX 2: This now works as the class is pre-defined\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                   n_clusters=5)),\n",
    "    ('drop',    ColumnDropper(drop_cols_A)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    ('clf',     RandomForestClassifier(\n",
    "                   n_estimators=500, criterion='entropy', max_depth=8,\n",
    "                   min_samples_split=5, min_samples_leaf=1, max_features='sqrt',\n",
    "                   class_weight='balanced_subsample', random_state=24, n_jobs=-1\n",
    "               )),\n",
    "])\n",
    "\n",
    "tscv    = TimeSeriesSplit(n_splits=5)\n",
    "scores  = cross_val_score(pipeline_A, X_A, y_A, cv=tscv, scoring='roc_auc', n_jobs=-1)\n",
    "print(\"Stage A ROC-AUC:\", np.round(scores.mean(),4))\n",
    "\n"
   ],
   "id": "28d6bbd8dffe00a6",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 32\u001B[39m\n\u001B[32m     15\u001B[39m pipeline_A = ImbPipeline([\n\u001B[32m     16\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mte\u001B[39m\u001B[33m'\u001B[39m,      InCVTargetEncoder(cols=[\u001B[33m'\u001B[39m\u001B[33mSite\u001B[39m\u001B[33m'\u001B[39m,\u001B[33m'\u001B[39m\u001B[33mMenuBase\u001B[39m\u001B[33m'\u001B[39m], smoothing=\u001B[32m0.3\u001B[39m)), \u001B[38;5;66;03m# FIX 2: This now works as the class is pre-defined\u001B[39;00m\n\u001B[32m     17\u001B[39m     (\u001B[33m'\u001B[39m\u001B[33mhist\u001B[39m\u001B[33m'\u001B[39m,    HistCancelRateTransformer()),\n\u001B[32m   (...)\u001B[39m\u001B[32m     28\u001B[39m                )),\n\u001B[32m     29\u001B[39m ])\n\u001B[32m     31\u001B[39m tscv    = TimeSeriesSplit(n_splits=\u001B[32m5\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m scores  = \u001B[43mcross_val_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_A\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtscv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mroc_auc\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStage A ROC-AUC:\u001B[39m\u001B[33m\"\u001B[39m, np.round(scores.mean(),\u001B[32m4\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    212\u001B[39m         skip_parameter_validation=(\n\u001B[32m    213\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    214\u001B[39m         )\n\u001B[32m    215\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    218\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    219\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    222\u001B[39m     msg = re.sub(\n\u001B[32m    223\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    224\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    225\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    226\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:684\u001B[39m, in \u001B[36mcross_val_score\u001B[39m\u001B[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001B[39m\n\u001B[32m    681\u001B[39m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[32m    682\u001B[39m scorer = check_scoring(estimator, scoring=scoring)\n\u001B[32m--> \u001B[39m\u001B[32m684\u001B[39m cv_results = \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    685\u001B[39m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    686\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m=\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    687\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m=\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    688\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    689\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscoring\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mscore\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    690\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    691\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    692\u001B[39m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    693\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    694\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpre_dispatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    695\u001B[39m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    696\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    697\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[33m\"\u001B[39m\u001B[33mtest_score\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216\u001B[39m, in \u001B[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m    212\u001B[39m         skip_parameter_validation=(\n\u001B[32m    213\u001B[39m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m    214\u001B[39m         )\n\u001B[32m    215\u001B[39m     ):\n\u001B[32m--> \u001B[39m\u001B[32m216\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    218\u001B[39m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[32m    219\u001B[39m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[32m    220\u001B[39m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[32m    222\u001B[39m     msg = re.sub(\n\u001B[32m    223\u001B[39m         \u001B[33mr\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[33m\\\u001B[39m\u001B[33mw+ must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    224\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc.\u001B[34m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must be\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    225\u001B[39m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[32m    226\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:411\u001B[39m, in \u001B[36mcross_validate\u001B[39m\u001B[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[39m\n\u001B[32m    408\u001B[39m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[32m    409\u001B[39m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[32m    410\u001B[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001B[32m--> \u001B[39m\u001B[32m411\u001B[39m results = \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    412\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    413\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    415\u001B[39m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    416\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    417\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    418\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    419\u001B[39m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    420\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    421\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    422\u001B[39m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrouted_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscorer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    423\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    424\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    425\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    426\u001B[39m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m=\u001B[49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    427\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    428\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[32m    429\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    431\u001B[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[32m    433\u001B[39m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[32m    434\u001B[39m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[32m    435\u001B[39m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/sklearn/utils/parallel.py:77\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m     72\u001B[39m config = get_config()\n\u001B[32m     73\u001B[39m iterable_with_config = (\n\u001B[32m     74\u001B[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[32m     75\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[32m     76\u001B[39m )\n\u001B[32m---> \u001B[39m\u001B[32m77\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/joblib/parallel.py:2071\u001B[39m, in \u001B[36mParallel.__call__\u001B[39m\u001B[34m(self, iterable)\u001B[39m\n\u001B[32m   2065\u001B[39m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[32m   2066\u001B[39m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[32m   2067\u001B[39m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[32m   2068\u001B[39m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[32m   2069\u001B[39m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[32m-> \u001B[39m\u001B[32m2071\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/joblib/parallel.py:1681\u001B[39m, in \u001B[36mParallel._get_outputs\u001B[39m\u001B[34m(self, iterator, pre_dispatch)\u001B[39m\n\u001B[32m   1678\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[32m   1680\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backend.retrieval_context():\n\u001B[32m-> \u001B[39m\u001B[32m1681\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._retrieve()\n\u001B[32m   1683\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[32m   1684\u001B[39m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[32m   1685\u001B[39m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[32m   1686\u001B[39m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[32m   1687\u001B[39m     \u001B[38;5;28mself\u001B[39m._exception = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/IDS2/Stattkueche/venv_arm/lib/python3.13/site-packages/joblib/parallel.py:1799\u001B[39m, in \u001B[36mParallel._retrieve\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1788\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.return_ordered:\n\u001B[32m   1789\u001B[39m     \u001B[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001B[39;00m\n\u001B[32m   1790\u001B[39m     \u001B[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1794\u001B[39m     \u001B[38;5;66;03m# control only have to be done on the amount of time the next\u001B[39;00m\n\u001B[32m   1795\u001B[39m     \u001B[38;5;66;03m# dispatched job is pending.\u001B[39;00m\n\u001B[32m   1796\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (nb_jobs == \u001B[32m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[32m   1797\u001B[39m         \u001B[38;5;28mself\u001B[39m._jobs[\u001B[32m0\u001B[39m].get_status(timeout=\u001B[38;5;28mself\u001B[39m.timeout) == TASK_PENDING\n\u001B[32m   1798\u001B[39m     ):\n\u001B[32m-> \u001B[39m\u001B[32m1799\u001B[39m         \u001B[43mtime\u001B[49m\u001B[43m.\u001B[49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1800\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m   1802\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m nb_jobs == \u001B[32m0\u001B[39m:\n\u001B[32m   1803\u001B[39m     \u001B[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001B[39;00m\n\u001B[32m   1804\u001B[39m     \u001B[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1810\u001B[39m     \u001B[38;5;66;03m# timeouts before any other dispatched job has completed and\u001B[39;00m\n\u001B[32m   1811\u001B[39m     \u001B[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 5. STAGE A: LGBM\n",
    "# =============================================================================\n",
    "pipeline_A_l = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                   profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                   n_clusters=5)),\n",
    "    ('drop',    ColumnDropper(drop_cols_A)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    ('clf',       LGBMClassifier(objective='binary', # Changed to binary for 0/1 target\n",
    "                                 random_state=24,\n",
    "                                 metric=\"roc_auc\", # Common metric for binary classification\n",
    "                                 n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_lgbm = {\n",
    "    'clf__n_estimators':      [200,500,800],\n",
    "    'clf__learning_rate':     [0.01,0.03,0.05],\n",
    "    'clf__num_leaves':        [10, 15, 31],\n",
    "    'clf__max_depth':         [6, 10, 20],\n",
    "    'clf__subsample':         [0.7, 1.0],\n",
    "    'clf__colsample_bytree':  [0.7, 1.0],\n",
    "    'clf__min_child_samples': [10, 20]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# FIX 3: RandomizedSearchCV is now imported\n",
    "search_A_lgbm = RandomizedSearchCV(\n",
    "    pipeline_A_l,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc', # roc_auc is for binary, roc_auc_ovo is for multiclass\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_A_lgbm.fit(X_A, y_A)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC:\", np.round(search_A_lgbm.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_A_lgbm.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n"
   ],
   "id": "3ed33f4c9f21d408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 6. STAGE B2: RANDOM FOREST\n",
    "# =============================================================================\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le    = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty', 'cancel_timing','timing_code', 'DateOfOrder','DateOfService',\n",
    "    'DateOfCancel', 'OrderId','TransactionId','BookingNr','hist_cancel_rate',\n",
    "    'GroupName','SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "\n",
    "drop_cols_B2 = ['MenuName','GroupName','MenuNorm','MenuCode']\n",
    "drop_post = ['Site','MenuBase','net_qty','days_to_cancel']\n",
    "\n",
    "pipeline_B2_smote = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    ('smote',   SMOTE(random_state=24)),\n",
    "    ('clf',     RandomForestClassifier(\n",
    "                   class_weight='balanced', random_state=24, n_jobs=-1\n",
    "               )),\n",
    "])\n",
    "\n",
    "param_dist_rf = {\n",
    "    'clf__n_estimators':      [600, 1200],\n",
    "    'clf__max_depth':         [6, 10],\n",
    "    'clf__min_samples_split': [10, 20],\n",
    "    'clf__min_samples_leaf':  [15],\n",
    "    'clf__max_features':      [0.8, 'sqrt'], # max_features > 1.0 is not valid\n",
    "    'clf__criterion':         ['entropy'],\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_rf = RandomizedSearchCV(\n",
    "    pipeline_B2_smote,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_rf.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best RF roc_auc_ovo_weighted:\", np.round(search_rf.best_score_, 4))\n",
    "print(\"Best hyper-parameters:\")\n",
    "for k, v in search_rf.best_params_.items():\n",
    "    print(f\"  {k} = {v}\")\n"
   ],
   "id": "431285e2920baab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 7. STAGE B2: LGBM\n",
    "# =============================================================================\n",
    "pipeline_B2_lgbm = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "   ('clf',       LGBMClassifier(objective='multiclass',\n",
    "                                 num_class=len(np.unique(y_B2)),\n",
    "                                 random_state=24,\n",
    "                                 metric=\"multi_logloss\",\n",
    "                                 n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_lgbm_b2 = {\n",
    "    'clf__n_estimators':      [200, 600],\n",
    "    'clf__learning_rate':     [0.01, 0.07],\n",
    "    'clf__num_leaves':        [31, 40],\n",
    "    'clf__max_depth':         [10, 20],\n",
    "    'clf__subsample':         [0.7, 1.0],\n",
    "    'clf__colsample_bytree':  [1.0], # colsample_bytree cannot be > 1.0\n",
    "    'clf__min_child_samples': [20, 30]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_lgbm_b2 = RandomizedSearchCV(\n",
    "    pipeline_B2_lgbm,\n",
    "    param_distributions=param_dist_lgbm_b2,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_lgbm_b2.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_lgbm_b2.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_lgbm_b2.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n"
   ],
   "id": "2153d324d5ce3847"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 8. STAGE B2: CATBOOST\n",
    "# =============================================================================\n",
    "pipeline_B2_cat = ImbPipeline([\n",
    "    ('te',      InCVTargetEncoder(cols=['Site','MenuBase'], smoothing=0.3)),\n",
    "    ('drop',    ColumnDropper(drop_cols_B2)),\n",
    "    ('hist',    HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "                    profile_feats=['hist_cancel_rate','rain_flag','temp_dev','sin_doy', 'cos_doy', 'month',\n",
    "       'day_of_month', 'is_month_end', 'is_month_start','tavg_C', 'prcp_mm'],\n",
    "                    n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),\n",
    "    ('impute',  MissingFlagImputer()),\n",
    "    ('clf',     CatBoostClassifier(\n",
    "                    iterations=500,\n",
    "                    auto_class_weights='Balanced',\n",
    "                    loss_function='MultiClass',\n",
    "                    learning_rate=0.05,\n",
    "                    depth=6,\n",
    "                    early_stopping_rounds=50,\n",
    "                    l2_leaf_reg=3,\n",
    "                    verbose=False,\n",
    "                    random_seed=24,\n",
    "                    thread_count=-1\n",
    "               ))\n",
    "])\n",
    "\n",
    "cat_param_dist = {\n",
    "    'clf__iterations':       [500, 800, 1000],\n",
    "    'clf__learning_rate':    [0.01, 0.03, 0.1],\n",
    "    'clf__depth':            [6, 8, 10],\n",
    "    'clf__l2_leaf_reg':      [1, 3, 10],\n",
    "    'clf__bagging_temperature': [0, 3, 7],\n",
    "    'clf__rsm':              [0.5, 0.8, 1.0] # same as colsample_bylevel\n",
    "}\n",
    "scoring_1 = {\n",
    "    'pr_auc':       'average_precision',\n",
    "    'roc_auc_ovo':  'roc_auc_ovo',\n",
    "    'f1_macro':     'f1_macro',\n",
    "    'bal_acc':      make_scorer(balanced_accuracy_score),\n",
    "    'neg_log_loss': 'neg_log_loss'\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_cat = RandomizedSearchCV(\n",
    "    pipeline_B2_cat,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    refit='roc_auc_ovo',\n",
    "    scoring=scoring_1,\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_cat.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best CatBoost Score (refit='roc_auc_ovo'):\", np.round(search_cat.best_score_,4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_cat.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")"
   ],
   "id": "3dfa05017c6f9111"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# 1. IMPORTS & SETUP\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.display_functions import display\n",
    "\n",
    "# Sklearn & related imports\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Other libraries\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "\n",
    "# Load data\n",
    "df_weather2 = pd.read_csv('/Users/shayan/Desktop/IDS2/Stattkueche/df_weather3.csv',\n",
    "                          parse_dates=['DateOfCancel', 'DateOfService'])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CUSTOM TRANSFORMER DEFINITIONS\n",
    "# =============================================================================\n",
    "\n",
    "class InCVTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, smoothing=1.0):\n",
    "        self.cols = cols\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.global_mean_ = y.mean()\n",
    "        self.mapping_ = {}\n",
    "        for c in self.cols:\n",
    "            df = pd.DataFrame({c: X[c], 'target': y})\n",
    "            agg = df.groupby(c)['target'].agg(['mean', 'count'])\n",
    "            # smoothing formula\n",
    "            agg['enc'] = (\n",
    "                    (agg['count'] * agg['mean'] +\n",
    "                     self.smoothing * self.global_mean_)\n",
    "                    / (agg['count'] + self.smoothing)\n",
    "            )\n",
    "            self.mapping_[c] = agg['enc']\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            X[c + '_te'] = X[c] \\\n",
    "                .map(self.mapping_.get(c)) \\\n",
    "                .fillna(self.global_mean_)\n",
    "        return X\n",
    "\n",
    "\n",
    "class HistCancelRateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_keys=('Site', 'MenuBase'), value_col='net_qty', out_col='hist_cancel_rate'):\n",
    "        self.group_keys = group_keys\n",
    "        self.value_col = value_col\n",
    "        self.out_col = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        self.hist_ = X.groupby(keys)[self.value_col].mean()\n",
    "        self.default_ = self.hist_.median()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        keys = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X = X.copy()\n",
    "        X[self.out_col] = [self.hist_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "class ClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, profile_feats, group_keys=('Site', 'MenuBase'),\n",
    "                 n_clusters=5, out_col='cluster_id'):\n",
    "        self.profile_feats = profile_feats\n",
    "        self.group_keys = group_keys\n",
    "        self.n_clusters = n_clusters\n",
    "        self.out_col = out_col\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        keys = list(self.group_keys)\n",
    "        prof = (X.groupby(keys)[self.profile_feats].mean().reset_index())\n",
    "        prof[self.profile_feats] = prof[self.profile_feats].fillna(prof[self.profile_feats].median())\n",
    "        self.scaler_ = StandardScaler().fit(prof[self.profile_feats])\n",
    "        scaled = self.scaler_.transform(prof[self.profile_feats])\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10).fit(\n",
    "            scaled)  # Set n_init explicitly\n",
    "        tuples = [tuple(r) for r in prof[keys].values]\n",
    "        self.cluster_map_ = dict(zip(tuples, self.kmeans_.labels_))\n",
    "        self.default_ = int(np.median(self.kmeans_.labels_))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        keys = list(self.group_keys)\n",
    "        tuples = [tuple(r) for r in X[keys].values]\n",
    "        X[self.out_col] = [self.cluster_map_.get(t, self.default_) for t in tuples]\n",
    "        return X\n",
    "\n",
    "\n",
    "class MissingFlagImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='median'):\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.num_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        clean = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        self.imputer_ = SimpleImputer(strategy=self.strategy).fit(clean)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.num_cols] = X[self.num_cols].replace([np.inf, -np.inf], np.nan)\n",
    "        for c in self.num_cols:\n",
    "            X[c + '_missing'] = X[c].isna().astype(int)\n",
    "        X[self.num_cols] = self.imputer_.transform(X[self.num_cols])\n",
    "        return X\n",
    "\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_to_drop):\n",
    "        self.cols_to_drop = cols_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.cols_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. VIF CHECK\n",
    "# =============================================================================\n",
    "num_cols = df_weather2.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "drop_col = ['DateOfOrder', 'DateOfService', 'days_to_cancel', 'CanceledQty', 'OrderId', 'TransactionId',\n",
    "            'cancel_timing', 'BookingNr', 'DateOfCancel', 'net_qty']\n",
    "vif_col_1 = [c for c in num_cols if c not in drop_col]\n",
    "\n",
    "# FIX 1: Changed df_encoded to df_weather2\n",
    "vif_dat_1 = df_weather2[vif_col_1].copy()\n",
    "vif_dat_1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "vif_dat_1 = vif_dat_1.dropna()\n",
    "\n",
    "vif_scores_1 = [variance_inflation_factor(vif_dat_1.values, i) for i in range(vif_dat_1.shape[1])]\n",
    "\n",
    "vif_table_1 = (\n",
    "    pd.DataFrame({'feature': vif_col_1, 'VIF': vif_scores_1}).sort_values('VIF', ascending=False).reset_index(\n",
    "        drop=True))\n",
    "print('VIF values')\n",
    "display(vif_table_1)\n",
    "\n",
    "hg_vif_1 = vif_table_1.loc[vif_table_1['VIF'] > 10, 'feature'].to_list()\n",
    "print('the high vif columns dropped')\n",
    "df_without_vif_1 = vif_dat_1.drop(columns=hg_vif_1)\n",
    "print(df_without_vif_1.columns)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. STAGE A: RANDOM FOREST\n",
    "# =============================================================================\n",
    "df_A = df_weather2.copy()\n",
    "y_A = (df_A['CanceledQty'] > 0).astype(int)\n",
    "X_A = df_A.drop(columns=[\n",
    "    'CanceledQty', 'cancel_timing', 'DateOfOrder', 'DateOfService', 'DateOfCancel',\n",
    "    'OrderId', 'TransactionId', 'BookingNr', 'hist_cancel_rate', 'GroupName', 'SchoolID'\n",
    "])\n",
    "\n",
    "drop_cols_A = [\n",
    "    'Site', 'MenuBase', 'MenuName', 'GroupName', 'MenuNorm', 'MenuCode', 'net_qty', 'days_to_cancel'\n",
    "]\n",
    "\n",
    "pipeline_A = ImbPipeline([\n",
    "    ('te', InCVTargetEncoder(cols=['Site', 'MenuBase'], smoothing=0.3)),\n",
    "    # FIX 2: This now works as the class is pre-defined\n",
    "    ('hist', HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "        profile_feats=['hist_cancel_rate', 'rain_flag', 'temp_dev', 'sin_doy', 'cos_doy', 'month',\n",
    "                       'day_of_month', 'is_month_end', 'is_month_start', 'tavg_C', 'prcp_mm'],\n",
    "        n_clusters=5)),\n",
    "    ('drop', ColumnDropper(drop_cols_A)),\n",
    "    ('impute', MissingFlagImputer()),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        n_estimators=500, criterion='entropy', max_depth=8,\n",
    "        min_samples_split=5, min_samples_leaf=1, max_features='sqrt',\n",
    "        class_weight='balanced_subsample', random_state=24, n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scores = cross_val_score(pipeline_A, X_A, y_A, cv=tscv, scoring='roc_auc', n_jobs=-1)\n",
    "print(\"Stage A ROC-AUC:\", np.round(scores.mean(), 4))\n",
    "\n",
    "# =============================================================================\n",
    "# 5. STAGE A: LGBM\n",
    "# =============================================================================\n",
    "pipeline_A_l = ImbPipeline([\n",
    "    ('te', InCVTargetEncoder(cols=['Site', 'MenuBase'], smoothing=0.3)),\n",
    "    ('hist', HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "        profile_feats=['hist_cancel_rate', 'rain_flag', 'temp_dev', 'sin_doy', 'cos_doy', 'month',\n",
    "                       'day_of_month', 'is_month_end', 'is_month_start', 'tavg_C', 'prcp_mm'],\n",
    "        n_clusters=5)),\n",
    "    ('drop', ColumnDropper(drop_cols_A)),\n",
    "    ('impute', MissingFlagImputer()),\n",
    "    ('clf', LGBMClassifier(objective='binary',  # Changed to binary for 0/1 target\n",
    "                           random_state=24,\n",
    "                           metric=\"roc_auc\",  # Common metric for binary classification\n",
    "                           n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_lgbm = {\n",
    "    'clf__n_estimators': [200, 500, 800],\n",
    "    'clf__learning_rate': [0.01, 0.03, 0.05],\n",
    "    'clf__num_leaves': [10, 15, 31],\n",
    "    'clf__max_depth': [6, 10, 20],\n",
    "    'clf__subsample': [0.7, 1.0],\n",
    "    'clf__colsample_bytree': [0.7, 1.0],\n",
    "    'clf__min_child_samples': [10, 20]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# FIX 3: RandomizedSearchCV is now imported\n",
    "search_A_lgbm = RandomizedSearchCV(\n",
    "    pipeline_A_l,\n",
    "    param_distributions=param_dist_lgbm,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc',  # roc_auc is for binary, roc_auc_ovo is for multiclass\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_A_lgbm.fit(X_A, y_A)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC:\", np.round(search_A_lgbm.best_score_, 4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_A_lgbm.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. STAGE B2: RANDOM FOREST\n",
    "# =============================================================================\n",
    "df_B2 = df_weather2[df_weather2.cancel_timing != 'no_cancel'].copy()\n",
    "le = LabelEncoder().fit(df_B2.cancel_timing)\n",
    "df_B2['timing_code'] = le.transform(df_B2.cancel_timing)\n",
    "\n",
    "X_B2 = df_B2.drop(columns=[\n",
    "    'CanceledQty', 'cancel_timing', 'timing_code', 'DateOfOrder', 'DateOfService',\n",
    "    'DateOfCancel', 'OrderId', 'TransactionId', 'BookingNr', 'hist_cancel_rate',\n",
    "    'GroupName', 'SchoolID'\n",
    "])\n",
    "y_B2 = df_B2['timing_code']\n",
    "\n",
    "drop_cols_B2 = ['MenuName', 'GroupName', 'MenuNorm', 'MenuCode']\n",
    "drop_post = ['Site', 'MenuBase', 'net_qty', 'days_to_cancel']\n",
    "\n",
    "pipeline_B2_smote = ImbPipeline([\n",
    "    ('te', InCVTargetEncoder(cols=['Site', 'MenuBase'], smoothing=0.3)),\n",
    "    ('drop', ColumnDropper(drop_cols_B2)),\n",
    "    ('hist', HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "        profile_feats=['hist_cancel_rate', 'rain_flag', 'temp_dev', 'sin_doy', 'cos_doy', 'month',\n",
    "                       'day_of_month', 'is_month_end', 'is_month_start', 'tavg_C', 'prcp_mm'],\n",
    "        n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),\n",
    "    ('impute', MissingFlagImputer()),\n",
    "    ('smote', SMOTE(random_state=24)),\n",
    "    ('clf', RandomForestClassifier(\n",
    "        class_weight='balanced', random_state=24, n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_dist_rf = {\n",
    "    'clf__n_estimators': [600, 1200],\n",
    "    'clf__max_depth': [6, 10],\n",
    "    'clf__min_samples_split': [10, 20],\n",
    "    'clf__min_samples_leaf': [15],\n",
    "    'clf__max_features': [0.8, 'sqrt'],  # max_features > 1.0 is not valid\n",
    "    'clf__criterion': ['entropy'],\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_rf = RandomizedSearchCV(\n",
    "    pipeline_B2_smote,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_rf.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best RF roc_auc_ovo_weighted:\", np.round(search_rf.best_score_, 4))\n",
    "print(\"Best hyper-parameters:\")\n",
    "for k, v in search_rf.best_params_.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7. STAGE B2: LGBM\n",
    "# =============================================================================\n",
    "pipeline_B2_lgbm = ImbPipeline([\n",
    "    ('te', InCVTargetEncoder(cols=['Site', 'MenuBase'], smoothing=0.3)),\n",
    "    ('drop', ColumnDropper(drop_cols_B2)),\n",
    "    ('hist', HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "        profile_feats=['hist_cancel_rate', 'rain_flag', 'temp_dev', 'sin_doy', 'cos_doy', 'month',\n",
    "                       'day_of_month', 'is_month_end', 'is_month_start', 'tavg_C', 'prcp_mm'],\n",
    "        n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),\n",
    "    ('impute', MissingFlagImputer()),\n",
    "    ('clf', LGBMClassifier(objective='multiclass',\n",
    "                           num_class=len(np.unique(y_B2)),\n",
    "                           random_state=24,\n",
    "                           metric=\"multi_logloss\",\n",
    "                           n_jobs=-1))\n",
    "])\n",
    "\n",
    "param_dist_lgbm_b2 = {\n",
    "    'clf__n_estimators': [200, 600],\n",
    "    'clf__learning_rate': [0.01, 0.07],\n",
    "    'clf__num_leaves': [31, 40],\n",
    "    'clf__max_depth': [10, 20],\n",
    "    'clf__subsample': [0.7, 1.0],\n",
    "    'clf__colsample_bytree': [1.0],  # colsample_bytree cannot be > 1.0\n",
    "    'clf__min_child_samples': [20, 30]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_lgbm_b2 = RandomizedSearchCV(\n",
    "    pipeline_B2_lgbm,\n",
    "    param_distributions=param_dist_lgbm_b2,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    scoring='roc_auc_ovo_weighted',\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_lgbm_b2.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best LGBM ROC_AUC_OVO_weighted:\", np.round(search_lgbm_b2.best_score_, 4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_lgbm_b2.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 8. STAGE B2: CATBOOST\n",
    "# =============================================================================\n",
    "pipeline_B2_cat = ImbPipeline([\n",
    "    ('te', InCVTargetEncoder(cols=['Site', 'MenuBase'], smoothing=0.3)),\n",
    "    ('drop', ColumnDropper(drop_cols_B2)),\n",
    "    ('hist', HistCancelRateTransformer()),\n",
    "    ('cluster', ClusterTransformer(\n",
    "        profile_feats=['hist_cancel_rate', 'rain_flag', 'temp_dev', 'sin_doy', 'cos_doy', 'month',\n",
    "                       'day_of_month', 'is_month_end', 'is_month_start', 'tavg_C', 'prcp_mm'],\n",
    "        n_clusters=5)),\n",
    "    ('drop_post', ColumnDropper(drop_post)),\n",
    "    ('impute', MissingFlagImputer()),\n",
    "    ('clf', CatBoostClassifier(\n",
    "        iterations=500,\n",
    "        auto_class_weights='Balanced',\n",
    "        loss_function='MultiClass',\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        early_stopping_rounds=50,\n",
    "        l2_leaf_reg=3,\n",
    "        verbose=False,\n",
    "        random_seed=24,\n",
    "        thread_count=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "cat_param_dist = {\n",
    "    'clf__iterations': [500, 800, 1000],\n",
    "    'clf__learning_rate': [0.01, 0.03, 0.1],\n",
    "    'clf__depth': [6, 8, 10],\n",
    "    'clf__l2_leaf_reg': [1, 3, 10],\n",
    "    'clf__bagging_temperature': [0, 3, 7],\n",
    "    'clf__rsm': [0.5, 0.8, 1.0]  # same as colsample_bylevel\n",
    "}\n",
    "scoring_1 = {\n",
    "    'pr_auc': 'average_precision',\n",
    "    'roc_auc_ovo': 'roc_auc_ovo',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'bal_acc': make_scorer(balanced_accuracy_score),\n",
    "    'neg_log_loss': 'neg_log_loss'\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "search_cat = RandomizedSearchCV(\n",
    "    pipeline_B2_cat,\n",
    "    param_distributions=cat_param_dist,\n",
    "    n_iter=30,\n",
    "    cv=tscv,\n",
    "    refit='roc_auc_ovo',\n",
    "    scoring=scoring_1,\n",
    "    n_jobs=-1,\n",
    "    random_state=24\n",
    ")\n",
    "\n",
    "search_cat.fit(X_B2, y_B2)\n",
    "\n",
    "print(\"Best CatBoost Score (refit='roc_auc_ovo'):\", np.round(search_cat.best_score_, 4))\n",
    "print(\"Best hyper‐parameters:\")\n",
    "for k, v in search_cat.best_params_.items():\n",
    "    print(f\"  {k}: {v}\")"
   ],
   "id": "e2f8c41edb4aa940"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
